{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 数据加载\n",
    "我们的误差补偿训练数据存放在<I>adjustments.tsv</I>中，测试数据放在<I>test_adjustments.tsv</I>。\n",
    "\n",
    "**测试数据集为真实值，不能进行调整，否则会导致实际模型测试结果和实际的预测结果存在偏差，出现过于乐观或者过于消极的测试结果**\n",
    "\n",
    "只需要执行加载训练数据的代码即可，当然也可以修改代码加载指定文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1rRo8oNqZ-Rj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 加载训练数据\n",
    "train_dataset = pd.read_csv('./adjustments.tsv',\n",
    "                          sep='\\t',\n",
    "                          skipinitialspace=True)\n",
    "# 加载测试数据     ！！！ 测试数据集为真实值，不能进行调整，否则将会导致实际模型测试结果和真实预测结果存在偏差，使得最终加工的作品和预期不一致\n",
    "test_dataset = pd.read_csv('./test_adjustments.tsv',\n",
    "                          sep='\\t',\n",
    "                          skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印并查看数据，可以看出，adjustments.tsv文件的前n列为特征值，这些值代表着真实世界中影响机床的环境因素，例如刀具磨损、温度、湿度等等；后面几列为补偿指令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>特征0</th>\n",
       "      <th>特征1</th>\n",
       "      <th>特征2</th>\n",
       "      <th>特征3</th>\n",
       "      <th>特征4</th>\n",
       "      <th>特征5</th>\n",
       "      <th>特征6</th>\n",
       "      <th>特征7</th>\n",
       "      <th>特征8</th>\n",
       "      <th>特征9</th>\n",
       "      <th>...</th>\n",
       "      <th>特征16</th>\n",
       "      <th>特征17</th>\n",
       "      <th>补偿0</th>\n",
       "      <th>补偿1</th>\n",
       "      <th>补偿2</th>\n",
       "      <th>补偿3</th>\n",
       "      <th>补偿4</th>\n",
       "      <th>补偿5</th>\n",
       "      <th>补偿6</th>\n",
       "      <th>补偿7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4158</td>\n",
       "      <td>2.9711</td>\n",
       "      <td>10.7935</td>\n",
       "      <td>7.5279</td>\n",
       "      <td>2.3352</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>2.3096</td>\n",
       "      <td>3.3367</td>\n",
       "      <td>11.8639</td>\n",
       "      <td>12.7142</td>\n",
       "      <td>...</td>\n",
       "      <td>171.764</td>\n",
       "      <td>1434.24</td>\n",
       "      <td>0.331511</td>\n",
       "      <td>-0.932553</td>\n",
       "      <td>0.285048</td>\n",
       "      <td>-0.1435</td>\n",
       "      <td>-0.833982</td>\n",
       "      <td>0.767568</td>\n",
       "      <td>0.463969</td>\n",
       "      <td>1.904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6280</td>\n",
       "      <td>1.8616</td>\n",
       "      <td>10.1770</td>\n",
       "      <td>7.4684</td>\n",
       "      <td>2.1915</td>\n",
       "      <td>8.5945</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>2.9661</td>\n",
       "      <td>11.5816</td>\n",
       "      <td>12.2487</td>\n",
       "      <td>...</td>\n",
       "      <td>185.824</td>\n",
       "      <td>1469.19</td>\n",
       "      <td>0.894066</td>\n",
       "      <td>-0.446796</td>\n",
       "      <td>0.058519</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.715252</td>\n",
       "      <td>0.999105</td>\n",
       "      <td>0.988844</td>\n",
       "      <td>0.689742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9648</td>\n",
       "      <td>1.8103</td>\n",
       "      <td>10.1682</td>\n",
       "      <td>5.9705</td>\n",
       "      <td>2.0629</td>\n",
       "      <td>6.5349</td>\n",
       "      <td>2.8694</td>\n",
       "      <td>3.1185</td>\n",
       "      <td>11.7464</td>\n",
       "      <td>12.2074</td>\n",
       "      <td>...</td>\n",
       "      <td>187.576</td>\n",
       "      <td>1540.76</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.460716</td>\n",
       "      <td>0.997809</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.723031</td>\n",
       "      <td>0.992935</td>\n",
       "      <td>0.682903</td>\n",
       "      <td>0.749580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7119</td>\n",
       "      <td>1.6221</td>\n",
       "      <td>10.1487</td>\n",
       "      <td>6.8678</td>\n",
       "      <td>2.0694</td>\n",
       "      <td>6.8806</td>\n",
       "      <td>1.5791</td>\n",
       "      <td>2.3003</td>\n",
       "      <td>11.5545</td>\n",
       "      <td>12.0659</td>\n",
       "      <td>...</td>\n",
       "      <td>189.938</td>\n",
       "      <td>1498.29</td>\n",
       "      <td>0.998794</td>\n",
       "      <td>-0.862448</td>\n",
       "      <td>0.329694</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.891745</td>\n",
       "      <td>0.015078</td>\n",
       "      <td>0.997127</td>\n",
       "      <td>0.984439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3797</td>\n",
       "      <td>1.6852</td>\n",
       "      <td>10.9601</td>\n",
       "      <td>5.0035</td>\n",
       "      <td>3.1659</td>\n",
       "      <td>5.9471</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>2.6402</td>\n",
       "      <td>11.7458</td>\n",
       "      <td>12.7041</td>\n",
       "      <td>...</td>\n",
       "      <td>181.275</td>\n",
       "      <td>1465.11</td>\n",
       "      <td>0.442893</td>\n",
       "      <td>-0.990703</td>\n",
       "      <td>-0.151400</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.617880</td>\n",
       "      <td>-0.506397</td>\n",
       "      <td>-0.997502</td>\n",
       "      <td>0.376126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      特征0     特征1      特征2     特征3     特征4     特征5     特征6     特征7      特征8  \\\n",
       "0  1.4158  2.9711  10.7935  7.5279  2.3352  8.1042  2.3096  3.3367  11.8639   \n",
       "1  0.6280  1.8616  10.1770  7.4684  2.1915  8.5945  0.1379  2.9661  11.5816   \n",
       "2  0.9648  1.8103  10.1682  5.9705  2.0629  6.5349  2.8694  3.1185  11.7464   \n",
       "3  0.7119  1.6221  10.1487  6.8678  2.0694  6.8806  1.5791  2.3003  11.5545   \n",
       "4  0.3797  1.6852  10.9601  5.0035  3.1659  5.9471  0.0858  2.6402  11.7458   \n",
       "\n",
       "       特征9  ...     特征16     特征17       补偿0       补偿1       补偿2     补偿3  \\\n",
       "0  12.7142  ...  171.764  1434.24  0.331511 -0.932553  0.285048 -0.1435   \n",
       "1  12.2487  ...  185.824  1469.19  0.894066 -0.446796  0.058519 -0.4624   \n",
       "2  12.2074  ...  187.576  1540.76  0.999982  0.460716  0.997809 -0.4624   \n",
       "3  12.0659  ...  189.938  1498.29  0.998794 -0.862448  0.329694 -0.4624   \n",
       "4  12.7041  ...  181.275  1465.11  0.442893 -0.990703 -0.151400  0.2245   \n",
       "\n",
       "        补偿4       补偿5       补偿6       补偿7  \n",
       "0 -0.833982  0.767568  0.463969  1.904800  \n",
       "1  0.715252  0.999105  0.988844  0.689742  \n",
       "2  0.723031  0.992935  0.682903  0.749580  \n",
       "3  0.891745  0.015078  0.997127  0.984439  \n",
       "4  0.617880 -0.506397 -0.997502  0.376126  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "dataset = train_dataset.copy()\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 26)\n",
      "(2998, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>特征0</th>\n",
       "      <th>特征1</th>\n",
       "      <th>特征2</th>\n",
       "      <th>特征3</th>\n",
       "      <th>特征4</th>\n",
       "      <th>特征5</th>\n",
       "      <th>特征6</th>\n",
       "      <th>特征7</th>\n",
       "      <th>特征8</th>\n",
       "      <th>特征9</th>\n",
       "      <th>...</th>\n",
       "      <th>特征16</th>\n",
       "      <th>特征17</th>\n",
       "      <th>补偿0</th>\n",
       "      <th>补偿1</th>\n",
       "      <th>补偿2</th>\n",
       "      <th>补偿3</th>\n",
       "      <th>补偿4</th>\n",
       "      <th>补偿5</th>\n",
       "      <th>补偿6</th>\n",
       "      <th>补偿7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.008837</td>\n",
       "      <td>2.018090</td>\n",
       "      <td>10.501365</td>\n",
       "      <td>6.980399</td>\n",
       "      <td>2.991379</td>\n",
       "      <td>6.986831</td>\n",
       "      <td>1.983407</td>\n",
       "      <td>2.994988</td>\n",
       "      <td>11.739018</td>\n",
       "      <td>12.600981</td>\n",
       "      <td>...</td>\n",
       "      <td>172.242767</td>\n",
       "      <td>1474.643939</td>\n",
       "      <td>0.525279</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.250365</td>\n",
       "      <td>0.010439</td>\n",
       "      <td>-0.269551</td>\n",
       "      <td>0.524640</td>\n",
       "      <td>0.222959</td>\n",
       "      <td>0.561442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.573132</td>\n",
       "      <td>0.578950</td>\n",
       "      <td>0.286296</td>\n",
       "      <td>1.134771</td>\n",
       "      <td>0.574884</td>\n",
       "      <td>1.142659</td>\n",
       "      <td>1.144038</td>\n",
       "      <td>0.577503</td>\n",
       "      <td>0.128526</td>\n",
       "      <td>0.196063</td>\n",
       "      <td>...</td>\n",
       "      <td>12.198489</td>\n",
       "      <td>54.894522</td>\n",
       "      <td>0.399656</td>\n",
       "      <td>0.699439</td>\n",
       "      <td>0.497970</td>\n",
       "      <td>0.330564</td>\n",
       "      <td>0.672572</td>\n",
       "      <td>0.494762</td>\n",
       "      <td>0.716798</td>\n",
       "      <td>1.124138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000300</td>\n",
       "      <td>5.000700</td>\n",
       "      <td>2.000400</td>\n",
       "      <td>5.000800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.000700</td>\n",
       "      <td>11.155400</td>\n",
       "      <td>11.750600</td>\n",
       "      <td>...</td>\n",
       "      <td>121.207000</td>\n",
       "      <td>1377.980000</td>\n",
       "      <td>-0.950279</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-1.240660</td>\n",
       "      <td>-0.533200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.863171</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-3.354270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.518975</td>\n",
       "      <td>1.506825</td>\n",
       "      <td>10.257700</td>\n",
       "      <td>6.033325</td>\n",
       "      <td>2.501750</td>\n",
       "      <td>6.004525</td>\n",
       "      <td>1.001925</td>\n",
       "      <td>2.509325</td>\n",
       "      <td>11.668100</td>\n",
       "      <td>12.485025</td>\n",
       "      <td>...</td>\n",
       "      <td>165.299500</td>\n",
       "      <td>1437.355000</td>\n",
       "      <td>0.279705</td>\n",
       "      <td>-0.693314</td>\n",
       "      <td>-0.096690</td>\n",
       "      <td>-0.143500</td>\n",
       "      <td>-0.847946</td>\n",
       "      <td>0.307439</td>\n",
       "      <td>-0.492667</td>\n",
       "      <td>-0.107755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.014300</td>\n",
       "      <td>2.042550</td>\n",
       "      <td>10.497350</td>\n",
       "      <td>6.962800</td>\n",
       "      <td>2.955600</td>\n",
       "      <td>6.956750</td>\n",
       "      <td>1.985950</td>\n",
       "      <td>2.986350</td>\n",
       "      <td>11.762100</td>\n",
       "      <td>12.644450</td>\n",
       "      <td>...</td>\n",
       "      <td>174.141000</td>\n",
       "      <td>1456.825000</td>\n",
       "      <td>0.617192</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>0.250205</td>\n",
       "      <td>-0.143500</td>\n",
       "      <td>-0.550868</td>\n",
       "      <td>0.709205</td>\n",
       "      <td>0.495988</td>\n",
       "      <td>0.390784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.488325</td>\n",
       "      <td>2.523625</td>\n",
       "      <td>10.750450</td>\n",
       "      <td>7.944000</td>\n",
       "      <td>3.481775</td>\n",
       "      <td>7.957600</td>\n",
       "      <td>2.953800</td>\n",
       "      <td>3.496425</td>\n",
       "      <td>11.835725</td>\n",
       "      <td>12.747475</td>\n",
       "      <td>...</td>\n",
       "      <td>181.637250</td>\n",
       "      <td>1496.542500</td>\n",
       "      <td>0.860046</td>\n",
       "      <td>0.696679</td>\n",
       "      <td>0.604809</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.370577</td>\n",
       "      <td>0.905374</td>\n",
       "      <td>0.877369</td>\n",
       "      <td>1.159790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.998200</td>\n",
       "      <td>2.998600</td>\n",
       "      <td>10.999300</td>\n",
       "      <td>8.996300</td>\n",
       "      <td>3.999400</td>\n",
       "      <td>8.998500</td>\n",
       "      <td>3.996200</td>\n",
       "      <td>3.999400</td>\n",
       "      <td>11.950100</td>\n",
       "      <td>12.933200</td>\n",
       "      <td>...</td>\n",
       "      <td>193.145000</td>\n",
       "      <td>1785.880000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.832180</td>\n",
       "      <td>0.632400</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.585870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               特征0          特征1          特征2          特征3          特征4  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000   \n",
       "mean      1.008837     2.018090    10.501365     6.980399     2.991379   \n",
       "std       0.573132     0.578950     0.286296     1.134771     0.574884   \n",
       "min       0.000500     1.000000    10.000300     5.000700     2.000400   \n",
       "25%       0.518975     1.506825    10.257700     6.033325     2.501750   \n",
       "50%       1.014300     2.042550    10.497350     6.962800     2.955600   \n",
       "75%       1.488325     2.523625    10.750450     7.944000     3.481775   \n",
       "max       1.998200     2.998600    10.999300     8.996300     3.999400   \n",
       "\n",
       "               特征5          特征6          特征7          特征8          特征9  ...  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000  ...   \n",
       "mean      6.986831     1.983407     2.994988    11.739018    12.600981  ...   \n",
       "std       1.142659     1.144038     0.577503     0.128526     0.196063  ...   \n",
       "min       5.000800     0.000100     2.000700    11.155400    11.750600  ...   \n",
       "25%       6.004525     1.001925     2.509325    11.668100    12.485025  ...   \n",
       "50%       6.956750     1.985950     2.986350    11.762100    12.644450  ...   \n",
       "75%       7.957600     2.953800     3.496425    11.835725    12.747475  ...   \n",
       "max       8.998500     3.996200     3.999400    11.950100    12.933200  ...   \n",
       "\n",
       "              特征16         特征17          补偿0          补偿1          补偿2  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000   \n",
       "mean    172.242767  1474.643939     0.525279     0.005173     0.250365   \n",
       "std      12.198489    54.894522     0.399656     0.699439     0.497970   \n",
       "min     121.207000  1377.980000    -0.950279    -0.999992    -1.240660   \n",
       "25%     165.299500  1437.355000     0.279705    -0.693314    -0.096690   \n",
       "50%     174.141000  1456.825000     0.617192     0.015394     0.250205   \n",
       "75%     181.637250  1496.542500     0.860046     0.696679     0.604809   \n",
       "max     193.145000  1785.880000     1.000000     1.000000     1.832180   \n",
       "\n",
       "               补偿3          补偿4          补偿5          补偿6          补偿7  \n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000  \n",
       "mean      0.010439    -0.269551     0.524640     0.222959     0.561442  \n",
       "std       0.330564     0.672572     0.494762     0.716798     1.124138  \n",
       "min      -0.533200    -1.000000    -0.863171    -0.999998    -3.354270  \n",
       "25%      -0.143500    -0.847946     0.307439    -0.492667    -0.107755  \n",
       "50%      -0.143500    -0.550868     0.709205     0.495988     0.390784  \n",
       "75%       0.224500     0.370577     0.905374     0.877369     1.159790  \n",
       "max       0.632400     0.999997     0.999997     1.000000     6.585870  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds=dataset.dropna()\n",
    "w=['特征'+str(i) for i in range(18)]\n",
    "ds=ds.drop_duplicates(w)\n",
    "print(ds.shape)\n",
    "ds.describe()\n",
    "\n",
    "ds1=test_dataset.dropna()\n",
    "w=['特征'+str(i) for i in range(18)]\n",
    "ds1=ds1.drop_duplicates(w)\n",
    "print(ds1.shape)\n",
    "ds1.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# # 选择需要可视化的数据列\n",
    "# #data = ds[ds]\n",
    "\n",
    "# # 循环遍历每个数据列\n",
    "# for col in ds.columns:\n",
    "#     # 绘制单个数据列的核密度估计图\n",
    "#     fig = plt.figure(figsize=(5, 3))\n",
    "#     sns.kdeplot(ds[col])\n",
    "    \n",
    "#     # 添加网格线\n",
    "#     plt.grid()\n",
    "#     #plt.xticks(np.arange(-0.2, 1.2, 0.1),rotation=45)\n",
    "   \n",
    "#     # 设置图的标题和横轴标签\n",
    "#     plt.title(f'Kernel Density Estimate - {col}')\n",
    "#     plt.xlabel(col)\n",
    "    \n",
    "#     # 显示绘制的图形\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们可以简单分析数据，比如我们可以查看特征值和补偿值的分布特性，比如均值和方差："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均值 [   1.       1.998   10.5      6.998    3.001    6.995    2.005    2.999\n",
      "   11.736   12.604    1.768    0.649   19.417   19.598    3.349   17.983\n",
      "  172.057 1475.054]\n",
      "方差 [   0.332    0.332    0.083    1.327    0.333    1.337    1.331    0.336\n",
      "    0.017    0.039    0.074    0.213    0.163    0.069   10.31   525.861\n",
      "  149.833 3136.668]\n"
     ]
    }
   ],
   "source": [
    "average = np.average(ds.values[:,:18], axis=0)\n",
    "variance = np.var(ds.values[:,:18], axis=0)\n",
    "print('均值', average)\n",
    "print('方差', variance)\n",
    "\n",
    "# average1 = np.average(ds1.values[:,:18], axis=0)\n",
    "# variance1 = np.var(ds1.values[:,:18], axis=0)\n",
    "# print('均值', average1)\n",
    "# print('方差', variance1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45455, 26)\n",
      "(2771, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>特征0</th>\n",
       "      <th>特征1</th>\n",
       "      <th>特征2</th>\n",
       "      <th>特征3</th>\n",
       "      <th>特征4</th>\n",
       "      <th>特征5</th>\n",
       "      <th>特征6</th>\n",
       "      <th>特征7</th>\n",
       "      <th>特征8</th>\n",
       "      <th>特征9</th>\n",
       "      <th>...</th>\n",
       "      <th>特征16</th>\n",
       "      <th>特征17</th>\n",
       "      <th>补偿0</th>\n",
       "      <th>补偿1</th>\n",
       "      <th>补偿2</th>\n",
       "      <th>补偿3</th>\n",
       "      <th>补偿4</th>\n",
       "      <th>补偿5</th>\n",
       "      <th>补偿6</th>\n",
       "      <th>补偿7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "      <td>45455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.014642</td>\n",
       "      <td>2.013183</td>\n",
       "      <td>10.510766</td>\n",
       "      <td>7.003799</td>\n",
       "      <td>3.013533</td>\n",
       "      <td>7.014718</td>\n",
       "      <td>2.033679</td>\n",
       "      <td>3.004369</td>\n",
       "      <td>11.744101</td>\n",
       "      <td>12.614738</td>\n",
       "      <td>...</td>\n",
       "      <td>172.279479</td>\n",
       "      <td>1468.611993</td>\n",
       "      <td>0.511533</td>\n",
       "      <td>-0.000280</td>\n",
       "      <td>0.243483</td>\n",
       "      <td>0.014763</td>\n",
       "      <td>-0.344803</td>\n",
       "      <td>0.518227</td>\n",
       "      <td>0.218633</td>\n",
       "      <td>0.525979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.572315</td>\n",
       "      <td>0.573504</td>\n",
       "      <td>0.285374</td>\n",
       "      <td>1.135992</td>\n",
       "      <td>0.563851</td>\n",
       "      <td>1.134305</td>\n",
       "      <td>1.147723</td>\n",
       "      <td>0.572401</td>\n",
       "      <td>0.119637</td>\n",
       "      <td>0.178904</td>\n",
       "      <td>...</td>\n",
       "      <td>10.874075</td>\n",
       "      <td>47.381258</td>\n",
       "      <td>0.387979</td>\n",
       "      <td>0.707994</td>\n",
       "      <td>0.489074</td>\n",
       "      <td>0.317333</td>\n",
       "      <td>0.641076</td>\n",
       "      <td>0.500380</td>\n",
       "      <td>0.713947</td>\n",
       "      <td>1.102260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000100</td>\n",
       "      <td>5.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.000100</td>\n",
       "      <td>11.343700</td>\n",
       "      <td>12.015500</td>\n",
       "      <td>...</td>\n",
       "      <td>135.392000</td>\n",
       "      <td>1367.270000</td>\n",
       "      <td>-0.960185</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.417860</td>\n",
       "      <td>-0.533200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882233</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-3.625560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.524750</td>\n",
       "      <td>1.521700</td>\n",
       "      <td>10.268100</td>\n",
       "      <td>6.037000</td>\n",
       "      <td>2.536300</td>\n",
       "      <td>6.051750</td>\n",
       "      <td>1.045250</td>\n",
       "      <td>2.507600</td>\n",
       "      <td>11.673000</td>\n",
       "      <td>12.507600</td>\n",
       "      <td>...</td>\n",
       "      <td>165.404000</td>\n",
       "      <td>1436.090000</td>\n",
       "      <td>0.266459</td>\n",
       "      <td>-0.707664</td>\n",
       "      <td>-0.097622</td>\n",
       "      <td>-0.143500</td>\n",
       "      <td>-0.872061</td>\n",
       "      <td>0.288231</td>\n",
       "      <td>-0.491356</td>\n",
       "      <td>-0.130062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.019800</td>\n",
       "      <td>2.021500</td>\n",
       "      <td>10.513000</td>\n",
       "      <td>7.007800</td>\n",
       "      <td>3.021300</td>\n",
       "      <td>7.019900</td>\n",
       "      <td>2.053300</td>\n",
       "      <td>3.006100</td>\n",
       "      <td>11.765700</td>\n",
       "      <td>12.647400</td>\n",
       "      <td>...</td>\n",
       "      <td>173.779000</td>\n",
       "      <td>1453.560000</td>\n",
       "      <td>0.591393</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.246505</td>\n",
       "      <td>-0.143500</td>\n",
       "      <td>-0.627452</td>\n",
       "      <td>0.706473</td>\n",
       "      <td>0.478781</td>\n",
       "      <td>0.363849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.507000</td>\n",
       "      <td>2.506900</td>\n",
       "      <td>10.756150</td>\n",
       "      <td>7.974000</td>\n",
       "      <td>3.494600</td>\n",
       "      <td>7.980150</td>\n",
       "      <td>3.027750</td>\n",
       "      <td>3.496800</td>\n",
       "      <td>11.835500</td>\n",
       "      <td>12.751400</td>\n",
       "      <td>...</td>\n",
       "      <td>180.708500</td>\n",
       "      <td>1489.290000</td>\n",
       "      <td>0.834392</td>\n",
       "      <td>0.706349</td>\n",
       "      <td>0.595703</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.182918</td>\n",
       "      <td>0.909821</td>\n",
       "      <td>0.875031</td>\n",
       "      <td>1.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>8.999900</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.958200</td>\n",
       "      <td>12.940900</td>\n",
       "      <td>...</td>\n",
       "      <td>193.672000</td>\n",
       "      <td>1642.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.905170</td>\n",
       "      <td>0.632400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.422190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                特征0           特征1           特征2           特征3           特征4  \\\n",
       "count  45455.000000  45455.000000  45455.000000  45455.000000  45455.000000   \n",
       "mean       1.014642      2.013183     10.510766      7.003799      3.013533   \n",
       "std        0.572315      0.573504      0.285374      1.135992      0.563851   \n",
       "min        0.000100      1.000000     10.000000      5.000000      2.000100   \n",
       "25%        0.524750      1.521700     10.268100      6.037000      2.536300   \n",
       "50%        1.019800      2.021500     10.513000      7.007800      3.021300   \n",
       "75%        1.507000      2.506900     10.756150      7.974000      3.494600   \n",
       "max        2.000000      3.000000     11.000000      8.999900      4.000000   \n",
       "\n",
       "                特征5           特征6           特征7           特征8           特征9  \\\n",
       "count  45455.000000  45455.000000  45455.000000  45455.000000  45455.000000   \n",
       "mean       7.014718      2.033679      3.004369     11.744101     12.614738   \n",
       "std        1.134305      1.147723      0.572401      0.119637      0.178904   \n",
       "min        5.000100      0.000100      2.000100     11.343700     12.015500   \n",
       "25%        6.051750      1.045250      2.507600     11.673000     12.507600   \n",
       "50%        7.019900      2.053300      3.006100     11.765700     12.647400   \n",
       "75%        7.980150      3.027750      3.496800     11.835500     12.751400   \n",
       "max        9.000000      4.000000      4.000000     11.958200     12.940900   \n",
       "\n",
       "       ...          特征16          特征17           补偿0           补偿1  \\\n",
       "count  ...  45455.000000  45455.000000  45455.000000  45455.000000   \n",
       "mean   ...    172.279479   1468.611993      0.511533     -0.000280   \n",
       "std    ...     10.874075     47.381258      0.387979      0.707994   \n",
       "min    ...    135.392000   1367.270000     -0.960185     -1.000000   \n",
       "25%    ...    165.404000   1436.090000      0.266459     -0.707664   \n",
       "50%    ...    173.779000   1453.560000      0.591393      0.002151   \n",
       "75%    ...    180.708500   1489.290000      0.834392      0.706349   \n",
       "max    ...    193.672000   1642.800000      1.000000      1.000000   \n",
       "\n",
       "                补偿2           补偿3           补偿4           补偿5           补偿6  \\\n",
       "count  45455.000000  45455.000000  45455.000000  45455.000000  45455.000000   \n",
       "mean       0.243483      0.014763     -0.344803      0.518227      0.218633   \n",
       "std        0.489074      0.317333      0.641076      0.500380      0.713947   \n",
       "min       -1.417860     -0.533200     -1.000000     -0.882233     -1.000000   \n",
       "25%       -0.097622     -0.143500     -0.872061      0.288231     -0.491356   \n",
       "50%        0.246505     -0.143500     -0.627452      0.706473      0.478781   \n",
       "75%        0.595703      0.224500      0.182918      0.909821      0.875031   \n",
       "max        1.905170      0.632400      1.000000      1.000000      1.000000   \n",
       "\n",
       "                补偿7  \n",
       "count  45455.000000  \n",
       "mean       0.525979  \n",
       "std        1.102260  \n",
       "min       -3.625560  \n",
       "25%       -0.130062  \n",
       "50%        0.363849  \n",
       "75%        1.122345  \n",
       "max        7.422190  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算前18列特征的均值和标准差\n",
    "mean = np.mean(ds.values[:, :18], axis=0)\n",
    "std = np.std(ds.values[:, :18], axis=0)\n",
    "\n",
    "\n",
    "mean1 = np.mean(ds1.values[:, :18], axis=0)\n",
    "std1 = np.std(ds1.values[:, :18], axis=0)\n",
    "# 定义上限和下限\n",
    "upper_limit = mean + 3 * std\n",
    "lower_limit = mean - 3 * std\n",
    "\n",
    "\n",
    "\n",
    "upper_limit1 = mean1 + 3 * std\n",
    "lower_limit1 = mean1 - 3 * std\n",
    "# 使用布尔索引删除超出上限和下限的行\n",
    "cleaned_ds = ds[~((ds.values[:, :18] > upper_limit) | (ds.values[:, :18] < lower_limit)).any(axis=1)]\n",
    "\n",
    "\n",
    "cleaned_ds1 = ds1[~((ds1.values[:, :18] > upper_limit1) | (ds1.values[:, :18] < lower_limit1)).any(axis=1)]\n",
    "# # 计算特征的均值和标准差\n",
    "# mean = np.mean(ds.values, axis=0)\n",
    "# std = np.std(ds.values, axis=0)\n",
    "\n",
    "# # 定义上限和下限\n",
    "# upper_limit = mean + 3 * std\n",
    "# lower_limit = mean - 3 * std\n",
    "\n",
    "# # 使用布尔索引删除超出上限和下限的行\n",
    "# cleaned_ds = ds[~((ds.values > upper_limit) | (ds.values < lower_limit)).any(axis=1)]\n",
    "\n",
    "# 打印清理后数据集的形状\n",
    "print(cleaned_ds.shape)\n",
    "\n",
    "print(cleaned_ds1.shape)\n",
    "cleaned_ds.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # 提取特征数据和标签数据\n",
    "# X = cleaned_ds.values[:, :18]  # 提取前18列作为特征数据\n",
    "# y = cleaned_ds.values[:, 18:]  # 提取后8列作为标签数据\n",
    "\n",
    "# # 创建MinMaxScaler对象，并对特征数据进行归一化\n",
    "# scaler = MinMaxScaler()\n",
    "# X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# # 打印归一化后特征数据的范围（最小值和最大值）\n",
    "# print('特征数据归一化范围:', np.min(X_normalized), np.max(X_normalized))\n",
    "\n",
    "# # 打印训练数据集上计算得到的均值和标准差\n",
    "# print('训练数据集均值:', scaler.mean_)\n",
    "# print('训练数据集标准差:', scaler.scale_)\n",
    "\n",
    "# # 将归一化后的特征数据与标签数据重新合并成一个数据集\n",
    "# normalized_ds = np.concatenate((X_normalized, y), axis=1)\n",
    "# # 将归一化后的数据集转换为DataFrame\n",
    "# normalized_ds = pd.DataFrame(normalized_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建训练集和测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，我们将数据分为训练集和测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们分别获取训练集和测试集的特征以及补偿值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40910, 18) (40910, 8)\n",
      "(4545, 18) (4545, 8)\n",
      "(2771, 18) (2771, 8)\n"
     ]
    }
   ],
   "source": [
    "train_ds=cleaned_ds.sample(frac=0.9,random_state=0)\n",
    "val_ds=cleaned_ds.drop(train_ds.index)\n",
    "\n",
    "\n",
    "#训练集\n",
    "train_features=train_ds.values[:,:18]\n",
    "train_labels=train_ds.values[:,18:]\n",
    "\n",
    "#验证集\n",
    "val_features=val_ds.values[:,:18]\n",
    "val_labels=val_ds.values[:,18:]\n",
    "\n",
    "# 测试集\n",
    "# test_features=test_dataset.values[:,:18]\n",
    "# test_labels=test_dataset.values[:,18:]\n",
    "\n",
    "test_features = cleaned_ds1.copy()\n",
    "test_labels = test_features[['补偿'+str(i) for i in range(8)]].copy()\n",
    "test_features = test_features.drop(['补偿'+str(i) for i in range(8)], axis=1)\n",
    "\n",
    "# test_features=cleaned_ds1.values[:,:18]\n",
    "# test_labels=cleaned_ds1.values[:,18:]\n",
    "\n",
    "print(train_features.shape,train_labels.shape)\n",
    "print(val_features.shape,val_labels.shape)\n",
    "print(test_features.shape,test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据的分析和预处理的方法有很多种，我们只展示了一种方法。用户可根据自己的需要使用其他方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据的分析和预处理的方法有很多种，我们只展示了一种方法。用户可根据自己的需要使用其他方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 模型构建\n",
    "\n",
    "本平台支持基于Tensorflow-Serving的HTTP调用方式：该方式支持任何部署在TensorFlow Serving上的模型\n",
    "\n",
    "### TensorFlow\n",
    "首先，我们导入相关的依赖包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9xQKvCJ85kCQ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from numpy import array\n",
    "from numpy.random import uniform\n",
    "from numpy import hstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，我们开始构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normalizat  (None, 18)               37        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 300)               5700      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8)                 2408      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 188,745\n",
      "Trainable params: 188,708\n",
      "Non-trainable params: 37\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#----------构建模型及训练-----------------\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import optimizers\n",
    "# model = tf.keras.Sequential([\n",
    "#     layers.Dense(100, input_dim=train_features.shape[1], activation=\"relu\"),\n",
    "#     Dropout(0.5),\n",
    "#     layers.Dense(train_labels.shape[1])\n",
    "# ])\n",
    "\n",
    "\n",
    "# 创建Normalizaiton层\n",
    "normalizer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "# 计算并设置归一化参数\n",
    "normalizer.adapt(train_features)\n",
    "\n",
    "# 构建模型\n",
    "model = tf.keras.Sequential([\n",
    "    normalizer,  # 归一化层作为第一层\n",
    "    layers.Dense(300, activation=\"relu\", input_dim=train_features.shape[1]),\n",
    "    layers.Dense(300, activation=\"relu\"),\n",
    "    layers.Dense(300, activation=\"relu\"),\n",
    "    layers.Dense(train_labels.shape[1])\n",
    "    # normalizer,  # 归一化层作为第一层\n",
    "    # layers.Dense(100, activation=\"relu\"),\n",
    "    # layers.Dense(train_labels.shape[1])\n",
    "])\n",
    "\n",
    "# optimizers=optimizers.Nadam(learning_rate=0.01)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")  # 根据情况调整参数\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "设置模型训练参数进行模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.1354 - val_loss: 0.1205 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1279/1279 [==============================] - 2s 2ms/step - loss: 0.1150 - val_loss: 0.1117 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1279/1279 [==============================] - 2s 2ms/step - loss: 0.1099 - val_loss: 0.1077 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.1067 - val_loss: 0.1069 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.1047 - val_loss: 0.1046 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.1038 - val_loss: 0.1030 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.1030 - val_loss: 0.1030 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.1021 - val_loss: 0.1022 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.1018 - val_loss: 0.1020 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.1008 - val_loss: 0.1024 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.1004 - val_loss: 0.1022 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0997 - val_loss: 0.1005 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0990 - val_loss: 0.1017 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0987 - val_loss: 0.1015 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0982 - val_loss: 0.1005 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0976 - val_loss: 0.1023 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0973 - val_loss: 0.1025 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0961 - val_loss: 0.1014 - lr: 9.0000e-04\n",
      "Epoch 19/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0955 - val_loss: 0.1016 - lr: 9.0000e-04\n",
      "Epoch 20/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0949 - val_loss: 0.1019 - lr: 9.0000e-04\n",
      "Epoch 21/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0943 - val_loss: 0.1015 - lr: 9.0000e-04\n",
      "Epoch 22/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0938 - val_loss: 0.1034 - lr: 9.0000e-04\n",
      "Epoch 23/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0926 - val_loss: 0.1030 - lr: 8.1000e-04\n",
      "Epoch 24/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0919 - val_loss: 0.1023 - lr: 8.1000e-04\n",
      "Epoch 25/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0911 - val_loss: 0.1034 - lr: 8.1000e-04\n",
      "Epoch 26/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0905 - val_loss: 0.1050 - lr: 8.1000e-04\n",
      "Epoch 27/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0896 - val_loss: 0.1048 - lr: 8.1000e-04\n",
      "Epoch 28/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0881 - val_loss: 0.1059 - lr: 7.2900e-04\n",
      "Epoch 29/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0873 - val_loss: 0.1063 - lr: 7.2900e-04\n",
      "Epoch 30/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0865 - val_loss: 0.1055 - lr: 7.2900e-04\n",
      "Epoch 31/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0857 - val_loss: 0.1075 - lr: 7.2900e-04\n",
      "Epoch 32/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0850 - val_loss: 0.1073 - lr: 7.2900e-04\n",
      "Epoch 33/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0833 - val_loss: 0.1100 - lr: 6.5610e-04\n",
      "Epoch 34/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0825 - val_loss: 0.1107 - lr: 6.5610e-04\n",
      "Epoch 35/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0816 - val_loss: 0.1111 - lr: 6.5610e-04\n",
      "Epoch 36/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0807 - val_loss: 0.1119 - lr: 6.5610e-04\n",
      "Epoch 37/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0800 - val_loss: 0.1117 - lr: 6.5610e-04\n",
      "Epoch 38/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0782 - val_loss: 0.1141 - lr: 5.9049e-04\n",
      "Epoch 39/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0775 - val_loss: 0.1145 - lr: 5.9049e-04\n",
      "Epoch 40/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0766 - val_loss: 0.1161 - lr: 5.9049e-04\n",
      "Epoch 41/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0758 - val_loss: 0.1167 - lr: 5.9049e-04\n",
      "Epoch 42/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0750 - val_loss: 0.1161 - lr: 5.9049e-04\n",
      "Epoch 43/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0732 - val_loss: 0.1183 - lr: 5.3144e-04\n",
      "Epoch 44/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0725 - val_loss: 0.1173 - lr: 5.3144e-04\n",
      "Epoch 45/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0718 - val_loss: 0.1186 - lr: 5.3144e-04\n",
      "Epoch 46/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0710 - val_loss: 0.1207 - lr: 5.3144e-04\n",
      "Epoch 47/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0704 - val_loss: 0.1210 - lr: 5.3144e-04\n",
      "Epoch 48/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0688 - val_loss: 0.1208 - lr: 4.7830e-04\n",
      "Epoch 49/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0679 - val_loss: 0.1227 - lr: 4.7830e-04\n",
      "Epoch 50/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0673 - val_loss: 0.1228 - lr: 4.7830e-04\n",
      "Epoch 51/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0667 - val_loss: 0.1250 - lr: 4.7830e-04\n",
      "Epoch 52/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0662 - val_loss: 0.1281 - lr: 4.7830e-04\n",
      "Epoch 53/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0647 - val_loss: 0.1254 - lr: 4.3047e-04\n",
      "Epoch 54/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0640 - val_loss: 0.1259 - lr: 4.3047e-04\n",
      "Epoch 55/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0634 - val_loss: 0.1267 - lr: 4.3047e-04\n",
      "Epoch 56/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0628 - val_loss: 0.1277 - lr: 4.3047e-04\n",
      "Epoch 57/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0622 - val_loss: 0.1285 - lr: 4.3047e-04\n",
      "Epoch 58/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0609 - val_loss: 0.1295 - lr: 3.8742e-04\n",
      "Epoch 59/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0604 - val_loss: 0.1299 - lr: 3.8742e-04\n",
      "Epoch 60/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0598 - val_loss: 0.1310 - lr: 3.8742e-04\n",
      "Epoch 61/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0593 - val_loss: 0.1312 - lr: 3.8742e-04\n",
      "Epoch 62/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0588 - val_loss: 0.1338 - lr: 3.8742e-04\n",
      "Epoch 63/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0575 - val_loss: 0.1348 - lr: 3.4868e-04\n",
      "Epoch 64/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0570 - val_loss: 0.1331 - lr: 3.4868e-04\n",
      "Epoch 65/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0565 - val_loss: 0.1342 - lr: 3.4868e-04\n",
      "Epoch 66/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0562 - val_loss: 0.1365 - lr: 3.4868e-04\n",
      "Epoch 67/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0557 - val_loss: 0.1358 - lr: 3.4868e-04\n",
      "Epoch 68/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0546 - val_loss: 0.1374 - lr: 3.1381e-04\n",
      "Epoch 69/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0541 - val_loss: 0.1380 - lr: 3.1381e-04\n",
      "Epoch 70/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0537 - val_loss: 0.1374 - lr: 3.1381e-04\n",
      "Epoch 71/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0534 - val_loss: 0.1383 - lr: 3.1381e-04\n",
      "Epoch 72/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0529 - val_loss: 0.1392 - lr: 3.1381e-04\n",
      "Epoch 73/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0519 - val_loss: 0.1403 - lr: 2.8243e-04\n",
      "Epoch 74/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0515 - val_loss: 0.1401 - lr: 2.8243e-04\n",
      "Epoch 75/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0512 - val_loss: 0.1422 - lr: 2.8243e-04\n",
      "Epoch 76/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0509 - val_loss: 0.1420 - lr: 2.8243e-04\n",
      "Epoch 77/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0505 - val_loss: 0.1422 - lr: 2.8243e-04\n",
      "Epoch 78/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0495 - val_loss: 0.1423 - lr: 2.5419e-04\n",
      "Epoch 79/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0492 - val_loss: 0.1430 - lr: 2.5419e-04\n",
      "Epoch 80/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0489 - val_loss: 0.1460 - lr: 2.5419e-04\n",
      "Epoch 81/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0486 - val_loss: 0.1453 - lr: 2.5419e-04\n",
      "Epoch 82/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0483 - val_loss: 0.1459 - lr: 2.5419e-04\n",
      "Epoch 83/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0474 - val_loss: 0.1465 - lr: 2.2877e-04\n",
      "Epoch 84/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0472 - val_loss: 0.1471 - lr: 2.2877e-04\n",
      "Epoch 85/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0468 - val_loss: 0.1483 - lr: 2.2877e-04\n",
      "Epoch 86/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0466 - val_loss: 0.1480 - lr: 2.2877e-04\n",
      "Epoch 87/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0464 - val_loss: 0.1489 - lr: 2.2877e-04\n",
      "Epoch 88/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0457 - val_loss: 0.1488 - lr: 2.0589e-04\n",
      "Epoch 89/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0453 - val_loss: 0.1507 - lr: 2.0589e-04\n",
      "Epoch 90/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0452 - val_loss: 0.1505 - lr: 2.0589e-04\n",
      "Epoch 91/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0449 - val_loss: 0.1509 - lr: 2.0589e-04\n",
      "Epoch 92/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0447 - val_loss: 0.1507 - lr: 2.0589e-04\n",
      "Epoch 93/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0440 - val_loss: 0.1526 - lr: 1.8530e-04\n",
      "Epoch 94/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0438 - val_loss: 0.1527 - lr: 1.8530e-04\n",
      "Epoch 95/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0435 - val_loss: 0.1526 - lr: 1.8530e-04\n",
      "Epoch 96/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0434 - val_loss: 0.1538 - lr: 1.8530e-04\n",
      "Epoch 97/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0432 - val_loss: 0.1545 - lr: 1.8530e-04\n",
      "Epoch 98/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0426 - val_loss: 0.1547 - lr: 1.6677e-04\n",
      "Epoch 99/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0423 - val_loss: 0.1544 - lr: 1.6677e-04\n",
      "Epoch 100/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0422 - val_loss: 0.1552 - lr: 1.6677e-04\n",
      "Epoch 101/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0420 - val_loss: 0.1566 - lr: 1.6677e-04\n",
      "Epoch 102/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0418 - val_loss: 0.1553 - lr: 1.6677e-04\n",
      "Epoch 103/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0413 - val_loss: 0.1558 - lr: 1.5009e-04\n",
      "Epoch 104/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0411 - val_loss: 0.1571 - lr: 1.5009e-04\n",
      "Epoch 105/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0409 - val_loss: 0.1578 - lr: 1.5009e-04\n",
      "Epoch 106/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0407 - val_loss: 0.1575 - lr: 1.5009e-04\n",
      "Epoch 107/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0407 - val_loss: 0.1596 - lr: 1.5009e-04\n",
      "Epoch 108/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0402 - val_loss: 0.1591 - lr: 1.3509e-04\n",
      "Epoch 109/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0400 - val_loss: 0.1595 - lr: 1.3509e-04\n",
      "Epoch 110/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0398 - val_loss: 0.1606 - lr: 1.3509e-04\n",
      "Epoch 111/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0397 - val_loss: 0.1599 - lr: 1.3509e-04\n",
      "Epoch 112/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0395 - val_loss: 0.1601 - lr: 1.3509e-04\n",
      "Epoch 113/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0391 - val_loss: 0.1611 - lr: 1.2158e-04\n",
      "Epoch 114/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0390 - val_loss: 0.1605 - lr: 1.2158e-04\n",
      "Epoch 115/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0388 - val_loss: 0.1624 - lr: 1.2158e-04\n",
      "Epoch 116/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0387 - val_loss: 0.1625 - lr: 1.2158e-04\n",
      "Epoch 117/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0386 - val_loss: 0.1632 - lr: 1.2158e-04\n",
      "Epoch 118/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0382 - val_loss: 0.1629 - lr: 1.0942e-04\n",
      "Epoch 119/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0381 - val_loss: 0.1631 - lr: 1.0942e-04\n",
      "Epoch 120/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0380 - val_loss: 0.1627 - lr: 1.0942e-04\n",
      "Epoch 121/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0378 - val_loss: 0.1639 - lr: 1.0942e-04\n",
      "Epoch 122/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0378 - val_loss: 0.1640 - lr: 1.0942e-04\n",
      "Epoch 123/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0374 - val_loss: 0.1654 - lr: 9.8477e-05\n",
      "Epoch 124/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0373 - val_loss: 0.1652 - lr: 9.8477e-05\n",
      "Epoch 125/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0372 - val_loss: 0.1659 - lr: 9.8477e-05\n",
      "Epoch 126/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0371 - val_loss: 0.1655 - lr: 9.8477e-05\n",
      "Epoch 127/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0370 - val_loss: 0.1668 - lr: 9.8477e-05\n",
      "Epoch 128/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0367 - val_loss: 0.1663 - lr: 8.8629e-05\n",
      "Epoch 129/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0366 - val_loss: 0.1673 - lr: 8.8629e-05\n",
      "Epoch 130/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0365 - val_loss: 0.1672 - lr: 8.8629e-05\n",
      "Epoch 131/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0364 - val_loss: 0.1681 - lr: 8.8629e-05\n",
      "Epoch 132/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0363 - val_loss: 0.1675 - lr: 8.8629e-05\n",
      "Epoch 133/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0361 - val_loss: 0.1683 - lr: 7.9766e-05\n",
      "Epoch 134/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0360 - val_loss: 0.1683 - lr: 7.9766e-05\n",
      "Epoch 135/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0359 - val_loss: 0.1687 - lr: 7.9766e-05\n",
      "Epoch 136/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0358 - val_loss: 0.1690 - lr: 7.9766e-05\n",
      "Epoch 137/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0357 - val_loss: 0.1688 - lr: 7.9766e-05\n",
      "Epoch 138/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0355 - val_loss: 0.1690 - lr: 7.1790e-05\n",
      "Epoch 139/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0354 - val_loss: 0.1695 - lr: 7.1790e-05\n",
      "Epoch 140/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0354 - val_loss: 0.1703 - lr: 7.1790e-05\n",
      "Epoch 141/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0353 - val_loss: 0.1701 - lr: 7.1790e-05\n",
      "Epoch 142/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0352 - val_loss: 0.1703 - lr: 7.1790e-05\n",
      "Epoch 143/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0350 - val_loss: 0.1703 - lr: 6.4611e-05\n",
      "Epoch 144/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0349 - val_loss: 0.1710 - lr: 6.4611e-05\n",
      "Epoch 145/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0349 - val_loss: 0.1716 - lr: 6.4611e-05\n",
      "Epoch 146/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0348 - val_loss: 0.1717 - lr: 6.4611e-05\n",
      "Epoch 147/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0347 - val_loss: 0.1722 - lr: 6.4611e-05\n",
      "Epoch 148/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0346 - val_loss: 0.1714 - lr: 5.8150e-05\n",
      "Epoch 149/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0345 - val_loss: 0.1719 - lr: 5.8150e-05\n",
      "Epoch 150/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0344 - val_loss: 0.1727 - lr: 5.8150e-05\n",
      "Epoch 151/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0344 - val_loss: 0.1728 - lr: 5.8150e-05\n",
      "Epoch 152/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0343 - val_loss: 0.1730 - lr: 5.8150e-05\n",
      "Epoch 153/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0342 - val_loss: 0.1732 - lr: 5.2335e-05\n",
      "Epoch 154/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0341 - val_loss: 0.1737 - lr: 5.2335e-05\n",
      "Epoch 155/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0340 - val_loss: 0.1733 - lr: 5.2335e-05\n",
      "Epoch 156/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0340 - val_loss: 0.1739 - lr: 5.2335e-05\n",
      "Epoch 157/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0339 - val_loss: 0.1737 - lr: 5.2335e-05\n",
      "Epoch 158/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0338 - val_loss: 0.1744 - lr: 4.7101e-05\n",
      "Epoch 159/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0337 - val_loss: 0.1743 - lr: 4.7101e-05\n",
      "Epoch 160/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0337 - val_loss: 0.1744 - lr: 4.7101e-05\n",
      "Epoch 161/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0336 - val_loss: 0.1744 - lr: 4.7101e-05\n",
      "Epoch 162/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0336 - val_loss: 0.1750 - lr: 4.7101e-05\n",
      "Epoch 163/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0334 - val_loss: 0.1748 - lr: 4.2391e-05\n",
      "Epoch 164/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0334 - val_loss: 0.1754 - lr: 4.2391e-05\n",
      "Epoch 165/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0334 - val_loss: 0.1753 - lr: 4.2391e-05\n",
      "Epoch 166/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0333 - val_loss: 0.1757 - lr: 4.2391e-05\n",
      "Epoch 167/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0333 - val_loss: 0.1759 - lr: 4.2391e-05\n",
      "Epoch 168/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0332 - val_loss: 0.1759 - lr: 3.8152e-05\n",
      "Epoch 169/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0331 - val_loss: 0.1761 - lr: 3.8152e-05\n",
      "Epoch 170/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0331 - val_loss: 0.1764 - lr: 3.8152e-05\n",
      "Epoch 171/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0330 - val_loss: 0.1766 - lr: 3.8152e-05\n",
      "Epoch 172/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0330 - val_loss: 0.1763 - lr: 3.8152e-05\n",
      "Epoch 173/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0329 - val_loss: 0.1763 - lr: 3.4337e-05\n",
      "Epoch 174/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0328 - val_loss: 0.1770 - lr: 3.4337e-05\n",
      "Epoch 175/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0328 - val_loss: 0.1765 - lr: 3.4337e-05\n",
      "Epoch 176/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0328 - val_loss: 0.1775 - lr: 3.4337e-05\n",
      "Epoch 177/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0328 - val_loss: 0.1775 - lr: 3.4337e-05\n",
      "Epoch 178/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0327 - val_loss: 0.1776 - lr: 3.0903e-05\n",
      "Epoch 179/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0326 - val_loss: 0.1777 - lr: 3.0903e-05\n",
      "Epoch 180/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0326 - val_loss: 0.1775 - lr: 3.0903e-05\n",
      "Epoch 181/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0326 - val_loss: 0.1781 - lr: 3.0903e-05\n",
      "Epoch 182/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0325 - val_loss: 0.1780 - lr: 3.0903e-05\n",
      "Epoch 183/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0324 - val_loss: 0.1783 - lr: 2.7813e-05\n",
      "Epoch 184/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0324 - val_loss: 0.1783 - lr: 2.7813e-05\n",
      "Epoch 185/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0324 - val_loss: 0.1785 - lr: 2.7813e-05\n",
      "Epoch 186/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0324 - val_loss: 0.1783 - lr: 2.7813e-05\n",
      "Epoch 187/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0323 - val_loss: 0.1784 - lr: 2.7813e-05\n",
      "Epoch 188/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0323 - val_loss: 0.1787 - lr: 2.5032e-05\n",
      "Epoch 189/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0322 - val_loss: 0.1787 - lr: 2.5032e-05\n",
      "Epoch 190/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0322 - val_loss: 0.1793 - lr: 2.5032e-05\n",
      "Epoch 191/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0322 - val_loss: 0.1791 - lr: 2.5032e-05\n",
      "Epoch 192/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0322 - val_loss: 0.1791 - lr: 2.5032e-05\n",
      "Epoch 193/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0321 - val_loss: 0.1791 - lr: 2.2528e-05\n",
      "Epoch 194/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0321 - val_loss: 0.1793 - lr: 2.2528e-05\n",
      "Epoch 195/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0320 - val_loss: 0.1794 - lr: 2.2528e-05\n",
      "Epoch 196/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0320 - val_loss: 0.1793 - lr: 2.2528e-05\n",
      "Epoch 197/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0320 - val_loss: 0.1796 - lr: 2.2528e-05\n",
      "Epoch 198/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0319 - val_loss: 0.1795 - lr: 2.0276e-05\n",
      "Epoch 199/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0319 - val_loss: 0.1794 - lr: 2.0276e-05\n",
      "Epoch 200/200\n",
      "1279/1279 [==============================] - 3s 2ms/step - loss: 0.0319 - val_loss: 0.1796 - lr: 2.0276e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff05c3d7650>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 设置 EarlyStopping 回调函数，如果验证集的损失不再改善，则停止训练\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.9, patience=5)\n",
    "model.fit(   # 根据情况调整参数\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    validation_data=(val_features, val_labels),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试模型训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 MSE:0.0468\n",
      "y1 MSE:2771.0000 ------ 2771\n"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(test_features)\n",
    "print(\"y1 MSE:%.4f\" % mean_squared_error(test_labels, test_preds))\n",
    "print(\"y1 MSE:%.4f\" % len(test_labels), '------',len(test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型部署\n",
    "\n",
    "误差补偿模型的部署路径为<I>v1/models/slot0/versions/<版本号>/</I> ，且版本号必须为数字。注意，tensorflow-serving在加载模型的时候会自动加载版本号最高的模型，并卸载低版本号的模型。因此，每次部署新部署模型时需要递增版本号。由于我们的系统已经预置了一个低精度版本的模型，并且将版本号设置为1，所以用户在部署自定义模型时应当至少将版本号设置为2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 09:42:37.590323: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /models/slot0/1111111111111/assets\n"
     ]
    }
   ],
   "source": [
    "model_version = 1111111111111\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    f'/models/slot0/{model_version}/', # v1/models/slot0/为tensorflow-serving的模型根目录\n",
    "    overwrite=True,\n",
    "    include_optimizer=True,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，tensorflow-serving卸载旧版本模型并加载新版本模型的过程往往需要数十秒的时间，在次期间对模型发送请求会得到“Servable not found for request”的错误。用户可以使用<I>docker logs adjustment-serving-container</I>查看是否已经加载完毕。\n",
    "\n",
    "接下来我们测试是否部署成功："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"inputs\": [[1.4158, 2.9711, 10.7935, 7.5279, 2.3352, 8.1042, 2.3096, 3.3367, 11.8639, 12.7142, 1.8581, 0.3898, 19.8309, 19.771, 0.0001, 1.7768, 171.764, 1434.24]]}\n",
      "{'outputs': [[0.293064177, -0.683718443, 0.314387083, -0.135694787, -0.841075361, 0.85469383, 0.470254153, 1.90548074]]}\n",
      "{'outputs': [[0.331511, -0.932553, 0.285048, -0.1435, -0.833982, 0.767568, 0.463969, 1.9048]]\n",
      "y1 MSE:0.0090\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "test_features=pd.DataFrame(test_features)\n",
    "req_data = json.dumps({\n",
    "            'inputs': test_features.values[:1].tolist()\n",
    "        })  \n",
    "print(req_data)\n",
    "response = requests.post(f'http://fireeye-test-model-container:8501/v1/models/slot0/versions/{model_version}:predict', # 根据部署地址填写\n",
    "                         data=req_data,\n",
    "                         headers={\"content-type\": \"application/json\"})\n",
    "if response.status_code != 200:\n",
    "    raise RuntimeError('Request tf-serving failed: ' + response.text)\n",
    "resp_data = json.loads(response.text)    \n",
    "if 'outputs' not in resp_data \\\n",
    "                    or type(resp_data['outputs']) is not list:\n",
    "    raise ValueError('Malformed tf-serving response')\n",
    "\n",
    "print(resp_data)\n",
    "print(\"{'outputs':\",test_labels.values[:1].tolist())\n",
    "\n",
    "print(\"y1 MSE:%.4f\" % mean_squared_error(test_labels.values[:1].tolist(), resp_data['outputs']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试成功之后，用户需要在web页面配置相关任务的服务地址，地址的格式为：<I>“http://fireeye-test-model-container:8501/v1/models/slot0/versions/<版本号>:predict ”</I>。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "regression.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
