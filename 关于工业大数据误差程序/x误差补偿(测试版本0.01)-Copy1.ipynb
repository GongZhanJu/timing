{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9774d2d4-2fa6-40d2-9d2f-f59e23c00eba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 加载训练数据\n",
    "train_dataset = pd.read_csv('./adjustments.tsv',\n",
    "                          sep='\\t',\n",
    "                          skipinitialspace=True)\n",
    "# 加载测试数据     ！！！ 测试数据集为真实值，不能进行调整，否则将会导致实际模型测试结果和真实预测结果存在偏差，使得最终加工的作品和预期不一致\n",
    "test_dataset = pd.read_csv('./test_adjustments.tsv',\n",
    "                          sep='\\t',\n",
    "                          skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55ff9bac-642c-46ef-a1d5-dcfe243e1261",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>特征0</th>\n",
       "      <th>特征1</th>\n",
       "      <th>特征2</th>\n",
       "      <th>特征3</th>\n",
       "      <th>特征4</th>\n",
       "      <th>特征5</th>\n",
       "      <th>特征6</th>\n",
       "      <th>特征7</th>\n",
       "      <th>特征8</th>\n",
       "      <th>特征9</th>\n",
       "      <th>...</th>\n",
       "      <th>特征16</th>\n",
       "      <th>特征17</th>\n",
       "      <th>补偿0</th>\n",
       "      <th>补偿1</th>\n",
       "      <th>补偿2</th>\n",
       "      <th>补偿3</th>\n",
       "      <th>补偿4</th>\n",
       "      <th>补偿5</th>\n",
       "      <th>补偿6</th>\n",
       "      <th>补偿7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4158</td>\n",
       "      <td>2.9711</td>\n",
       "      <td>10.7935</td>\n",
       "      <td>7.5279</td>\n",
       "      <td>2.3352</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>2.3096</td>\n",
       "      <td>3.3367</td>\n",
       "      <td>11.8639</td>\n",
       "      <td>12.7142</td>\n",
       "      <td>...</td>\n",
       "      <td>171.764</td>\n",
       "      <td>1434.24</td>\n",
       "      <td>0.331511</td>\n",
       "      <td>-0.932553</td>\n",
       "      <td>0.285048</td>\n",
       "      <td>-0.1435</td>\n",
       "      <td>-0.833982</td>\n",
       "      <td>0.767568</td>\n",
       "      <td>0.463969</td>\n",
       "      <td>1.904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6280</td>\n",
       "      <td>1.8616</td>\n",
       "      <td>10.1770</td>\n",
       "      <td>7.4684</td>\n",
       "      <td>2.1915</td>\n",
       "      <td>8.5945</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>2.9661</td>\n",
       "      <td>11.5816</td>\n",
       "      <td>12.2487</td>\n",
       "      <td>...</td>\n",
       "      <td>185.824</td>\n",
       "      <td>1469.19</td>\n",
       "      <td>0.894066</td>\n",
       "      <td>-0.446796</td>\n",
       "      <td>0.058519</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.715252</td>\n",
       "      <td>0.999105</td>\n",
       "      <td>0.988844</td>\n",
       "      <td>0.689742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9648</td>\n",
       "      <td>1.8103</td>\n",
       "      <td>10.1682</td>\n",
       "      <td>5.9705</td>\n",
       "      <td>2.0629</td>\n",
       "      <td>6.5349</td>\n",
       "      <td>2.8694</td>\n",
       "      <td>3.1185</td>\n",
       "      <td>11.7464</td>\n",
       "      <td>12.2074</td>\n",
       "      <td>...</td>\n",
       "      <td>187.576</td>\n",
       "      <td>1540.76</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.460716</td>\n",
       "      <td>0.997809</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.723031</td>\n",
       "      <td>0.992935</td>\n",
       "      <td>0.682903</td>\n",
       "      <td>0.749580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7119</td>\n",
       "      <td>1.6221</td>\n",
       "      <td>10.1487</td>\n",
       "      <td>6.8678</td>\n",
       "      <td>2.0694</td>\n",
       "      <td>6.8806</td>\n",
       "      <td>1.5791</td>\n",
       "      <td>2.3003</td>\n",
       "      <td>11.5545</td>\n",
       "      <td>12.0659</td>\n",
       "      <td>...</td>\n",
       "      <td>189.938</td>\n",
       "      <td>1498.29</td>\n",
       "      <td>0.998794</td>\n",
       "      <td>-0.862448</td>\n",
       "      <td>0.329694</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.891745</td>\n",
       "      <td>0.015078</td>\n",
       "      <td>0.997127</td>\n",
       "      <td>0.984439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3797</td>\n",
       "      <td>1.6852</td>\n",
       "      <td>10.9601</td>\n",
       "      <td>5.0035</td>\n",
       "      <td>3.1659</td>\n",
       "      <td>5.9471</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>2.6402</td>\n",
       "      <td>11.7458</td>\n",
       "      <td>12.7041</td>\n",
       "      <td>...</td>\n",
       "      <td>181.275</td>\n",
       "      <td>1465.11</td>\n",
       "      <td>0.442893</td>\n",
       "      <td>-0.990703</td>\n",
       "      <td>-0.151400</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.617880</td>\n",
       "      <td>-0.506397</td>\n",
       "      <td>-0.997502</td>\n",
       "      <td>0.376126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      特征0     特征1      特征2     特征3     特征4     特征5     特征6     特征7      特征8  \\\n",
       "0  1.4158  2.9711  10.7935  7.5279  2.3352  8.1042  2.3096  3.3367  11.8639   \n",
       "1  0.6280  1.8616  10.1770  7.4684  2.1915  8.5945  0.1379  2.9661  11.5816   \n",
       "2  0.9648  1.8103  10.1682  5.9705  2.0629  6.5349  2.8694  3.1185  11.7464   \n",
       "3  0.7119  1.6221  10.1487  6.8678  2.0694  6.8806  1.5791  2.3003  11.5545   \n",
       "4  0.3797  1.6852  10.9601  5.0035  3.1659  5.9471  0.0858  2.6402  11.7458   \n",
       "\n",
       "       特征9  ...     特征16     特征17       补偿0       补偿1       补偿2     补偿3  \\\n",
       "0  12.7142  ...  171.764  1434.24  0.331511 -0.932553  0.285048 -0.1435   \n",
       "1  12.2487  ...  185.824  1469.19  0.894066 -0.446796  0.058519 -0.4624   \n",
       "2  12.2074  ...  187.576  1540.76  0.999982  0.460716  0.997809 -0.4624   \n",
       "3  12.0659  ...  189.938  1498.29  0.998794 -0.862448  0.329694 -0.4624   \n",
       "4  12.7041  ...  181.275  1465.11  0.442893 -0.990703 -0.151400  0.2245   \n",
       "\n",
       "        补偿4       补偿5       补偿6       补偿7  \n",
       "0 -0.833982  0.767568  0.463969  1.904800  \n",
       "1  0.715252  0.999105  0.988844  0.689742  \n",
       "2  0.723031  0.992935  0.682903  0.749580  \n",
       "3  0.891745  0.015078  0.997127  0.984439  \n",
       "4  0.617880 -0.506397 -0.997502  0.376126  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "dataset = train_dataset.copy()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf23b092-bf2a-4921-a67d-b183f589d185",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 26)\n",
      "(2998, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>特征0</th>\n",
       "      <th>特征1</th>\n",
       "      <th>特征2</th>\n",
       "      <th>特征3</th>\n",
       "      <th>特征4</th>\n",
       "      <th>特征5</th>\n",
       "      <th>特征6</th>\n",
       "      <th>特征7</th>\n",
       "      <th>特征8</th>\n",
       "      <th>特征9</th>\n",
       "      <th>...</th>\n",
       "      <th>特征16</th>\n",
       "      <th>特征17</th>\n",
       "      <th>补偿0</th>\n",
       "      <th>补偿1</th>\n",
       "      <th>补偿2</th>\n",
       "      <th>补偿3</th>\n",
       "      <th>补偿4</th>\n",
       "      <th>补偿5</th>\n",
       "      <th>补偿6</th>\n",
       "      <th>补偿7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.008837</td>\n",
       "      <td>2.018090</td>\n",
       "      <td>10.501365</td>\n",
       "      <td>6.980399</td>\n",
       "      <td>2.991379</td>\n",
       "      <td>6.986831</td>\n",
       "      <td>1.983407</td>\n",
       "      <td>2.994988</td>\n",
       "      <td>11.739018</td>\n",
       "      <td>12.600981</td>\n",
       "      <td>...</td>\n",
       "      <td>172.242767</td>\n",
       "      <td>1474.643939</td>\n",
       "      <td>0.525279</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.250365</td>\n",
       "      <td>0.010439</td>\n",
       "      <td>-0.269551</td>\n",
       "      <td>0.524640</td>\n",
       "      <td>0.222959</td>\n",
       "      <td>0.561442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.573132</td>\n",
       "      <td>0.578950</td>\n",
       "      <td>0.286296</td>\n",
       "      <td>1.134771</td>\n",
       "      <td>0.574884</td>\n",
       "      <td>1.142659</td>\n",
       "      <td>1.144038</td>\n",
       "      <td>0.577503</td>\n",
       "      <td>0.128526</td>\n",
       "      <td>0.196063</td>\n",
       "      <td>...</td>\n",
       "      <td>12.198489</td>\n",
       "      <td>54.894522</td>\n",
       "      <td>0.399656</td>\n",
       "      <td>0.699439</td>\n",
       "      <td>0.497970</td>\n",
       "      <td>0.330564</td>\n",
       "      <td>0.672572</td>\n",
       "      <td>0.494762</td>\n",
       "      <td>0.716798</td>\n",
       "      <td>1.124138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000300</td>\n",
       "      <td>5.000700</td>\n",
       "      <td>2.000400</td>\n",
       "      <td>5.000800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.000700</td>\n",
       "      <td>11.155400</td>\n",
       "      <td>11.750600</td>\n",
       "      <td>...</td>\n",
       "      <td>121.207000</td>\n",
       "      <td>1377.980000</td>\n",
       "      <td>-0.950279</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-1.240660</td>\n",
       "      <td>-0.533200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.863171</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-3.354270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.518975</td>\n",
       "      <td>1.506825</td>\n",
       "      <td>10.257700</td>\n",
       "      <td>6.033325</td>\n",
       "      <td>2.501750</td>\n",
       "      <td>6.004525</td>\n",
       "      <td>1.001925</td>\n",
       "      <td>2.509325</td>\n",
       "      <td>11.668100</td>\n",
       "      <td>12.485025</td>\n",
       "      <td>...</td>\n",
       "      <td>165.299500</td>\n",
       "      <td>1437.355000</td>\n",
       "      <td>0.279705</td>\n",
       "      <td>-0.693314</td>\n",
       "      <td>-0.096690</td>\n",
       "      <td>-0.143500</td>\n",
       "      <td>-0.847946</td>\n",
       "      <td>0.307439</td>\n",
       "      <td>-0.492667</td>\n",
       "      <td>-0.107755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.014300</td>\n",
       "      <td>2.042550</td>\n",
       "      <td>10.497350</td>\n",
       "      <td>6.962800</td>\n",
       "      <td>2.955600</td>\n",
       "      <td>6.956750</td>\n",
       "      <td>1.985950</td>\n",
       "      <td>2.986350</td>\n",
       "      <td>11.762100</td>\n",
       "      <td>12.644450</td>\n",
       "      <td>...</td>\n",
       "      <td>174.141000</td>\n",
       "      <td>1456.825000</td>\n",
       "      <td>0.617192</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>0.250205</td>\n",
       "      <td>-0.143500</td>\n",
       "      <td>-0.550868</td>\n",
       "      <td>0.709205</td>\n",
       "      <td>0.495988</td>\n",
       "      <td>0.390784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.488325</td>\n",
       "      <td>2.523625</td>\n",
       "      <td>10.750450</td>\n",
       "      <td>7.944000</td>\n",
       "      <td>3.481775</td>\n",
       "      <td>7.957600</td>\n",
       "      <td>2.953800</td>\n",
       "      <td>3.496425</td>\n",
       "      <td>11.835725</td>\n",
       "      <td>12.747475</td>\n",
       "      <td>...</td>\n",
       "      <td>181.637250</td>\n",
       "      <td>1496.542500</td>\n",
       "      <td>0.860046</td>\n",
       "      <td>0.696679</td>\n",
       "      <td>0.604809</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.370577</td>\n",
       "      <td>0.905374</td>\n",
       "      <td>0.877369</td>\n",
       "      <td>1.159790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.998200</td>\n",
       "      <td>2.998600</td>\n",
       "      <td>10.999300</td>\n",
       "      <td>8.996300</td>\n",
       "      <td>3.999400</td>\n",
       "      <td>8.998500</td>\n",
       "      <td>3.996200</td>\n",
       "      <td>3.999400</td>\n",
       "      <td>11.950100</td>\n",
       "      <td>12.933200</td>\n",
       "      <td>...</td>\n",
       "      <td>193.145000</td>\n",
       "      <td>1785.880000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.832180</td>\n",
       "      <td>0.632400</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.585870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               特征0          特征1          特征2          特征3          特征4  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000   \n",
       "mean      1.008837     2.018090    10.501365     6.980399     2.991379   \n",
       "std       0.573132     0.578950     0.286296     1.134771     0.574884   \n",
       "min       0.000500     1.000000    10.000300     5.000700     2.000400   \n",
       "25%       0.518975     1.506825    10.257700     6.033325     2.501750   \n",
       "50%       1.014300     2.042550    10.497350     6.962800     2.955600   \n",
       "75%       1.488325     2.523625    10.750450     7.944000     3.481775   \n",
       "max       1.998200     2.998600    10.999300     8.996300     3.999400   \n",
       "\n",
       "               特征5          特征6          特征7          特征8          特征9  ...  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000  ...   \n",
       "mean      6.986831     1.983407     2.994988    11.739018    12.600981  ...   \n",
       "std       1.142659     1.144038     0.577503     0.128526     0.196063  ...   \n",
       "min       5.000800     0.000100     2.000700    11.155400    11.750600  ...   \n",
       "25%       6.004525     1.001925     2.509325    11.668100    12.485025  ...   \n",
       "50%       6.956750     1.985950     2.986350    11.762100    12.644450  ...   \n",
       "75%       7.957600     2.953800     3.496425    11.835725    12.747475  ...   \n",
       "max       8.998500     3.996200     3.999400    11.950100    12.933200  ...   \n",
       "\n",
       "              特征16         特征17          补偿0          补偿1          补偿2  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000   \n",
       "mean    172.242767  1474.643939     0.525279     0.005173     0.250365   \n",
       "std      12.198489    54.894522     0.399656     0.699439     0.497970   \n",
       "min     121.207000  1377.980000    -0.950279    -0.999992    -1.240660   \n",
       "25%     165.299500  1437.355000     0.279705    -0.693314    -0.096690   \n",
       "50%     174.141000  1456.825000     0.617192     0.015394     0.250205   \n",
       "75%     181.637250  1496.542500     0.860046     0.696679     0.604809   \n",
       "max     193.145000  1785.880000     1.000000     1.000000     1.832180   \n",
       "\n",
       "               补偿3          补偿4          补偿5          补偿6          补偿7  \n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000  \n",
       "mean      0.010439    -0.269551     0.524640     0.222959     0.561442  \n",
       "std       0.330564     0.672572     0.494762     0.716798     1.124138  \n",
       "min      -0.533200    -1.000000    -0.863171    -0.999998    -3.354270  \n",
       "25%      -0.143500    -0.847946     0.307439    -0.492667    -0.107755  \n",
       "50%      -0.143500    -0.550868     0.709205     0.495988     0.390784  \n",
       "75%       0.224500     0.370577     0.905374     0.877369     1.159790  \n",
       "max       0.632400     0.999997     0.999997     1.000000     6.585870  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds=dataset.dropna()\n",
    "w=['特征'+str(i) for i in range(18)]\n",
    "ds=ds.drop_duplicates(w)\n",
    "print(ds.shape)\n",
    "ds.describe()\n",
    "\n",
    "ds1=test_dataset.dropna()\n",
    "w=['特征'+str(i) for i in range(18)]\n",
    "ds1=ds1.drop_duplicates(w)\n",
    "print(ds1.shape)\n",
    "ds1.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9880db93-b4a2-4d05-b31f-2170c37e4c81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均值 [   1.       1.998   10.5      6.998    3.001    6.995    2.005    2.999\n",
      "   11.736   12.604    1.768    0.649   19.417   19.598    3.349   17.983\n",
      "  172.057 1475.054]\n",
      "方差 [   0.332    0.332    0.083    1.327    0.333    1.337    1.331    0.336\n",
      "    0.017    0.039    0.074    0.213    0.163    0.069   10.31   525.861\n",
      "  149.833 3136.668]\n"
     ]
    }
   ],
   "source": [
    "average = np.average(ds.values[:,:18], axis=0)\n",
    "variance = np.var(ds.values[:,:18], axis=0)\n",
    "print('均值', average)\n",
    "print('方差', variance)\n",
    "#\n",
    "#{\"inputs\": [[1.4158, 2.9711, 10.7935, 7.5279, 2.3352, 8.1042, 2.3096, 3.3367, 11.8639,\n",
    "#             12.7142, 1.8581, 0.3898, 19.8309, 19.771, 0.0001, 1.7768, 171.764, 1434.24]]}\n",
    "\n",
    "#{'outputs': [[0.31156069, -0.752204835, 0.293267965, -0.0997265279, -0.819844842, 0.861867249,\n",
    "#              0.451463282, 1.86217737]]}\n",
    "\n",
    "#{'outputs': [[0.331511, -0.932553, 0.285048, -0.1435, -0.833982, 0.767568, 0.463969, 1.9048]]\n",
    "#y1 MSE:0.0057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b9fe5dc-c75c-4487-8662-e50c62cd34e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45261, 26)\n",
      "(45261, 26)\n",
      "特征0        0.327301\n",
      "特征1        0.328594\n",
      "特征2        0.081372\n",
      "特征3        1.289660\n",
      "特征4        0.317790\n",
      "特征5        1.284746\n",
      "特征6        1.317164\n",
      "特征7        0.327473\n",
      "特征8        0.014106\n",
      "特征9        0.032020\n",
      "特征10       0.054281\n",
      "特征11       0.190948\n",
      "特征12       0.106917\n",
      "特征13       0.049092\n",
      "特征14       8.075554\n",
      "特征15     298.280252\n",
      "特征16     117.274083\n",
      "特征17    2204.701086\n",
      "补偿0        0.150359\n",
      "补偿1        0.501306\n",
      "补偿2        0.238429\n",
      "补偿3        0.100420\n",
      "补偿4        0.408649\n",
      "补偿5        0.250138\n",
      "补偿6        0.509829\n",
      "补偿7        1.213095\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#计算前18列特征的均值和标准差\n",
    "# mean = np.mean(ds.values[:, :18], axis=0)\n",
    "# std = np.std(ds.values[:, :18], axis=0)\n",
    "\n",
    "mean = np.mean(test_dataset.values[:, :18], axis=0)\n",
    "std = np.std(test_dataset.values[:, :18], axis=0)\n",
    "# mean1 = np.mean(ds1.values[:, :18], axis=0)\n",
    "# std1 = np.std(ds1.values[:, :18], axis=0)\n",
    "# 定义上限和下限\n",
    "upper_limit = mean + 3 * std\n",
    "lower_limit = mean - 3 * std\n",
    "\n",
    "\n",
    "\n",
    "# upper_limit1 = mean1 + 3 * std\n",
    "# lower_limit1 = mean1 - 3 * std\n",
    "# 使用布尔索引删除超出上限和下限的行\n",
    "cleaned_ds = ds[~((ds.values[:, :18] > upper_limit) | (ds.values[:, :18] < lower_limit)).any(axis=1)]\n",
    "# cleaned_ds=cleaned_ds.drop(cleaned_ds[(cleaned_ds['补偿7']>-2)].sample(frac=0.2,random_state=0).index)\n",
    "\n",
    "# cleaned_ds=cleaned_ds.drop(cleaned_ds[(cleaned_ds['补偿3']>-0.5)].sample(frac=0.2,random_state=0).index)\n",
    "# cleaned_ds=cleaned_ds.drop(cleaned_ds[(cleaned_ds['特征9']>12)].sample(frac=0.2,random_state=0).index)\n",
    "# cleaned_ds=cleaned_ds.drop(cleaned_ds[(cleaned_ds['特征10']>1.75)].sample(frac=0.2,random_state=0).index)\n",
    "# cleaned_ds=cleaned_ds.drop(cleaned_ds[(cleaned_ds['特征12']>18)].sample(frac=0.2,random_state=0).index)\n",
    "# cleaned_ds=cleaned_ds.drop(cleaned_ds[(cleaned_ds['特征13']>18.75)].sample(frac=0.2,random_state=0).index)\n",
    "# cleaned_ds1 = ds1[~((ds1.values[:, :18] > upper_limit1) | (ds1.values[:, :18] < lower_limit1)).any(axis=1)]\n",
    "\n",
    "# 打印清理后数据集的形状\n",
    "print(cleaned_ds.shape)\n",
    "\n",
    "print(cleaned_ds.shape)\n",
    "cleaned_ds.describe()\n",
    "\n",
    "cleaned_ds=pd.DataFrame(cleaned_ds)\n",
    "variance=np.var(cleaned_ds)\n",
    "\n",
    "print(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e3eeff0-7786-4933-ae06-3de78efaaa4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 18) (50000, 8)\n",
      "(2500, 18) (2500, 8)\n",
      "(2998, 18) (2998, 8)\n"
     ]
    }
   ],
   "source": [
    "train_ds=dataset.sample(frac=1,random_state=0)\n",
    "# val_ds=dataset.drop(train_ds.index)\n",
    "val_ds=dataset.sample(frac=0.05,random_state=0)\n",
    "\n",
    "# train_ds=cleaned_ds.sample(frac=0.9,random_state=0)\n",
    "# val_ds=cleaned_ds.drop(train_ds.index)\n",
    "\n",
    "#训练集\n",
    "train_features=train_ds.values[:,:18]\n",
    "train_labels=train_ds.values[:,18:]\n",
    "\n",
    "#验证集\n",
    "val_features=val_ds.values[:,:18]\n",
    "val_labels=val_ds.values[:,18:]\n",
    "\n",
    "# 测试集\n",
    "test_features=test_dataset.values[:,:18]\n",
    "test_labels=test_dataset.values[:,18:]\n",
    "\n",
    "print(train_features.shape,train_labels.shape)\n",
    "print(val_features.shape,val_labels.shape)\n",
    "print(test_features.shape,test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3c8e7b0-992e-4600-b098-f4fcfd4c23c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from numpy import array\n",
    "from numpy.random import uniform\n",
    "from numpy import hstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "976c17ce-944d-4bb6-b843-246da048f49c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------构建模型及训练-----------------\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d9269ea-8503-41b3-b510-090df5d2777a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_2 (Normalizat  (None, 18)               37        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 300)               5700      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 8)                 2408      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 369,345\n",
      "Trainable params: 369,308\n",
      "Non-trainable params: 37\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 创建Normalizaiton层\n",
    "normalizer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "# # 计算并设置归一化参数\n",
    "#normalizer.adapt(train_features)\n",
    "# 构建模型\n",
    "normalizer.adapt(val_features)\n",
    "model = tf.keras.Sequential([\n",
    "    normalizer,  # 归一化层作为第一层\n",
    "    layers.Dense(300, activation=\"gelu\", input_dim=train_features.shape[1]),\n",
    "    layers.Dense(300, activation=\"gelu\"),\n",
    "    layers.Dense(300, activation=\"gelu\"),\n",
    "    layers.Dense(300, activation=\"gelu\"),\n",
    "    layers.Dense(300, activation=\"gelu\"),\n",
    "    # layers.Dropout(0.3),\n",
    "    layers.Dense(train_labels.shape[1])\n",
    "    # normalizer,  # 归一化层作为第一层\n",
    "    # layers.Dense(100, activation=\"relu\"),\n",
    "    # layers.Dense(train_labels.shape[1])\n",
    "])\n",
    "\n",
    "# optimizers=optimizers.Nadam(learning_rate=0.01)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")  # 根据情况调整参数\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0fb0d64-f9e3-4191-91e3-334ee0d3cdeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0013 - val_loss: 0.0013 - lr: 2.8243e-04\n",
      "Epoch 2/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0014 - val_loss: 0.0014 - lr: 2.8243e-04\n",
      "Epoch 3/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0013 - val_loss: 0.0013 - lr: 2.8243e-04\n",
      "Epoch 4/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0014 - val_loss: 0.0014 - lr: 2.8243e-04\n",
      "Epoch 5/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0014 - val_loss: 0.0014 - lr: 2.8243e-04\n",
      "Epoch 6/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0013 - val_loss: 0.0013 - lr: 2.8243e-04\n",
      "Epoch 7/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0012 - val_loss: 0.0011 - lr: 2.5419e-04\n",
      "Epoch 8/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 2.5419e-04\n",
      "Epoch 9/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 2.5419e-04\n",
      "Epoch 10/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0012 - val_loss: 0.0011 - lr: 2.5419e-04\n",
      "Epoch 11/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 2.5419e-04\n",
      "Epoch 12/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 2.5419e-04\n",
      "Epoch 13/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0010 - val_loss: 0.0010 - lr: 2.2877e-04\n",
      "Epoch 14/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.1134e-04 - val_loss: 9.7019e-04 - lr: 2.2877e-04\n",
      "Epoch 15/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.4968e-04 - val_loss: 9.3206e-04 - lr: 2.2877e-04\n",
      "Epoch 16/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.5771e-04 - val_loss: 9.3719e-04 - lr: 2.2877e-04\n",
      "Epoch 17/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.6179e-04 - val_loss: 9.5425e-04 - lr: 2.2877e-04\n",
      "Epoch 18/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.4946e-04 - val_loss: 9.4614e-04 - lr: 2.2877e-04\n",
      "Epoch 19/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.4652e-04 - val_loss: 8.8136e-04 - lr: 2.2877e-04\n",
      "Epoch 20/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.3722e-04 - val_loss: 7.7657e-04 - lr: 2.0589e-04\n",
      "Epoch 21/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 7.6930e-04 - val_loss: 7.8101e-04 - lr: 2.0589e-04\n",
      "Epoch 22/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2209e-04 - val_loss: 7.7514e-04 - lr: 2.0589e-04\n",
      "Epoch 23/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.4308e-04 - val_loss: 8.0714e-04 - lr: 2.0589e-04\n",
      "Epoch 24/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.0051e-04 - val_loss: 8.4210e-04 - lr: 2.0589e-04\n",
      "Epoch 25/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 7.8487e-04 - val_loss: 8.9018e-04 - lr: 2.0589e-04\n",
      "Epoch 26/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 7.1945e-04 - val_loss: 6.8512e-04 - lr: 1.8530e-04\n",
      "Epoch 27/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 6.5980e-04 - val_loss: 6.7050e-04 - lr: 1.8530e-04\n",
      "Epoch 28/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 6.7927e-04 - val_loss: 6.7823e-04 - lr: 1.8530e-04\n",
      "Epoch 29/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 6.9298e-04 - val_loss: 6.9157e-04 - lr: 1.8530e-04\n",
      "Epoch 30/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 6.8742e-04 - val_loss: 6.7352e-04 - lr: 1.8530e-04\n",
      "Epoch 31/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 6.7729e-04 - val_loss: 6.3713e-04 - lr: 1.8530e-04\n",
      "Epoch 32/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 6.7817e-04 - val_loss: 6.6650e-04 - lr: 1.8530e-04\n",
      "Epoch 33/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 6.0971e-04 - val_loss: 5.5849e-04 - lr: 1.6677e-04\n",
      "Epoch 34/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 5.6431e-04 - val_loss: 5.7916e-04 - lr: 1.6677e-04\n",
      "Epoch 35/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 5.8188e-04 - val_loss: 6.0001e-04 - lr: 1.6677e-04\n",
      "Epoch 36/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 5.9088e-04 - val_loss: 5.6086e-04 - lr: 1.6677e-04\n",
      "Epoch 37/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 5.9290e-04 - val_loss: 5.5807e-04 - lr: 1.6677e-04\n",
      "Epoch 38/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 5.8138e-04 - val_loss: 5.7033e-04 - lr: 1.6677e-04\n",
      "Epoch 39/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 5.2323e-04 - val_loss: 4.9581e-04 - lr: 1.5009e-04\n",
      "Epoch 40/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 4.8810e-04 - val_loss: 4.9694e-04 - lr: 1.5009e-04\n",
      "Epoch 41/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 5.0155e-04 - val_loss: 4.9307e-04 - lr: 1.5009e-04\n",
      "Epoch 42/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 5.0208e-04 - val_loss: 5.0094e-04 - lr: 1.5009e-04\n",
      "Epoch 43/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 5.0241e-04 - val_loss: 5.0125e-04 - lr: 1.5009e-04\n",
      "Epoch 44/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 4.6215e-04 - val_loss: 4.2523e-04 - lr: 1.3509e-04\n",
      "Epoch 45/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 4.2272e-04 - val_loss: 4.1651e-04 - lr: 1.3509e-04\n",
      "Epoch 46/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 4.3229e-04 - val_loss: 4.2116e-04 - lr: 1.3509e-04\n",
      "Epoch 47/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 4.4191e-04 - val_loss: 4.2024e-04 - lr: 1.3509e-04\n",
      "Epoch 48/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 4.3594e-04 - val_loss: 4.1616e-04 - lr: 1.3509e-04\n",
      "Epoch 49/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 4.3012e-04 - val_loss: 4.3030e-04 - lr: 1.3509e-04\n",
      "Epoch 50/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 3.9770e-04 - val_loss: 3.6541e-04 - lr: 1.2158e-04\n",
      "Epoch 51/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 3.6878e-04 - val_loss: 3.6275e-04 - lr: 1.2158e-04\n",
      "Epoch 52/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 3.7923e-04 - val_loss: 3.5881e-04 - lr: 1.2158e-04\n",
      "Epoch 53/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 3.8449e-04 - val_loss: 3.6585e-04 - lr: 1.2158e-04\n",
      "Epoch 54/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 3.7908e-04 - val_loss: 3.6265e-04 - lr: 1.2158e-04\n",
      "Epoch 55/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 3.4932e-04 - val_loss: 3.1532e-04 - lr: 1.0942e-04\n",
      "Epoch 56/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 3.2324e-04 - val_loss: 3.0688e-04 - lr: 1.0942e-04\n",
      "Epoch 57/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 3.3600e-04 - val_loss: 3.2121e-04 - lr: 1.0942e-04\n",
      "Epoch 58/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 3.3890e-04 - val_loss: 3.1992e-04 - lr: 1.0942e-04\n",
      "Epoch 59/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 3.3406e-04 - val_loss: 3.1355e-04 - lr: 1.0942e-04\n",
      "Epoch 60/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 3.3234e-04 - val_loss: 3.1618e-04 - lr: 1.0942e-04\n",
      "Epoch 61/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 3.0594e-04 - val_loss: 2.8602e-04 - lr: 9.8477e-05\n",
      "Epoch 62/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.8880e-04 - val_loss: 2.7757e-04 - lr: 9.8477e-05\n",
      "Epoch 63/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.9456e-04 - val_loss: 2.7681e-04 - lr: 9.8477e-05\n",
      "Epoch 64/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.9721e-04 - val_loss: 2.8849e-04 - lr: 9.8477e-05\n",
      "Epoch 65/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.9433e-04 - val_loss: 2.7612e-04 - lr: 9.8477e-05\n",
      "Epoch 66/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.7239e-04 - val_loss: 2.5288e-04 - lr: 8.8629e-05\n",
      "Epoch 67/200\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.5936e-04 - val_loss: 2.4576e-04 - lr: 8.8629e-05\n",
      "Epoch 68/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.6286e-04 - val_loss: 2.4314e-04 - lr: 8.8629e-05\n",
      "Epoch 69/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.6316e-04 - val_loss: 2.4926e-04 - lr: 8.8629e-05\n",
      "Epoch 70/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.6286e-04 - val_loss: 2.5129e-04 - lr: 8.8629e-05\n",
      "Epoch 71/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4581e-04 - val_loss: 2.2491e-04 - lr: 7.9766e-05\n",
      "Epoch 72/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3200e-04 - val_loss: 2.1380e-04 - lr: 7.9766e-05\n",
      "Epoch 73/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3586e-04 - val_loss: 2.2861e-04 - lr: 7.9766e-05\n",
      "Epoch 74/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3799e-04 - val_loss: 2.3115e-04 - lr: 7.9766e-05\n",
      "Epoch 75/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3682e-04 - val_loss: 2.2238e-04 - lr: 7.9766e-05\n",
      "Epoch 76/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3457e-04 - val_loss: 2.1422e-04 - lr: 7.9766e-05\n",
      "Epoch 77/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3441e-04 - val_loss: 2.2793e-04 - lr: 7.9766e-05\n",
      "Epoch 78/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.1931e-04 - val_loss: 1.9726e-04 - lr: 7.1790e-05\n",
      "Epoch 79/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0906e-04 - val_loss: 2.0178e-04 - lr: 7.1790e-05\n",
      "Epoch 80/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.1248e-04 - val_loss: 2.0406e-04 - lr: 7.1790e-05\n",
      "Epoch 81/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.1350e-04 - val_loss: 2.0101e-04 - lr: 7.1790e-05\n",
      "Epoch 82/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.1329e-04 - val_loss: 1.9696e-04 - lr: 7.1790e-05\n",
      "Epoch 83/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.9978e-04 - val_loss: 1.7937e-04 - lr: 6.4611e-05\n",
      "Epoch 84/200\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9116e-04 - val_loss: 1.7859e-04 - lr: 6.4611e-05\n",
      "Epoch 85/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.9361e-04 - val_loss: 1.8695e-04 - lr: 6.4611e-05\n",
      "Epoch 86/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.9386e-04 - val_loss: 1.8212e-04 - lr: 6.4611e-05\n",
      "Epoch 87/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.9385e-04 - val_loss: 1.8067e-04 - lr: 6.4611e-05\n",
      "Epoch 88/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.8295e-04 - val_loss: 1.6581e-04 - lr: 5.8150e-05\n",
      "Epoch 89/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.7631e-04 - val_loss: 1.6608e-04 - lr: 5.8150e-05\n",
      "Epoch 90/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.7782e-04 - val_loss: 1.6283e-04 - lr: 5.8150e-05\n",
      "Epoch 91/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.7861e-04 - val_loss: 1.6450e-04 - lr: 5.8150e-05\n",
      "Epoch 92/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.7770e-04 - val_loss: 1.6427e-04 - lr: 5.8150e-05\n",
      "Epoch 93/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6912e-04 - val_loss: 1.5046e-04 - lr: 5.2335e-05\n",
      "Epoch 94/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6316e-04 - val_loss: 1.5057e-04 - lr: 5.2335e-05\n",
      "Epoch 95/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6490e-04 - val_loss: 1.5588e-04 - lr: 5.2335e-05\n",
      "Epoch 96/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6545e-04 - val_loss: 1.5484e-04 - lr: 5.2335e-05\n",
      "Epoch 97/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6427e-04 - val_loss: 1.5121e-04 - lr: 5.2335e-05\n",
      "Epoch 98/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5622e-04 - val_loss: 1.4303e-04 - lr: 4.7101e-05\n",
      "Epoch 99/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5262e-04 - val_loss: 1.4090e-04 - lr: 4.7101e-05\n",
      "Epoch 100/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5303e-04 - val_loss: 1.4069e-04 - lr: 4.7101e-05\n",
      "Epoch 101/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5287e-04 - val_loss: 1.3999e-04 - lr: 4.7101e-05\n",
      "Epoch 102/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5261e-04 - val_loss: 1.4047e-04 - lr: 4.7101e-05\n",
      "Epoch 103/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4674e-04 - val_loss: 1.3235e-04 - lr: 4.2391e-05\n",
      "Epoch 104/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4287e-04 - val_loss: 1.3116e-04 - lr: 4.2391e-05\n",
      "Epoch 105/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4322e-04 - val_loss: 1.3128e-04 - lr: 4.2391e-05\n",
      "Epoch 106/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4360e-04 - val_loss: 1.3299e-04 - lr: 4.2391e-05\n",
      "Epoch 107/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4280e-04 - val_loss: 1.3007e-04 - lr: 4.2391e-05\n",
      "Epoch 108/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3765e-04 - val_loss: 1.2291e-04 - lr: 3.8152e-05\n",
      "Epoch 109/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3488e-04 - val_loss: 1.2270e-04 - lr: 3.8152e-05\n",
      "Epoch 110/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3555e-04 - val_loss: 1.2247e-04 - lr: 3.8152e-05\n",
      "Epoch 111/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3539e-04 - val_loss: 1.2272e-04 - lr: 3.8152e-05\n",
      "Epoch 112/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3429e-04 - val_loss: 1.2242e-04 - lr: 3.8152e-05\n",
      "Epoch 113/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3030e-04 - val_loss: 1.1621e-04 - lr: 3.4337e-05\n",
      "Epoch 114/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2779e-04 - val_loss: 1.1654e-04 - lr: 3.4337e-05\n",
      "Epoch 115/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2809e-04 - val_loss: 1.1526e-04 - lr: 3.4337e-05\n",
      "Epoch 116/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2777e-04 - val_loss: 1.1670e-04 - lr: 3.4337e-05\n",
      "Epoch 117/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2765e-04 - val_loss: 1.1453e-04 - lr: 3.4337e-05\n",
      "Epoch 118/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2367e-04 - val_loss: 1.1109e-04 - lr: 3.0903e-05\n",
      "Epoch 119/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2165e-04 - val_loss: 1.1052e-04 - lr: 3.0903e-05\n",
      "Epoch 120/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2173e-04 - val_loss: 1.1055e-04 - lr: 3.0903e-05\n",
      "Epoch 121/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2168e-04 - val_loss: 1.0988e-04 - lr: 3.0903e-05\n",
      "Epoch 122/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2102e-04 - val_loss: 1.0864e-04 - lr: 3.0903e-05\n",
      "Epoch 123/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2076e-04 - val_loss: 1.0828e-04 - lr: 3.0903e-05\n",
      "Epoch 124/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1762e-04 - val_loss: 1.0809e-04 - lr: 2.7813e-05\n",
      "Epoch 125/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1600e-04 - val_loss: 1.0423e-04 - lr: 2.7813e-05\n",
      "Epoch 126/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1585e-04 - val_loss: 1.0480e-04 - lr: 2.7813e-05\n",
      "Epoch 127/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1591e-04 - val_loss: 1.0286e-04 - lr: 2.7813e-05\n",
      "Epoch 128/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1518e-04 - val_loss: 1.0336e-04 - lr: 2.7813e-05\n",
      "Epoch 129/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1284e-04 - val_loss: 1.0041e-04 - lr: 2.5032e-05\n",
      "Epoch 130/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1139e-04 - val_loss: 1.0017e-04 - lr: 2.5032e-05\n",
      "Epoch 131/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1123e-04 - val_loss: 1.0039e-04 - lr: 2.5032e-05\n",
      "Epoch 132/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1109e-04 - val_loss: 9.9044e-05 - lr: 2.5032e-05\n",
      "Epoch 133/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1083e-04 - val_loss: 9.8691e-05 - lr: 2.5032e-05\n",
      "Epoch 134/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0843e-04 - val_loss: 9.7558e-05 - lr: 2.2528e-05\n",
      "Epoch 135/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0731e-04 - val_loss: 9.6807e-05 - lr: 2.2528e-05\n",
      "Epoch 136/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0700e-04 - val_loss: 9.6718e-05 - lr: 2.2528e-05\n",
      "Epoch 137/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0697e-04 - val_loss: 9.5904e-05 - lr: 2.2528e-05\n",
      "Epoch 138/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0668e-04 - val_loss: 9.5912e-05 - lr: 2.2528e-05\n",
      "Epoch 139/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0476e-04 - val_loss: 9.2754e-05 - lr: 2.0276e-05\n",
      "Epoch 140/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0370e-04 - val_loss: 9.3822e-05 - lr: 2.0276e-05\n",
      "Epoch 141/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0343e-04 - val_loss: 9.3040e-05 - lr: 2.0276e-05\n",
      "Epoch 142/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0336e-04 - val_loss: 9.2952e-05 - lr: 2.0276e-05\n",
      "Epoch 143/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0302e-04 - val_loss: 9.2436e-05 - lr: 2.0276e-05\n",
      "Epoch 144/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0136e-04 - val_loss: 9.0362e-05 - lr: 1.8248e-05\n",
      "Epoch 145/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0060e-04 - val_loss: 9.1212e-05 - lr: 1.8248e-05\n",
      "Epoch 146/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0043e-04 - val_loss: 9.0745e-05 - lr: 1.8248e-05\n",
      "Epoch 147/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0022e-04 - val_loss: 8.9954e-05 - lr: 1.8248e-05\n",
      "Epoch 148/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.9816e-05 - val_loss: 8.9425e-05 - lr: 1.8248e-05\n",
      "Epoch 149/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.8471e-05 - val_loss: 8.7660e-05 - lr: 1.6423e-05\n",
      "Epoch 150/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.7876e-05 - val_loss: 8.7351e-05 - lr: 1.6423e-05\n",
      "Epoch 151/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.7526e-05 - val_loss: 8.8150e-05 - lr: 1.6423e-05\n",
      "Epoch 152/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.7373e-05 - val_loss: 8.6581e-05 - lr: 1.6423e-05\n",
      "Epoch 153/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.7137e-05 - val_loss: 8.6771e-05 - lr: 1.6423e-05\n",
      "Epoch 154/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.6015e-05 - val_loss: 8.5218e-05 - lr: 1.4781e-05\n",
      "Epoch 155/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.5322e-05 - val_loss: 8.5288e-05 - lr: 1.4781e-05\n",
      "Epoch 156/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.5088e-05 - val_loss: 8.5226e-05 - lr: 1.4781e-05\n",
      "Epoch 157/200\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 9.4923e-05 - val_loss: 8.5265e-05 - lr: 1.4781e-05\n",
      "Epoch 158/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.4637e-05 - val_loss: 8.5260e-05 - lr: 1.4781e-05\n",
      "Epoch 159/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.3622e-05 - val_loss: 8.4090e-05 - lr: 1.3303e-05\n",
      "Epoch 160/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.3105e-05 - val_loss: 8.3367e-05 - lr: 1.3303e-05\n",
      "Epoch 161/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.2909e-05 - val_loss: 8.3814e-05 - lr: 1.3303e-05\n",
      "Epoch 162/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.2692e-05 - val_loss: 8.3049e-05 - lr: 1.3303e-05\n",
      "Epoch 163/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.2477e-05 - val_loss: 8.2820e-05 - lr: 1.3303e-05\n",
      "Epoch 164/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.1588e-05 - val_loss: 8.2198e-05 - lr: 1.1973e-05\n",
      "Epoch 165/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.1059e-05 - val_loss: 8.1720e-05 - lr: 1.1973e-05\n",
      "Epoch 166/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.0924e-05 - val_loss: 8.2407e-05 - lr: 1.1973e-05\n",
      "Epoch 167/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.0769e-05 - val_loss: 8.2133e-05 - lr: 1.1973e-05\n",
      "Epoch 168/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 9.0523e-05 - val_loss: 8.0835e-05 - lr: 1.1973e-05\n",
      "Epoch 169/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.9738e-05 - val_loss: 8.0263e-05 - lr: 1.0775e-05\n",
      "Epoch 170/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.9355e-05 - val_loss: 7.9996e-05 - lr: 1.0775e-05\n",
      "Epoch 171/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.9145e-05 - val_loss: 7.9865e-05 - lr: 1.0775e-05\n",
      "Epoch 172/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.8963e-05 - val_loss: 7.9842e-05 - lr: 1.0775e-05\n",
      "Epoch 173/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.8765e-05 - val_loss: 7.9311e-05 - lr: 1.0775e-05\n",
      "Epoch 174/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.8048e-05 - val_loss: 7.8978e-05 - lr: 9.6977e-06\n",
      "Epoch 175/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.7739e-05 - val_loss: 7.8406e-05 - lr: 9.6977e-06\n",
      "Epoch 176/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.7590e-05 - val_loss: 7.8152e-05 - lr: 9.6977e-06\n",
      "Epoch 177/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.7419e-05 - val_loss: 7.8094e-05 - lr: 9.6977e-06\n",
      "Epoch 178/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.7217e-05 - val_loss: 7.8338e-05 - lr: 9.6977e-06\n",
      "Epoch 179/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.6596e-05 - val_loss: 7.7680e-05 - lr: 8.7280e-06\n",
      "Epoch 180/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.6293e-05 - val_loss: 7.7304e-05 - lr: 8.7280e-06\n",
      "Epoch 181/200\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 8.6192e-05 - val_loss: 7.7099e-05 - lr: 8.7280e-06\n",
      "Epoch 182/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.5980e-05 - val_loss: 7.6635e-05 - lr: 8.7280e-06\n",
      "Epoch 183/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.5803e-05 - val_loss: 7.6647e-05 - lr: 8.7280e-06\n",
      "Epoch 184/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.5278e-05 - val_loss: 7.6143e-05 - lr: 7.8552e-06\n",
      "Epoch 185/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.5044e-05 - val_loss: 7.6194e-05 - lr: 7.8552e-06\n",
      "Epoch 186/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.4886e-05 - val_loss: 7.6367e-05 - lr: 7.8552e-06\n",
      "Epoch 187/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.4731e-05 - val_loss: 7.6233e-05 - lr: 7.8552e-06\n",
      "Epoch 188/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.4552e-05 - val_loss: 7.5942e-05 - lr: 7.8552e-06\n",
      "Epoch 189/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.4062e-05 - val_loss: 7.5652e-05 - lr: 7.0697e-06\n",
      "Epoch 190/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.3873e-05 - val_loss: 7.5083e-05 - lr: 7.0697e-06\n",
      "Epoch 191/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.3710e-05 - val_loss: 7.5158e-05 - lr: 7.0697e-06\n",
      "Epoch 192/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.3604e-05 - val_loss: 7.5279e-05 - lr: 7.0697e-06\n",
      "Epoch 193/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.3407e-05 - val_loss: 7.4750e-05 - lr: 7.0697e-06\n",
      "Epoch 194/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2990e-05 - val_loss: 7.4483e-05 - lr: 6.3627e-06\n",
      "Epoch 195/200\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2830e-05 - val_loss: 7.4234e-05 - lr: 6.3627e-06\n",
      "Epoch 196/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2669e-05 - val_loss: 7.4529e-05 - lr: 6.3627e-06\n",
      "Epoch 197/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2568e-05 - val_loss: 7.4350e-05 - lr: 6.3627e-06\n",
      "Epoch 198/200\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2430e-05 - val_loss: 7.3890e-05 - lr: 6.3627e-06\n",
      "Epoch 199/200\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2029e-05 - val_loss: 7.3255e-05 - lr: 5.7264e-06\n",
      "Epoch 200/200\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 8.1855e-05 - val_loss: 7.3751e-05 - lr: 5.7264e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc04ede2690>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 设置 EarlyStopping 回调函数，如果验证集的损失不再改善，则停止训练\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.9, patience=5)\n",
    "model.fit(   # 根据情况调整参数\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    validation_data=(val_features, val_labels),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c4e5f76-cdd8-4d3f-a6da-32263c60d88d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 MSE:0.0001\n",
      "y1 MSE:2998.0000 ------ 2998\n"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(test_features)\n",
    "print(\"y1 MSE:%.4f\" % mean_squared_error(test_labels, test_preds))\n",
    "print(\"y1 MSE:%.4f\" % len(test_labels), '------',len(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f97739de-d01c-49fc-b8f3-b1b6ac40d4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /models/slot0/20230905193524/assets\n"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "from datetime import datetime                                                                                                                                                                \n",
    "\n",
    "model_version =  datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y%m%d%H%M%S')\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    f'/models/slot0/{model_version}/', # v1/models/slot0/为tensorflow-serving的模型根目录\n",
    "    overwrite=True,\n",
    "    include_optimizer=True,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc678317-f4a5-49b2-85e4-7f25f0d95332",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"inputs\": [[1.4158, 2.9711, 10.7935, 7.5279, 2.3352, 8.1042, 2.3096, 3.3367, 11.8639, 12.7142, 1.8581, 0.3898, 19.8309, 19.771, 0.0001, 1.7768, 171.764, 1434.24]]}\n",
      "{'outputs': [[0.328873724, -0.928518772, 0.288097501, -0.144908905, -0.834479749, 0.76588279, 0.469486177, 1.90728557]]}\n",
      "{'outputs': [[0.331511, -0.932553, 0.285048, -0.1435, -0.833982, 0.767568, 0.463969, 1.9048]]\n",
      "y1 MSE:0.0000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "test_features=pd.DataFrame(test_features)\n",
    "test_labels=pd.DataFrame(test_labels)\n",
    "req_data = json.dumps({\n",
    "            'inputs': test_features.values[:1].tolist()\n",
    "        })  \n",
    "print(req_data)\n",
    "response = requests.post(f'http://fireeye-test-model-container:8501/v1/models/slot0/versions/{model_version}:predict', # 根据部署地址填写\n",
    "                         data=req_data,\n",
    "                         headers={\"content-type\": \"application/json\"})\n",
    "if response.status_code != 200:\n",
    "    raise RuntimeError('Request tf-serving failed: ' + response.text)\n",
    "resp_data = json.loads(response.text)    \n",
    "if 'outputs' not in resp_data \\\n",
    "                    or type(resp_data['outputs']) is not list:\n",
    "    raise ValueError('Malformed tf-serving response')\n",
    "\n",
    "print(resp_data)\n",
    "print(\"{'outputs':\",test_labels.values[:1].tolist())\n",
    "\n",
    "print(\"y1 MSE:%.4f\" % mean_squared_error(test_labels.values[:1].tolist(), resp_data['outputs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5011d7-455f-44c0-b2ed-e2e10bbb1101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea2950-0241-45aa-b1df-fc920b21649f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
