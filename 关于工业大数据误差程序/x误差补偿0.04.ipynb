{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 数据加载\n",
    "我们的误差补偿训练数据存放在<I>adjustments.tsv</I>中，测试数据放在<I>test_adjustments.tsv</I>。\n",
    "\n",
    "**测试数据集为真实值，不能进行调整，否则会导致实际模型测试结果和实际的预测结果存在偏差，出现过于乐观或者过于消极的测试结果**\n",
    "\n",
    "只需要执行加载训练数据的代码即可，当然也可以修改代码加载指定文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1rRo8oNqZ-Rj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 加载训练数据\n",
    "train_dataset = pd.read_csv('./adjustments.tsv',\n",
    "                          sep='\\t',\n",
    "                          skipinitialspace=True)\n",
    "# 加载测试数据     ！！！ 测试数据集为真实值，不能进行调整，否则将会导致实际模型测试结果和真实预测结果存在偏差，使得最终加工的作品和预期不一致\n",
    "test_dataset = pd.read_csv('./test_adjustments.tsv',\n",
    "                          sep='\\t',\n",
    "                          skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印并查看数据，可以看出，adjustments.tsv文件的前n列为特征值，这些值代表着真实世界中影响机床的环境因素，例如刀具磨损、温度、湿度等等；后面几列为补偿指令。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印并查看数据，可以看出，adjustments.tsv文件的前n列为特征值，这些值代表着真实世界中影响机床的环境因素，例如刀具磨损、温度、湿度等等；后面几列为补偿指令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>特征0</th>\n",
       "      <th>特征1</th>\n",
       "      <th>特征2</th>\n",
       "      <th>特征3</th>\n",
       "      <th>特征4</th>\n",
       "      <th>特征5</th>\n",
       "      <th>特征6</th>\n",
       "      <th>特征7</th>\n",
       "      <th>特征8</th>\n",
       "      <th>特征9</th>\n",
       "      <th>...</th>\n",
       "      <th>特征16</th>\n",
       "      <th>特征17</th>\n",
       "      <th>补偿0</th>\n",
       "      <th>补偿1</th>\n",
       "      <th>补偿2</th>\n",
       "      <th>补偿3</th>\n",
       "      <th>补偿4</th>\n",
       "      <th>补偿5</th>\n",
       "      <th>补偿6</th>\n",
       "      <th>补偿7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4158</td>\n",
       "      <td>2.9711</td>\n",
       "      <td>10.7935</td>\n",
       "      <td>7.5279</td>\n",
       "      <td>2.3352</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>2.3096</td>\n",
       "      <td>3.3367</td>\n",
       "      <td>11.8639</td>\n",
       "      <td>12.7142</td>\n",
       "      <td>...</td>\n",
       "      <td>171.764</td>\n",
       "      <td>1434.24</td>\n",
       "      <td>0.331511</td>\n",
       "      <td>-0.932553</td>\n",
       "      <td>0.285048</td>\n",
       "      <td>-0.1435</td>\n",
       "      <td>-0.833982</td>\n",
       "      <td>0.767568</td>\n",
       "      <td>0.463969</td>\n",
       "      <td>1.904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6280</td>\n",
       "      <td>1.8616</td>\n",
       "      <td>10.1770</td>\n",
       "      <td>7.4684</td>\n",
       "      <td>2.1915</td>\n",
       "      <td>8.5945</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>2.9661</td>\n",
       "      <td>11.5816</td>\n",
       "      <td>12.2487</td>\n",
       "      <td>...</td>\n",
       "      <td>185.824</td>\n",
       "      <td>1469.19</td>\n",
       "      <td>0.894066</td>\n",
       "      <td>-0.446796</td>\n",
       "      <td>0.058519</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.715252</td>\n",
       "      <td>0.999105</td>\n",
       "      <td>0.988844</td>\n",
       "      <td>0.689742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9648</td>\n",
       "      <td>1.8103</td>\n",
       "      <td>10.1682</td>\n",
       "      <td>5.9705</td>\n",
       "      <td>2.0629</td>\n",
       "      <td>6.5349</td>\n",
       "      <td>2.8694</td>\n",
       "      <td>3.1185</td>\n",
       "      <td>11.7464</td>\n",
       "      <td>12.2074</td>\n",
       "      <td>...</td>\n",
       "      <td>187.576</td>\n",
       "      <td>1540.76</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.460716</td>\n",
       "      <td>0.997809</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.723031</td>\n",
       "      <td>0.992935</td>\n",
       "      <td>0.682903</td>\n",
       "      <td>0.749580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7119</td>\n",
       "      <td>1.6221</td>\n",
       "      <td>10.1487</td>\n",
       "      <td>6.8678</td>\n",
       "      <td>2.0694</td>\n",
       "      <td>6.8806</td>\n",
       "      <td>1.5791</td>\n",
       "      <td>2.3003</td>\n",
       "      <td>11.5545</td>\n",
       "      <td>12.0659</td>\n",
       "      <td>...</td>\n",
       "      <td>189.938</td>\n",
       "      <td>1498.29</td>\n",
       "      <td>0.998794</td>\n",
       "      <td>-0.862448</td>\n",
       "      <td>0.329694</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.891745</td>\n",
       "      <td>0.015078</td>\n",
       "      <td>0.997127</td>\n",
       "      <td>0.984439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3797</td>\n",
       "      <td>1.6852</td>\n",
       "      <td>10.9601</td>\n",
       "      <td>5.0035</td>\n",
       "      <td>3.1659</td>\n",
       "      <td>5.9471</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>2.6402</td>\n",
       "      <td>11.7458</td>\n",
       "      <td>12.7041</td>\n",
       "      <td>...</td>\n",
       "      <td>181.275</td>\n",
       "      <td>1465.11</td>\n",
       "      <td>0.442893</td>\n",
       "      <td>-0.990703</td>\n",
       "      <td>-0.151400</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.617880</td>\n",
       "      <td>-0.506397</td>\n",
       "      <td>-0.997502</td>\n",
       "      <td>0.376126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      特征0     特征1      特征2     特征3     特征4     特征5     特征6     特征7      特征8  \\\n",
       "0  1.4158  2.9711  10.7935  7.5279  2.3352  8.1042  2.3096  3.3367  11.8639   \n",
       "1  0.6280  1.8616  10.1770  7.4684  2.1915  8.5945  0.1379  2.9661  11.5816   \n",
       "2  0.9648  1.8103  10.1682  5.9705  2.0629  6.5349  2.8694  3.1185  11.7464   \n",
       "3  0.7119  1.6221  10.1487  6.8678  2.0694  6.8806  1.5791  2.3003  11.5545   \n",
       "4  0.3797  1.6852  10.9601  5.0035  3.1659  5.9471  0.0858  2.6402  11.7458   \n",
       "\n",
       "       特征9  ...     特征16     特征17       补偿0       补偿1       补偿2     补偿3  \\\n",
       "0  12.7142  ...  171.764  1434.24  0.331511 -0.932553  0.285048 -0.1435   \n",
       "1  12.2487  ...  185.824  1469.19  0.894066 -0.446796  0.058519 -0.4624   \n",
       "2  12.2074  ...  187.576  1540.76  0.999982  0.460716  0.997809 -0.4624   \n",
       "3  12.0659  ...  189.938  1498.29  0.998794 -0.862448  0.329694 -0.4624   \n",
       "4  12.7041  ...  181.275  1465.11  0.442893 -0.990703 -0.151400  0.2245   \n",
       "\n",
       "        补偿4       补偿5       补偿6       补偿7  \n",
       "0 -0.833982  0.767568  0.463969  1.904800  \n",
       "1  0.715252  0.999105  0.988844  0.689742  \n",
       "2  0.723031  0.992935  0.682903  0.749580  \n",
       "3  0.891745  0.015078  0.997127  0.984439  \n",
       "4  0.617880 -0.506397 -0.997502  0.376126  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "dataset = train_dataset.copy()\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 26)\n",
      "(2998, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>特征0</th>\n",
       "      <th>特征1</th>\n",
       "      <th>特征2</th>\n",
       "      <th>特征3</th>\n",
       "      <th>特征4</th>\n",
       "      <th>特征5</th>\n",
       "      <th>特征6</th>\n",
       "      <th>特征7</th>\n",
       "      <th>特征8</th>\n",
       "      <th>特征9</th>\n",
       "      <th>...</th>\n",
       "      <th>特征16</th>\n",
       "      <th>特征17</th>\n",
       "      <th>补偿0</th>\n",
       "      <th>补偿1</th>\n",
       "      <th>补偿2</th>\n",
       "      <th>补偿3</th>\n",
       "      <th>补偿4</th>\n",
       "      <th>补偿5</th>\n",
       "      <th>补偿6</th>\n",
       "      <th>补偿7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.008837</td>\n",
       "      <td>2.018090</td>\n",
       "      <td>10.501365</td>\n",
       "      <td>6.980399</td>\n",
       "      <td>2.991379</td>\n",
       "      <td>6.986831</td>\n",
       "      <td>1.983407</td>\n",
       "      <td>2.994988</td>\n",
       "      <td>11.739018</td>\n",
       "      <td>12.600981</td>\n",
       "      <td>...</td>\n",
       "      <td>172.242767</td>\n",
       "      <td>1474.643939</td>\n",
       "      <td>0.525279</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.250365</td>\n",
       "      <td>0.010439</td>\n",
       "      <td>-0.269551</td>\n",
       "      <td>0.524640</td>\n",
       "      <td>0.222959</td>\n",
       "      <td>0.561442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.573132</td>\n",
       "      <td>0.578950</td>\n",
       "      <td>0.286296</td>\n",
       "      <td>1.134771</td>\n",
       "      <td>0.574884</td>\n",
       "      <td>1.142659</td>\n",
       "      <td>1.144038</td>\n",
       "      <td>0.577503</td>\n",
       "      <td>0.128526</td>\n",
       "      <td>0.196063</td>\n",
       "      <td>...</td>\n",
       "      <td>12.198489</td>\n",
       "      <td>54.894522</td>\n",
       "      <td>0.399656</td>\n",
       "      <td>0.699439</td>\n",
       "      <td>0.497970</td>\n",
       "      <td>0.330564</td>\n",
       "      <td>0.672572</td>\n",
       "      <td>0.494762</td>\n",
       "      <td>0.716798</td>\n",
       "      <td>1.124138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000300</td>\n",
       "      <td>5.000700</td>\n",
       "      <td>2.000400</td>\n",
       "      <td>5.000800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.000700</td>\n",
       "      <td>11.155400</td>\n",
       "      <td>11.750600</td>\n",
       "      <td>...</td>\n",
       "      <td>121.207000</td>\n",
       "      <td>1377.980000</td>\n",
       "      <td>-0.950279</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-1.240660</td>\n",
       "      <td>-0.533200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.863171</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-3.354270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.518975</td>\n",
       "      <td>1.506825</td>\n",
       "      <td>10.257700</td>\n",
       "      <td>6.033325</td>\n",
       "      <td>2.501750</td>\n",
       "      <td>6.004525</td>\n",
       "      <td>1.001925</td>\n",
       "      <td>2.509325</td>\n",
       "      <td>11.668100</td>\n",
       "      <td>12.485025</td>\n",
       "      <td>...</td>\n",
       "      <td>165.299500</td>\n",
       "      <td>1437.355000</td>\n",
       "      <td>0.279705</td>\n",
       "      <td>-0.693314</td>\n",
       "      <td>-0.096690</td>\n",
       "      <td>-0.143500</td>\n",
       "      <td>-0.847946</td>\n",
       "      <td>0.307439</td>\n",
       "      <td>-0.492667</td>\n",
       "      <td>-0.107755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.014300</td>\n",
       "      <td>2.042550</td>\n",
       "      <td>10.497350</td>\n",
       "      <td>6.962800</td>\n",
       "      <td>2.955600</td>\n",
       "      <td>6.956750</td>\n",
       "      <td>1.985950</td>\n",
       "      <td>2.986350</td>\n",
       "      <td>11.762100</td>\n",
       "      <td>12.644450</td>\n",
       "      <td>...</td>\n",
       "      <td>174.141000</td>\n",
       "      <td>1456.825000</td>\n",
       "      <td>0.617192</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>0.250205</td>\n",
       "      <td>-0.143500</td>\n",
       "      <td>-0.550868</td>\n",
       "      <td>0.709205</td>\n",
       "      <td>0.495988</td>\n",
       "      <td>0.390784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.488325</td>\n",
       "      <td>2.523625</td>\n",
       "      <td>10.750450</td>\n",
       "      <td>7.944000</td>\n",
       "      <td>3.481775</td>\n",
       "      <td>7.957600</td>\n",
       "      <td>2.953800</td>\n",
       "      <td>3.496425</td>\n",
       "      <td>11.835725</td>\n",
       "      <td>12.747475</td>\n",
       "      <td>...</td>\n",
       "      <td>181.637250</td>\n",
       "      <td>1496.542500</td>\n",
       "      <td>0.860046</td>\n",
       "      <td>0.696679</td>\n",
       "      <td>0.604809</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.370577</td>\n",
       "      <td>0.905374</td>\n",
       "      <td>0.877369</td>\n",
       "      <td>1.159790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.998200</td>\n",
       "      <td>2.998600</td>\n",
       "      <td>10.999300</td>\n",
       "      <td>8.996300</td>\n",
       "      <td>3.999400</td>\n",
       "      <td>8.998500</td>\n",
       "      <td>3.996200</td>\n",
       "      <td>3.999400</td>\n",
       "      <td>11.950100</td>\n",
       "      <td>12.933200</td>\n",
       "      <td>...</td>\n",
       "      <td>193.145000</td>\n",
       "      <td>1785.880000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.832180</td>\n",
       "      <td>0.632400</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.585870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               特征0          特征1          特征2          特征3          特征4  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000   \n",
       "mean      1.008837     2.018090    10.501365     6.980399     2.991379   \n",
       "std       0.573132     0.578950     0.286296     1.134771     0.574884   \n",
       "min       0.000500     1.000000    10.000300     5.000700     2.000400   \n",
       "25%       0.518975     1.506825    10.257700     6.033325     2.501750   \n",
       "50%       1.014300     2.042550    10.497350     6.962800     2.955600   \n",
       "75%       1.488325     2.523625    10.750450     7.944000     3.481775   \n",
       "max       1.998200     2.998600    10.999300     8.996300     3.999400   \n",
       "\n",
       "               特征5          特征6          特征7          特征8          特征9  ...  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000  ...   \n",
       "mean      6.986831     1.983407     2.994988    11.739018    12.600981  ...   \n",
       "std       1.142659     1.144038     0.577503     0.128526     0.196063  ...   \n",
       "min       5.000800     0.000100     2.000700    11.155400    11.750600  ...   \n",
       "25%       6.004525     1.001925     2.509325    11.668100    12.485025  ...   \n",
       "50%       6.956750     1.985950     2.986350    11.762100    12.644450  ...   \n",
       "75%       7.957600     2.953800     3.496425    11.835725    12.747475  ...   \n",
       "max       8.998500     3.996200     3.999400    11.950100    12.933200  ...   \n",
       "\n",
       "              特征16         特征17          补偿0          补偿1          补偿2  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000   \n",
       "mean    172.242767  1474.643939     0.525279     0.005173     0.250365   \n",
       "std      12.198489    54.894522     0.399656     0.699439     0.497970   \n",
       "min     121.207000  1377.980000    -0.950279    -0.999992    -1.240660   \n",
       "25%     165.299500  1437.355000     0.279705    -0.693314    -0.096690   \n",
       "50%     174.141000  1456.825000     0.617192     0.015394     0.250205   \n",
       "75%     181.637250  1496.542500     0.860046     0.696679     0.604809   \n",
       "max     193.145000  1785.880000     1.000000     1.000000     1.832180   \n",
       "\n",
       "               补偿3          补偿4          补偿5          补偿6          补偿7  \n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000  \n",
       "mean      0.010439    -0.269551     0.524640     0.222959     0.561442  \n",
       "std       0.330564     0.672572     0.494762     0.716798     1.124138  \n",
       "min      -0.533200    -1.000000    -0.863171    -0.999998    -3.354270  \n",
       "25%      -0.143500    -0.847946     0.307439    -0.492667    -0.107755  \n",
       "50%      -0.143500    -0.550868     0.709205     0.495988     0.390784  \n",
       "75%       0.224500     0.370577     0.905374     0.877369     1.159790  \n",
       "max       0.632400     0.999997     0.999997     1.000000     6.585870  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds=dataset.dropna()\n",
    "w=['特征'+str(i) for i in range(18)]\n",
    "ds=ds.drop_duplicates(w)\n",
    "print(ds.shape)\n",
    "ds.describe()\n",
    "\n",
    "ds1=test_dataset.dropna()\n",
    "w=['特征'+str(i) for i in range(18)]\n",
    "ds1=ds1.drop_duplicates(w)\n",
    "print(ds1.shape)\n",
    "ds1.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# # 选择需要可视化的数据列\n",
    "# #data = ds[ds]\n",
    "\n",
    "# # 循环遍历每个数据列\n",
    "# for col in ds.columns:\n",
    "#     # 绘制单个数据列的核密度估计图\n",
    "#     fig = plt.figure(figsize=(5, 3))\n",
    "#     sns.kdeplot(ds[col])\n",
    "    \n",
    "#     # 添加网格线\n",
    "#     plt.grid()\n",
    "#     #plt.xticks(np.arange(-0.2, 1.2, 0.1),rotation=45)\n",
    "   \n",
    "#     # 设置图的标题和横轴标签\n",
    "#     plt.title(f'Kernel Density Estimate - {col}')\n",
    "#     plt.xlabel(col)\n",
    "    \n",
    "#     # 显示绘制的图形\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们可以简单分析数据，比如我们可以查看特征值和补偿值的分布特性，比如均值和方差："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均值 [   1.       1.998   10.5      6.998    3.001    6.995    2.005    2.999\n",
      "   11.736   12.604    1.768    0.649   19.417   19.598    3.349   17.983\n",
      "  172.057 1475.054]\n",
      "方差 [   0.332    0.332    0.083    1.327    0.333    1.337    1.331    0.336\n",
      "    0.017    0.039    0.074    0.213    0.163    0.069   10.31   525.861\n",
      "  149.833 3136.668]\n"
     ]
    }
   ],
   "source": [
    "average = np.average(ds.values[:,:18], axis=0)\n",
    "variance = np.var(ds.values[:,:18], axis=0)\n",
    "print('均值', average)\n",
    "print('方差', variance)\n",
    "\n",
    "# average1 = np.average(ds1.values[:,:18], axis=0)\n",
    "# variance1 = np.var(ds1.values[:,:18], axis=0)\n",
    "# print('均值', average1)\n",
    "# print('方差', variance1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45455, 26)\n",
      "(45455, 26)\n",
      "特征0        0.327537\n",
      "特征1        0.328899\n",
      "特征2        0.081437\n",
      "特征3        1.290450\n",
      "特征4        0.317921\n",
      "特征5        1.286620\n",
      "特征6        1.317239\n",
      "特征7        0.327636\n",
      "特征8        0.014313\n",
      "特征9        0.032006\n",
      "特征10       0.054821\n",
      "特征11       0.191810\n",
      "特征12       0.108303\n",
      "特征13       0.049248\n",
      "特征14       8.091563\n",
      "特征15     304.621928\n",
      "特征16     118.242898\n",
      "特征17    2244.934253\n",
      "补偿0        0.150524\n",
      "补偿1        0.501245\n",
      "补偿2        0.239188\n",
      "补偿3        0.100698\n",
      "补偿4        0.410969\n",
      "补偿5        0.250375\n",
      "补偿6        0.509709\n",
      "补偿7        1.214950\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#计算前18列特征的均值和标准差\n",
    "mean = np.mean(ds.values[:, :18], axis=0)\n",
    "std = np.std(ds.values[:, :18], axis=0)\n",
    "\n",
    "\n",
    "mean1 = np.mean(ds1.values[:, :18], axis=0)\n",
    "std1 = np.std(ds1.values[:, :18], axis=0)\n",
    "# 定义上限和下限\n",
    "upper_limit = mean + 3 * std\n",
    "lower_limit = mean - 3 * std\n",
    "\n",
    "\n",
    "\n",
    "upper_limit1 = mean1 + 3 * std\n",
    "lower_limit1 = mean1 - 3 * std\n",
    "# 使用布尔索引删除超出上限和下限的行\n",
    "cleaned_ds = ds[~((ds.values[:, :18] > upper_limit) | (ds.values[:, :18] < lower_limit)).any(axis=1)]\n",
    "\n",
    "\n",
    "cleaned_ds1 = ds1[~((ds1.values[:, :18] > upper_limit1) | (ds1.values[:, :18] < lower_limit1)).any(axis=1)]\n",
    "\n",
    "\n",
    "# cleaned_ds=cleaned_ds.drop(cleaned_ds[(cleaned_ds['特征16']<181)&(cleaned_ds['特征16']>165)].sample(frac=0.2,random_state=0).index)\n",
    "# cleaned_ds1=cleaned_ds1.drop(cleaned_ds1[(cleaned_ds1['特征16']<181)&(cleaned_ds1['特征16']>165)].sample(frac=0.2,random_state=0).index)\n",
    "# # 计算特征的均值和标准差\n",
    "# mean = np.mean(ds.values, axis=0)\n",
    "# std = np.std(ds.values, axis=0)\n",
    "\n",
    "# # 定义上限和下限\n",
    "# upper_limit = mean + 3 * std\n",
    "# lower_limit = mean - 3 * std\n",
    "\n",
    "# # 使用布尔索引删除超出上限和下限的行\n",
    "# cleaned_ds = ds[~((ds.values > upper_limit) | (ds.values < lower_limit)).any(axis=1)]\n",
    "\n",
    "# 打印清理后数据集的形状\n",
    "print(cleaned_ds.shape)\n",
    "\n",
    "print(cleaned_ds.shape)\n",
    "cleaned_ds.describe()\n",
    "\n",
    "cleaned_ds=pd.DataFrame(cleaned_ds)\n",
    "variance=np.var(cleaned_ds)\n",
    "\n",
    "print(variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # 提取特征数据和标签数据\n",
    "# X = cleaned_ds.values[:, :18]  # 提取前18列作为特征数据\n",
    "# y = cleaned_ds.values[:, 18:]  # 提取后8列作为标签数据\n",
    "\n",
    "# # 创建MinMaxScaler对象，并对特征数据进行归一化\n",
    "# scaler = MinMaxScaler()\n",
    "# X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# # 打印归一化后特征数据的范围（最小值和最大值）\n",
    "# print('特征数据归一化范围:', np.min(X_normalized), np.max(X_normalized))\n",
    "\n",
    "# # 打印训练数据集上计算得到的均值和标准差\n",
    "# print('训练数据集均值:', scaler.mean_)\n",
    "# print('训练数据集标准差:', scaler.scale_)\n",
    "\n",
    "# # 将归一化后的特征数据与标签数据重新合并成一个数据集\n",
    "# normalized_ds = np.concatenate((X_normalized, y), axis=1)\n",
    "# # 将归一化后的数据集转换为DataFrame\n",
    "# normalized_ds = pd.DataFrame(normalized_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建训练集和测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，我们将数据分为训练集和测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们分别获取训练集和测试集的特征以及补偿值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40910, 18) (40910, 8)\n",
      "(4545, 18) (4545, 8)\n",
      "(2771, 18) (2771, 8)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_ds=dataset.sample(frac=0.9,random_state=0)\n",
    "val_ds=dataset.drop(train_ds.index)\n",
    "\n",
    "\n",
    "train_ds=cleaned_ds.sample(frac=0.9,random_state=0)\n",
    "val_ds=cleaned_ds.drop(train_ds.index)\n",
    "\n",
    "\n",
    "#训练集\n",
    "train_features=train_ds.values[:,:18]\n",
    "# sc=MinMaxScaler(feature_range=(0,1))\n",
    "# train_features=sc.fit_transform(train_features)\n",
    "# train_features=pd.DataFrame(train_features)\n",
    "# train_features=train_features.drop(train_features[(train_features['8']<0.8)&(train_features['8']>0.53)].sample(frac=0.2,random_state=0).index)\n",
    "train_labels=train_ds.values[:,18:]\n",
    "\n",
    "#验证集\n",
    "val_features=val_ds.values[:,:18]\n",
    "# sc=MinMaxScaler(feature_range=(0,1))\n",
    "# val_features=sc.fit_transform(val_features)\n",
    "# val_features=pd.DataFrame(val_features)\n",
    "val_labels=val_ds.values[:,18:]\n",
    "\n",
    "# 测试集\n",
    "# test_features=test_dataset.values[:,:18]\n",
    "# test_labels=test_dataset.values[:,18:]\n",
    "test_features=cleaned_ds1.values[:,:18]\n",
    "test_labels=cleaned_ds1.values[:,18:]\n",
    "\n",
    "# test_features = cleaned_ds1.copy()\n",
    "# test_labels = test_features[['补偿'+str(i) for i in range(8)]].copy()\n",
    "# test_features = test_features.drop(['补偿'+str(i) for i in range(8)], axis=1)\n",
    "\n",
    "# test_features=cleaned_ds1.values[:,:18]\n",
    "# test_labels=cleaned_ds1.values[:,18:]\n",
    "# print(train_features)\n",
    "# print(train_features.describe())\n",
    "print(train_features.shape,train_labels.shape)\n",
    "print(val_features.shape,val_labels.shape)\n",
    "print(test_features.shape,test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据的分析和预处理的方法有很多种，我们只展示了一种方法。用户可根据自己的需要使用其他方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据的分析和预处理的方法有很多种，我们只展示了一种方法。用户可根据自己的需要使用其他方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 模型构建\n",
    "\n",
    "本平台支持基于Tensorflow-Serving的HTTP调用方式：该方式支持任何部署在TensorFlow Serving上的模型\n",
    "\n",
    "### TensorFlow\n",
    "首先，我们导入相关的依赖包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9xQKvCJ85kCQ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from numpy import array\n",
    "from numpy.random import uniform\n",
    "from numpy import hstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，我们开始构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_7 (Normalizat  (None, 18)               37        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               9728      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 184,989\n",
      "Trainable params: 184,952\n",
      "Non-trainable params: 37\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#----------构建模型及训练-----------------\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import optimizers\n",
    "# model = tf.keras.Sequential([\n",
    "#     layers.Dense(100, input_dim=train_features.shape[1], activation=\"relu\"),\n",
    "#     Dropout(0.5),\n",
    "#     layers.Dense(train_labels.shape[1])\n",
    "# ])\n",
    "\n",
    "\n",
    "# 创建Normalizaiton层\n",
    "normalizer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "# # 计算并设置归一化参数\n",
    "normalizer.adapt(train_features)\n",
    "# 构建模型\n",
    "normalizer.adapt(val_features)\n",
    "\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     normalizer,  # 归一化层作为第一层\n",
    "#     layers.Dense(300, activation=\"relu\",input_dim=train_features.shape[1]),\n",
    "#     layers.Dense(300, activation=\"relu\"),\n",
    "#     layers.Dense(300, activation=\"relu\"),\n",
    "#     layers.Dense(train_labels.shape[1])\n",
    "#     # normalizer,  # 归一化层作为第一层\n",
    "#     # layers.Dense(100, activation=\"relu\"),\n",
    "#     # layers.Dense(train_labels.shape[1])\n",
    "# ])\n",
    "\n",
    "# # optimizers=optimizers.Nadam(learning_rate=0.01)\n",
    "# model.compile(loss=\"mse\", optimizer=\"adam\")  # 根据情况调整参数\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "normalizer, # 归一化层作为第一层\n",
    "layers.Dense(512, activation=\"gelu\", input_dim=train_features.shape[1]),\n",
    "\n",
    "layers.Dense(256, activation=\"gelu\"),\n",
    "\n",
    "layers.Dense(128,activation=\"gelu\"),\n",
    "layers.Dense(64,activation=\"gelu\"),\n",
    "layers.Dense(32,activation=\"gelu\"),\n",
    "layers.Dense(16,activation=\"gelu\"),\n",
    "layers.Dropout(0.3),\n",
    "layers.Dense(train_labels.shape[1])\n",
    "# normalizer, # 归一化层作为第一层\n",
    "# layers.Dense(100, activation=\"relu\"),\n",
    "# layers.Dense(train_labels.shape[1])\n",
    "])\n",
    "\n",
    "# optimizers=optimizers.Nadam(learning_rate=0.01)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\") # 根据情况调整参数\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# best_features=[]\n",
    "# for i in range(train_features.shape[1]):\n",
    "#     selected_features=best_features+[i]\n",
    "#     selected_train_features=train_features[:,selected_features]\n",
    "# model.fit(   # 根据情况调整参数\n",
    "#     train_features,\n",
    "#     train_labels,\n",
    "#     validation_data=(val_features, val_labels),\n",
    "#     epochs=20,\n",
    "#     batch_size=32,\n",
    "#     callbacks=[lr_scheduler]\n",
    "# )   \n",
    "# baselin_acc=model.evaluate(selected_train_features,train_labels,verbose=0)    \n",
    "# if not best_features or baselin_acc >=best_accurancy:\n",
    "#     best_features=selected_features\n",
    "#     best_accurancy=baselin_acc\n",
    "# best_X=train_features[:,best_features]\n",
    "# model.fit(   # 根据情况调整参数\n",
    "#     best_X,\n",
    "#     train_labels,\n",
    "#     validation_data=(val_features, val_labels),\n",
    "#     epochs=20,\n",
    "#     batch_size=32,\n",
    "#     callbacks=[lr_scheduler]\n",
    "# )   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "设置模型训练参数进行模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1996 - val_loss: 0.1400 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1627 - val_loss: 0.1278 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1553 - val_loss: 0.1238 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1524 - val_loss: 0.1213 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1490 - val_loss: 0.1153 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1465 - val_loss: 0.1168 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1455 - val_loss: 0.1176 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1444 - val_loss: 0.1136 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1432 - val_loss: 0.1126 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1422 - val_loss: 0.1100 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1413 - val_loss: 0.1096 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1409 - val_loss: 0.1078 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1406 - val_loss: 0.1088 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1401 - val_loss: 0.1088 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1398 - val_loss: 0.1092 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1392 - val_loss: 0.1097 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1387 - val_loss: 0.1087 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1388 - val_loss: 0.1092 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1377 - val_loss: 0.1059 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1387 - val_loss: 0.1079 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1375 - val_loss: 0.1084 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1381 - val_loss: 0.1055 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1378 - val_loss: 0.1081 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1375 - val_loss: 0.1067 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1376 - val_loss: 0.1078 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1373 - val_loss: 0.1080 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1369 - val_loss: 0.1061 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1371 - val_loss: 0.1086 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1367 - val_loss: 0.1068 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1365 - val_loss: 0.1053 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1364 - val_loss: 0.1084 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1360 - val_loss: 0.1063 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1360 - val_loss: 0.1105 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1357 - val_loss: 0.1079 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1353 - val_loss: 0.1074 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1351 - val_loss: 0.1061 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1347 - val_loss: 0.1068 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1344 - val_loss: 0.1079 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1343 - val_loss: 0.1071 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1342 - val_loss: 0.1067 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1341 - val_loss: 0.1058 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1337 - val_loss: 0.1078 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1338 - val_loss: 0.1069 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1330 - val_loss: 0.1082 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1329 - val_loss: 0.1077 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1320 - val_loss: 0.1087 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1316 - val_loss: 0.1085 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1316 - val_loss: 0.1099 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1311 - val_loss: 0.1095 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1302 - val_loss: 0.1105 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1296 - val_loss: 0.1105 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1297 - val_loss: 0.1090 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1294 - val_loss: 0.1100 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1287 - val_loss: 0.1098 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1288 - val_loss: 0.1117 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1280 - val_loss: 0.1105 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1281 - val_loss: 0.1104 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1274 - val_loss: 0.1103 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1269 - val_loss: 0.1138 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1271 - val_loss: 0.1118 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1262 - val_loss: 0.1132 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1262 - val_loss: 0.1116 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1257 - val_loss: 0.1134 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1254 - val_loss: 0.1137 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1250 - val_loss: 0.1115 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1251 - val_loss: 0.1124 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1243 - val_loss: 0.1115 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1245 - val_loss: 0.1118 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1241 - val_loss: 0.1136 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1231 - val_loss: 0.1139 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1236 - val_loss: 0.1119 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1235 - val_loss: 0.1142 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1230 - val_loss: 0.1135 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1229 - val_loss: 0.1139 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1228 - val_loss: 0.1128 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1225 - val_loss: 0.1141 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1226 - val_loss: 0.1129 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1219 - val_loss: 0.1142 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1223 - val_loss: 0.1147 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1217 - val_loss: 0.1148 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1216 - val_loss: 0.1140 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1218 - val_loss: 0.1166 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1218 - val_loss: 0.1174 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1210 - val_loss: 0.1155 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1213 - val_loss: 0.1138 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1211 - val_loss: 0.1137 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1209 - val_loss: 0.1146 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1211 - val_loss: 0.1146 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1211 - val_loss: 0.1157 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1205 - val_loss: 0.1148 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1207 - val_loss: 0.1166 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1205 - val_loss: 0.1153 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1203 - val_loss: 0.1154 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1203 - val_loss: 0.1151 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1197 - val_loss: 0.1143 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1197 - val_loss: 0.1161 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1200 - val_loss: 0.1142 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1201 - val_loss: 0.1153 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1196 - val_loss: 0.1153 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1194 - val_loss: 0.1149 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1200 - val_loss: 0.1160 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1193 - val_loss: 0.1150 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1192 - val_loss: 0.1143 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1195 - val_loss: 0.1152 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1192 - val_loss: 0.1145 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1194 - val_loss: 0.1153 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1194 - val_loss: 0.1132 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1193 - val_loss: 0.1164 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1189 - val_loss: 0.1153 - lr: 0.0010\n",
      "Epoch 110/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1192 - val_loss: 0.1177 - lr: 0.0010\n",
      "Epoch 111/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1187 - val_loss: 0.1151 - lr: 0.0010\n",
      "Epoch 112/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1191 - val_loss: 0.1164 - lr: 0.0010\n",
      "Epoch 113/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1191 - val_loss: 0.1173 - lr: 0.0010\n",
      "Epoch 114/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1189 - val_loss: 0.1160 - lr: 0.0010\n",
      "Epoch 115/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1189 - val_loss: 0.1164 - lr: 0.0010\n",
      "Epoch 116/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1188 - val_loss: 0.1181 - lr: 0.0010\n",
      "Epoch 117/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1183 - val_loss: 0.1167 - lr: 0.0010\n",
      "Epoch 118/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1186 - val_loss: 0.1160 - lr: 0.0010\n",
      "Epoch 119/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1186 - val_loss: 0.1150 - lr: 0.0010\n",
      "Epoch 120/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1184 - val_loss: 0.1169 - lr: 0.0010\n",
      "Epoch 121/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1183 - val_loss: 0.1160 - lr: 0.0010\n",
      "Epoch 122/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1187 - val_loss: 0.1149 - lr: 0.0010\n",
      "Epoch 123/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1181 - val_loss: 0.1153 - lr: 0.0010\n",
      "Epoch 124/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1184 - val_loss: 0.1156 - lr: 0.0010\n",
      "Epoch 125/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1180 - val_loss: 0.1144 - lr: 0.0010\n",
      "Epoch 126/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1179 - val_loss: 0.1161 - lr: 0.0010\n",
      "Epoch 127/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1180 - val_loss: 0.1176 - lr: 0.0010\n",
      "Epoch 128/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1181 - val_loss: 0.1165 - lr: 0.0010\n",
      "Epoch 129/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1177 - val_loss: 0.1151 - lr: 0.0010\n",
      "Epoch 130/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1184 - val_loss: 0.1166 - lr: 0.0010\n",
      "Epoch 131/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1173 - val_loss: 0.1159 - lr: 9.0000e-04\n",
      "Epoch 132/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1165 - val_loss: 0.1152 - lr: 9.0000e-04\n",
      "Epoch 133/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1170 - val_loss: 0.1154 - lr: 9.0000e-04\n",
      "Epoch 134/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1170 - val_loss: 0.1156 - lr: 9.0000e-04\n",
      "Epoch 135/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1166 - val_loss: 0.1161 - lr: 9.0000e-04\n",
      "Epoch 136/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1170 - val_loss: 0.1151 - lr: 9.0000e-04\n",
      "Epoch 137/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1170 - val_loss: 0.1148 - lr: 9.0000e-04\n",
      "Epoch 138/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1165 - val_loss: 0.1145 - lr: 9.0000e-04\n",
      "Epoch 139/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1166 - val_loss: 0.1156 - lr: 9.0000e-04\n",
      "Epoch 140/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1169 - val_loss: 0.1143 - lr: 9.0000e-04\n",
      "Epoch 141/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1162 - val_loss: 0.1168 - lr: 9.0000e-04\n",
      "Epoch 142/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1164 - val_loss: 0.1176 - lr: 9.0000e-04\n",
      "Epoch 143/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1164 - val_loss: 0.1169 - lr: 9.0000e-04\n",
      "Epoch 144/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1163 - val_loss: 0.1161 - lr: 9.0000e-04\n",
      "Epoch 145/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1168 - val_loss: 0.1147 - lr: 9.0000e-04\n",
      "Epoch 146/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1165 - val_loss: 0.1164 - lr: 9.0000e-04\n",
      "Epoch 147/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1162 - val_loss: 0.1159 - lr: 9.0000e-04\n",
      "Epoch 148/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1168 - val_loss: 0.1146 - lr: 9.0000e-04\n",
      "Epoch 149/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1161 - val_loss: 0.1170 - lr: 9.0000e-04\n",
      "Epoch 150/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1162 - val_loss: 0.1166 - lr: 9.0000e-04\n",
      "Epoch 151/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1167 - val_loss: 0.1193 - lr: 9.0000e-04\n",
      "Epoch 152/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1159 - val_loss: 0.1172 - lr: 9.0000e-04\n",
      "Epoch 153/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1157 - val_loss: 0.1146 - lr: 9.0000e-04\n",
      "Epoch 154/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1159 - val_loss: 0.1147 - lr: 9.0000e-04\n",
      "Epoch 155/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1160 - val_loss: 0.1149 - lr: 9.0000e-04\n",
      "Epoch 156/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1159 - val_loss: 0.1156 - lr: 9.0000e-04\n",
      "Epoch 157/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1157 - val_loss: 0.1153 - lr: 9.0000e-04\n",
      "Epoch 158/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1157 - val_loss: 0.1137 - lr: 9.0000e-04\n",
      "Epoch 159/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1164 - val_loss: 0.1147 - lr: 9.0000e-04\n",
      "Epoch 160/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1162 - val_loss: 0.1174 - lr: 9.0000e-04\n",
      "Epoch 161/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1158 - val_loss: 0.1156 - lr: 9.0000e-04\n",
      "Epoch 162/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1156 - val_loss: 0.1142 - lr: 9.0000e-04\n",
      "Epoch 163/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1163 - val_loss: 0.1188 - lr: 9.0000e-04\n",
      "Epoch 164/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1160 - val_loss: 0.1176 - lr: 9.0000e-04\n",
      "Epoch 165/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1160 - val_loss: 0.1164 - lr: 9.0000e-04\n",
      "Epoch 166/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1155 - val_loss: 0.1179 - lr: 9.0000e-04\n",
      "Epoch 167/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1158 - val_loss: 0.1158 - lr: 9.0000e-04\n",
      "Epoch 168/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1159 - val_loss: 0.1147 - lr: 9.0000e-04\n",
      "Epoch 169/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1152 - val_loss: 0.1160 - lr: 9.0000e-04\n",
      "Epoch 170/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1156 - val_loss: 0.1158 - lr: 9.0000e-04\n",
      "Epoch 171/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1156 - val_loss: 0.1172 - lr: 9.0000e-04\n",
      "Epoch 172/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1151 - val_loss: 0.1169 - lr: 9.0000e-04\n",
      "Epoch 173/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1155 - val_loss: 0.1175 - lr: 9.0000e-04\n",
      "Epoch 174/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1156 - val_loss: 0.1157 - lr: 9.0000e-04\n",
      "Epoch 175/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1151 - val_loss: 0.1156 - lr: 9.0000e-04\n",
      "Epoch 176/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1155 - val_loss: 0.1169 - lr: 9.0000e-04\n",
      "Epoch 177/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1150 - val_loss: 0.1174 - lr: 9.0000e-04\n",
      "Epoch 178/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1157 - val_loss: 0.1180 - lr: 9.0000e-04\n",
      "Epoch 179/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1156 - val_loss: 0.1173 - lr: 9.0000e-04\n",
      "Epoch 180/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1150 - val_loss: 0.1163 - lr: 9.0000e-04\n",
      "Epoch 181/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1150 - val_loss: 0.1176 - lr: 9.0000e-04\n",
      "Epoch 182/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1157 - val_loss: 0.1205 - lr: 9.0000e-04\n",
      "Epoch 183/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1149 - val_loss: 0.1176 - lr: 9.0000e-04\n",
      "Epoch 184/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1147 - val_loss: 0.1197 - lr: 9.0000e-04\n",
      "Epoch 185/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1145 - val_loss: 0.1178 - lr: 9.0000e-04\n",
      "Epoch 186/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1137 - val_loss: 0.1186 - lr: 9.0000e-04\n",
      "Epoch 187/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1137 - val_loss: 0.1221 - lr: 9.0000e-04\n",
      "Epoch 188/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1134 - val_loss: 0.1207 - lr: 9.0000e-04\n",
      "Epoch 189/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1124 - val_loss: 0.1195 - lr: 9.0000e-04\n",
      "Epoch 190/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1119 - val_loss: 0.1216 - lr: 9.0000e-04\n",
      "Epoch 191/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1113 - val_loss: 0.1216 - lr: 9.0000e-04\n",
      "Epoch 192/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1098 - val_loss: 0.1220 - lr: 9.0000e-04\n",
      "Epoch 193/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1092 - val_loss: 0.1234 - lr: 9.0000e-04\n",
      "Epoch 194/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1079 - val_loss: 0.1255 - lr: 9.0000e-04\n",
      "Epoch 195/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1084 - val_loss: 0.1238 - lr: 9.0000e-04\n",
      "Epoch 196/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1070 - val_loss: 0.1272 - lr: 9.0000e-04\n",
      "Epoch 197/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1057 - val_loss: 0.1244 - lr: 9.0000e-04\n",
      "Epoch 198/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1054 - val_loss: 0.1281 - lr: 9.0000e-04\n",
      "Epoch 199/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1048 - val_loss: 0.1301 - lr: 9.0000e-04\n",
      "Epoch 200/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1042 - val_loss: 0.1279 - lr: 9.0000e-04\n",
      "Epoch 201/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1038 - val_loss: 0.1277 - lr: 9.0000e-04\n",
      "Epoch 202/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1026 - val_loss: 0.1288 - lr: 9.0000e-04\n",
      "Epoch 203/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1019 - val_loss: 0.1302 - lr: 9.0000e-04\n",
      "Epoch 204/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1014 - val_loss: 0.1309 - lr: 9.0000e-04\n",
      "Epoch 205/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1010 - val_loss: 0.1306 - lr: 9.0000e-04\n",
      "Epoch 206/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.1008 - val_loss: 0.1318 - lr: 9.0000e-04\n",
      "Epoch 207/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0996 - val_loss: 0.1300 - lr: 9.0000e-04\n",
      "Epoch 208/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0993 - val_loss: 0.1306 - lr: 9.0000e-04\n",
      "Epoch 209/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0994 - val_loss: 0.1350 - lr: 9.0000e-04\n",
      "Epoch 210/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0987 - val_loss: 0.1339 - lr: 9.0000e-04\n",
      "Epoch 211/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0980 - val_loss: 0.1320 - lr: 9.0000e-04\n",
      "Epoch 212/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0978 - val_loss: 0.1334 - lr: 9.0000e-04\n",
      "Epoch 213/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0973 - val_loss: 0.1347 - lr: 9.0000e-04\n",
      "Epoch 214/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0965 - val_loss: 0.1374 - lr: 9.0000e-04\n",
      "Epoch 215/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0961 - val_loss: 0.1346 - lr: 9.0000e-04\n",
      "Epoch 216/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0961 - val_loss: 0.1340 - lr: 9.0000e-04\n",
      "Epoch 217/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0952 - val_loss: 0.1366 - lr: 9.0000e-04\n",
      "Epoch 218/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0963 - val_loss: 0.1354 - lr: 9.0000e-04\n",
      "Epoch 219/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0948 - val_loss: 0.1359 - lr: 9.0000e-04\n",
      "Epoch 220/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0951 - val_loss: 0.1367 - lr: 9.0000e-04\n",
      "Epoch 221/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0945 - val_loss: 0.1375 - lr: 9.0000e-04\n",
      "Epoch 222/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0945 - val_loss: 0.1350 - lr: 9.0000e-04\n",
      "Epoch 223/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0942 - val_loss: 0.1369 - lr: 9.0000e-04\n",
      "Epoch 224/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0937 - val_loss: 0.1378 - lr: 9.0000e-04\n",
      "Epoch 225/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0930 - val_loss: 0.1397 - lr: 9.0000e-04\n",
      "Epoch 226/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0928 - val_loss: 0.1391 - lr: 9.0000e-04\n",
      "Epoch 227/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0920 - val_loss: 0.1398 - lr: 9.0000e-04\n",
      "Epoch 228/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0928 - val_loss: 0.1368 - lr: 9.0000e-04\n",
      "Epoch 229/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0923 - val_loss: 0.1367 - lr: 9.0000e-04\n",
      "Epoch 230/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0917 - val_loss: 0.1381 - lr: 9.0000e-04\n",
      "Epoch 231/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0894 - val_loss: 0.1404 - lr: 8.1000e-04\n",
      "Epoch 232/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0882 - val_loss: 0.1419 - lr: 8.1000e-04\n",
      "Epoch 233/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0892 - val_loss: 0.1400 - lr: 8.1000e-04\n",
      "Epoch 234/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0882 - val_loss: 0.1393 - lr: 8.1000e-04\n",
      "Epoch 235/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0880 - val_loss: 0.1408 - lr: 8.1000e-04\n",
      "Epoch 236/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0879 - val_loss: 0.1394 - lr: 8.1000e-04\n",
      "Epoch 237/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0878 - val_loss: 0.1415 - lr: 8.1000e-04\n",
      "Epoch 238/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0874 - val_loss: 0.1399 - lr: 8.1000e-04\n",
      "Epoch 239/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0870 - val_loss: 0.1429 - lr: 8.1000e-04\n",
      "Epoch 240/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0871 - val_loss: 0.1420 - lr: 8.1000e-04\n",
      "Epoch 241/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0878 - val_loss: 0.1416 - lr: 8.1000e-04\n",
      "Epoch 242/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0871 - val_loss: 0.1412 - lr: 8.1000e-04\n",
      "Epoch 243/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0866 - val_loss: 0.1413 - lr: 8.1000e-04\n",
      "Epoch 244/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0865 - val_loss: 0.1404 - lr: 8.1000e-04\n",
      "Epoch 245/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0871 - val_loss: 0.1406 - lr: 8.1000e-04\n",
      "Epoch 246/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0867 - val_loss: 0.1421 - lr: 8.1000e-04\n",
      "Epoch 247/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0856 - val_loss: 0.1439 - lr: 8.1000e-04\n",
      "Epoch 248/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0859 - val_loss: 0.1417 - lr: 8.1000e-04\n",
      "Epoch 249/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0864 - val_loss: 0.1418 - lr: 8.1000e-04\n",
      "Epoch 250/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0858 - val_loss: 0.1418 - lr: 8.1000e-04\n",
      "Epoch 251/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0853 - val_loss: 0.1430 - lr: 8.1000e-04\n",
      "Epoch 252/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0854 - val_loss: 0.1432 - lr: 8.1000e-04\n",
      "Epoch 253/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0854 - val_loss: 0.1453 - lr: 8.1000e-04\n",
      "Epoch 254/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0849 - val_loss: 0.1424 - lr: 8.1000e-04\n",
      "Epoch 255/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0850 - val_loss: 0.1437 - lr: 8.1000e-04\n",
      "Epoch 256/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0848 - val_loss: 0.1426 - lr: 8.1000e-04\n",
      "Epoch 257/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0848 - val_loss: 0.1419 - lr: 8.1000e-04\n",
      "Epoch 258/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0844 - val_loss: 0.1424 - lr: 8.1000e-04\n",
      "Epoch 259/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0846 - val_loss: 0.1415 - lr: 8.1000e-04\n",
      "Epoch 260/500\n",
      "1279/1279 [==============================] - 5s 4ms/step - loss: 0.0836 - val_loss: 0.1460 - lr: 8.1000e-04\n",
      "Epoch 261/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0844 - val_loss: 0.1416 - lr: 8.1000e-04\n",
      "Epoch 262/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0841 - val_loss: 0.1438 - lr: 8.1000e-04\n",
      "Epoch 263/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0840 - val_loss: 0.1438 - lr: 8.1000e-04\n",
      "Epoch 264/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0841 - val_loss: 0.1421 - lr: 8.1000e-04\n",
      "Epoch 265/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0840 - val_loss: 0.1436 - lr: 8.1000e-04\n",
      "Epoch 266/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0845 - val_loss: 0.1425 - lr: 8.1000e-04\n",
      "Epoch 267/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0842 - val_loss: 0.1455 - lr: 8.1000e-04\n",
      "Epoch 268/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0839 - val_loss: 0.1440 - lr: 8.1000e-04\n",
      "Epoch 269/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0838 - val_loss: 0.1464 - lr: 8.1000e-04\n",
      "Epoch 270/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0837 - val_loss: 0.1436 - lr: 8.1000e-04\n",
      "Epoch 271/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0836 - val_loss: 0.1444 - lr: 8.1000e-04\n",
      "Epoch 272/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0835 - val_loss: 0.1449 - lr: 8.1000e-04\n",
      "Epoch 273/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0837 - val_loss: 0.1440 - lr: 8.1000e-04\n",
      "Epoch 274/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0830 - val_loss: 0.1431 - lr: 8.1000e-04\n",
      "Epoch 275/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0831 - val_loss: 0.1452 - lr: 8.1000e-04\n",
      "Epoch 276/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0835 - val_loss: 0.1448 - lr: 8.1000e-04\n",
      "Epoch 277/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0824 - val_loss: 0.1444 - lr: 8.1000e-04\n",
      "Epoch 278/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0831 - val_loss: 0.1443 - lr: 8.1000e-04\n",
      "Epoch 279/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0827 - val_loss: 0.1450 - lr: 8.1000e-04\n",
      "Epoch 280/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0833 - val_loss: 0.1434 - lr: 8.1000e-04\n",
      "Epoch 281/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0829 - val_loss: 0.1457 - lr: 8.1000e-04\n",
      "Epoch 282/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0823 - val_loss: 0.1441 - lr: 8.1000e-04\n",
      "Epoch 283/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0827 - val_loss: 0.1470 - lr: 8.1000e-04\n",
      "Epoch 284/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0828 - val_loss: 0.1453 - lr: 8.1000e-04\n",
      "Epoch 285/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0831 - val_loss: 0.1435 - lr: 8.1000e-04\n",
      "Epoch 286/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0827 - val_loss: 0.1469 - lr: 8.1000e-04\n",
      "Epoch 287/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0828 - val_loss: 0.1471 - lr: 8.1000e-04\n",
      "Epoch 288/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0823 - val_loss: 0.1444 - lr: 8.1000e-04\n",
      "Epoch 289/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0830 - val_loss: 0.1458 - lr: 8.1000e-04\n",
      "Epoch 290/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0823 - val_loss: 0.1454 - lr: 8.1000e-04\n",
      "Epoch 291/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0834 - val_loss: 0.1452 - lr: 8.1000e-04\n",
      "Epoch 292/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0816 - val_loss: 0.1457 - lr: 8.1000e-04\n",
      "Epoch 293/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0823 - val_loss: 0.1434 - lr: 8.1000e-04\n",
      "Epoch 294/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0813 - val_loss: 0.1470 - lr: 8.1000e-04\n",
      "Epoch 295/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0820 - val_loss: 0.1422 - lr: 8.1000e-04\n",
      "Epoch 296/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0825 - val_loss: 0.1436 - lr: 8.1000e-04\n",
      "Epoch 297/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0822 - val_loss: 0.1417 - lr: 8.1000e-04\n",
      "Epoch 298/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0818 - val_loss: 0.1455 - lr: 8.1000e-04\n",
      "Epoch 299/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0813 - val_loss: 0.1454 - lr: 8.1000e-04\n",
      "Epoch 300/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0820 - val_loss: 0.1434 - lr: 8.1000e-04\n",
      "Epoch 301/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0819 - val_loss: 0.1453 - lr: 8.1000e-04\n",
      "Epoch 302/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0818 - val_loss: 0.1463 - lr: 8.1000e-04\n",
      "Epoch 303/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0821 - val_loss: 0.1470 - lr: 8.1000e-04\n",
      "Epoch 304/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0817 - val_loss: 0.1459 - lr: 8.1000e-04\n",
      "Epoch 305/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0817 - val_loss: 0.1463 - lr: 8.1000e-04\n",
      "Epoch 306/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0813 - val_loss: 0.1433 - lr: 8.1000e-04\n",
      "Epoch 307/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0815 - val_loss: 0.1454 - lr: 8.1000e-04\n",
      "Epoch 308/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0818 - val_loss: 0.1443 - lr: 8.1000e-04\n",
      "Epoch 309/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0815 - val_loss: 0.1450 - lr: 8.1000e-04\n",
      "Epoch 310/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0816 - val_loss: 0.1461 - lr: 8.1000e-04\n",
      "Epoch 311/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0813 - val_loss: 0.1467 - lr: 8.1000e-04\n",
      "Epoch 312/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0816 - val_loss: 0.1458 - lr: 8.1000e-04\n",
      "Epoch 313/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0820 - val_loss: 0.1449 - lr: 8.1000e-04\n",
      "Epoch 314/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0810 - val_loss: 0.1444 - lr: 8.1000e-04\n",
      "Epoch 315/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0815 - val_loss: 0.1459 - lr: 8.1000e-04\n",
      "Epoch 316/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0818 - val_loss: 0.1458 - lr: 8.1000e-04\n",
      "Epoch 317/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0811 - val_loss: 0.1438 - lr: 8.1000e-04\n",
      "Epoch 318/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0812 - val_loss: 0.1445 - lr: 8.1000e-04\n",
      "Epoch 319/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0813 - val_loss: 0.1460 - lr: 8.1000e-04\n",
      "Epoch 320/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0808 - val_loss: 0.1473 - lr: 8.1000e-04\n",
      "Epoch 321/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0807 - val_loss: 0.1457 - lr: 8.1000e-04\n",
      "Epoch 322/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0817 - val_loss: 0.1449 - lr: 8.1000e-04\n",
      "Epoch 323/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0805 - val_loss: 0.1469 - lr: 8.1000e-04\n",
      "Epoch 324/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0809 - val_loss: 0.1449 - lr: 8.1000e-04\n",
      "Epoch 325/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0807 - val_loss: 0.1445 - lr: 8.1000e-04\n",
      "Epoch 326/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0810 - val_loss: 0.1455 - lr: 8.1000e-04\n",
      "Epoch 327/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0812 - val_loss: 0.1502 - lr: 8.1000e-04\n",
      "Epoch 328/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0813 - val_loss: 0.1465 - lr: 8.1000e-04\n",
      "Epoch 329/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0800 - val_loss: 0.1454 - lr: 8.1000e-04\n",
      "Epoch 330/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0808 - val_loss: 0.1454 - lr: 8.1000e-04\n",
      "Epoch 331/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0794 - val_loss: 0.1449 - lr: 7.2900e-04\n",
      "Epoch 332/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0791 - val_loss: 0.1459 - lr: 7.2900e-04\n",
      "Epoch 333/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0790 - val_loss: 0.1472 - lr: 7.2900e-04\n",
      "Epoch 334/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0786 - val_loss: 0.1470 - lr: 7.2900e-04\n",
      "Epoch 335/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0784 - val_loss: 0.1457 - lr: 7.2900e-04\n",
      "Epoch 336/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0787 - val_loss: 0.1455 - lr: 7.2900e-04\n",
      "Epoch 337/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0787 - val_loss: 0.1457 - lr: 7.2900e-04\n",
      "Epoch 338/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0790 - val_loss: 0.1446 - lr: 7.2900e-04\n",
      "Epoch 339/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0789 - val_loss: 0.1480 - lr: 7.2900e-04\n",
      "Epoch 340/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0789 - val_loss: 0.1465 - lr: 7.2900e-04\n",
      "Epoch 341/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0788 - val_loss: 0.1471 - lr: 7.2900e-04\n",
      "Epoch 342/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0782 - val_loss: 0.1455 - lr: 7.2900e-04\n",
      "Epoch 343/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0787 - val_loss: 0.1449 - lr: 7.2900e-04\n",
      "Epoch 344/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0788 - val_loss: 0.1438 - lr: 7.2900e-04\n",
      "Epoch 345/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0780 - val_loss: 0.1466 - lr: 7.2900e-04\n",
      "Epoch 346/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0780 - val_loss: 0.1461 - lr: 7.2900e-04\n",
      "Epoch 347/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0779 - val_loss: 0.1474 - lr: 7.2900e-04\n",
      "Epoch 348/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0786 - val_loss: 0.1460 - lr: 7.2900e-04\n",
      "Epoch 349/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0790 - val_loss: 0.1466 - lr: 7.2900e-04\n",
      "Epoch 350/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0784 - val_loss: 0.1475 - lr: 7.2900e-04\n",
      "Epoch 351/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0784 - val_loss: 0.1450 - lr: 7.2900e-04\n",
      "Epoch 352/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0789 - val_loss: 0.1465 - lr: 7.2900e-04\n",
      "Epoch 353/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0779 - val_loss: 0.1463 - lr: 7.2900e-04\n",
      "Epoch 354/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0775 - val_loss: 0.1452 - lr: 7.2900e-04\n",
      "Epoch 355/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0779 - val_loss: 0.1465 - lr: 7.2900e-04\n",
      "Epoch 356/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0776 - val_loss: 0.1464 - lr: 7.2900e-04\n",
      "Epoch 357/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0781 - val_loss: 0.1469 - lr: 7.2900e-04\n",
      "Epoch 358/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0776 - val_loss: 0.1464 - lr: 7.2900e-04\n",
      "Epoch 359/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0786 - val_loss: 0.1455 - lr: 7.2900e-04\n",
      "Epoch 360/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0777 - val_loss: 0.1442 - lr: 7.2900e-04\n",
      "Epoch 361/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0779 - val_loss: 0.1454 - lr: 7.2900e-04\n",
      "Epoch 362/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0776 - val_loss: 0.1443 - lr: 7.2900e-04\n",
      "Epoch 363/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0781 - val_loss: 0.1459 - lr: 7.2900e-04\n",
      "Epoch 364/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0771 - val_loss: 0.1474 - lr: 7.2900e-04\n",
      "Epoch 365/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0780 - val_loss: 0.1469 - lr: 7.2900e-04\n",
      "Epoch 366/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0782 - val_loss: 0.1471 - lr: 7.2900e-04\n",
      "Epoch 367/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0777 - val_loss: 0.1445 - lr: 7.2900e-04\n",
      "Epoch 368/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0784 - val_loss: 0.1466 - lr: 7.2900e-04\n",
      "Epoch 369/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0775 - val_loss: 0.1482 - lr: 7.2900e-04\n",
      "Epoch 370/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0770 - val_loss: 0.1473 - lr: 7.2900e-04\n",
      "Epoch 371/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0781 - val_loss: 0.1472 - lr: 7.2900e-04\n",
      "Epoch 372/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0780 - val_loss: 0.1461 - lr: 7.2900e-04\n",
      "Epoch 373/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0776 - val_loss: 0.1458 - lr: 7.2900e-04\n",
      "Epoch 374/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0775 - val_loss: 0.1479 - lr: 7.2900e-04\n",
      "Epoch 375/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0780 - val_loss: 0.1480 - lr: 7.2900e-04\n",
      "Epoch 376/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0770 - val_loss: 0.1452 - lr: 7.2900e-04\n",
      "Epoch 377/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0778 - val_loss: 0.1486 - lr: 7.2900e-04\n",
      "Epoch 378/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0780 - val_loss: 0.1459 - lr: 7.2900e-04\n",
      "Epoch 379/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0769 - val_loss: 0.1484 - lr: 7.2900e-04\n",
      "Epoch 380/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0782 - val_loss: 0.1470 - lr: 7.2900e-04\n",
      "Epoch 381/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0775 - val_loss: 0.1480 - lr: 7.2900e-04\n",
      "Epoch 382/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0780 - val_loss: 0.1444 - lr: 7.2900e-04\n",
      "Epoch 383/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0783 - val_loss: 0.1469 - lr: 7.2900e-04\n",
      "Epoch 384/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0775 - val_loss: 0.1467 - lr: 7.2900e-04\n",
      "Epoch 385/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0778 - val_loss: 0.1447 - lr: 7.2900e-04\n",
      "Epoch 386/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0773 - val_loss: 0.1463 - lr: 7.2900e-04\n",
      "Epoch 387/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0775 - val_loss: 0.1453 - lr: 7.2900e-04\n",
      "Epoch 388/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0777 - val_loss: 0.1483 - lr: 7.2900e-04\n",
      "Epoch 389/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0773 - val_loss: 0.1474 - lr: 7.2900e-04\n",
      "Epoch 390/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0772 - val_loss: 0.1465 - lr: 7.2900e-04\n",
      "Epoch 391/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0774 - val_loss: 0.1493 - lr: 7.2900e-04\n",
      "Epoch 392/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0769 - val_loss: 0.1489 - lr: 7.2900e-04\n",
      "Epoch 393/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0780 - val_loss: 0.1475 - lr: 7.2900e-04\n",
      "Epoch 394/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0775 - val_loss: 0.1477 - lr: 7.2900e-04\n",
      "Epoch 395/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0778 - val_loss: 0.1450 - lr: 7.2900e-04\n",
      "Epoch 396/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0773 - val_loss: 0.1472 - lr: 7.2900e-04\n",
      "Epoch 397/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0774 - val_loss: 0.1463 - lr: 7.2900e-04\n",
      "Epoch 398/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0772 - val_loss: 0.1466 - lr: 7.2900e-04\n",
      "Epoch 399/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0778 - val_loss: 0.1452 - lr: 7.2900e-04\n",
      "Epoch 400/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0769 - val_loss: 0.1453 - lr: 7.2900e-04\n",
      "Epoch 401/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0771 - val_loss: 0.1455 - lr: 7.2900e-04\n",
      "Epoch 402/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0772 - val_loss: 0.1475 - lr: 7.2900e-04\n",
      "Epoch 403/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0776 - val_loss: 0.1465 - lr: 7.2900e-04\n",
      "Epoch 404/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0769 - val_loss: 0.1467 - lr: 7.2900e-04\n",
      "Epoch 405/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0772 - val_loss: 0.1489 - lr: 7.2900e-04\n",
      "Epoch 406/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0773 - val_loss: 0.1484 - lr: 7.2900e-04\n",
      "Epoch 407/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0772 - val_loss: 0.1466 - lr: 7.2900e-04\n",
      "Epoch 408/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0773 - val_loss: 0.1463 - lr: 7.2900e-04\n",
      "Epoch 409/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0773 - val_loss: 0.1499 - lr: 7.2900e-04\n",
      "Epoch 410/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0771 - val_loss: 0.1462 - lr: 7.2900e-04\n",
      "Epoch 411/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0772 - val_loss: 0.1466 - lr: 7.2900e-04\n",
      "Epoch 412/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0763 - val_loss: 0.1475 - lr: 7.2900e-04\n",
      "Epoch 413/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0775 - val_loss: 0.1472 - lr: 7.2900e-04\n",
      "Epoch 414/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0776 - val_loss: 0.1462 - lr: 7.2900e-04\n",
      "Epoch 415/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0768 - val_loss: 0.1488 - lr: 7.2900e-04\n",
      "Epoch 416/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0767 - val_loss: 0.1478 - lr: 7.2900e-04\n",
      "Epoch 417/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0770 - val_loss: 0.1498 - lr: 7.2900e-04\n",
      "Epoch 418/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0770 - val_loss: 0.1479 - lr: 7.2900e-04\n",
      "Epoch 419/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0769 - val_loss: 0.1487 - lr: 7.2900e-04\n",
      "Epoch 420/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0773 - val_loss: 0.1459 - lr: 7.2900e-04\n",
      "Epoch 421/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0775 - val_loss: 0.1469 - lr: 7.2900e-04\n",
      "Epoch 422/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0766 - val_loss: 0.1468 - lr: 7.2900e-04\n",
      "Epoch 423/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0766 - val_loss: 0.1461 - lr: 7.2900e-04\n",
      "Epoch 424/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0769 - val_loss: 0.1468 - lr: 7.2900e-04\n",
      "Epoch 425/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0770 - val_loss: 0.1456 - lr: 7.2900e-04\n",
      "Epoch 426/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0766 - val_loss: 0.1478 - lr: 7.2900e-04\n",
      "Epoch 427/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0770 - val_loss: 0.1509 - lr: 7.2900e-04\n",
      "Epoch 428/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0766 - val_loss: 0.1488 - lr: 7.2900e-04\n",
      "Epoch 429/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0761 - val_loss: 0.1474 - lr: 7.2900e-04\n",
      "Epoch 430/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0772 - val_loss: 0.1464 - lr: 7.2900e-04\n",
      "Epoch 431/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0756 - val_loss: 0.1458 - lr: 6.5610e-04\n",
      "Epoch 432/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0752 - val_loss: 0.1479 - lr: 6.5610e-04\n",
      "Epoch 433/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0758 - val_loss: 0.1469 - lr: 6.5610e-04\n",
      "Epoch 434/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0759 - val_loss: 0.1462 - lr: 6.5610e-04\n",
      "Epoch 435/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0754 - val_loss: 0.1468 - lr: 6.5610e-04\n",
      "Epoch 436/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0750 - val_loss: 0.1458 - lr: 6.5610e-04\n",
      "Epoch 437/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0751 - val_loss: 0.1463 - lr: 6.5610e-04\n",
      "Epoch 438/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0754 - val_loss: 0.1496 - lr: 6.5610e-04\n",
      "Epoch 439/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0753 - val_loss: 0.1451 - lr: 6.5610e-04\n",
      "Epoch 440/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0747 - val_loss: 0.1462 - lr: 6.5610e-04\n",
      "Epoch 441/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0751 - val_loss: 0.1466 - lr: 6.5610e-04\n",
      "Epoch 442/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0755 - val_loss: 0.1480 - lr: 6.5610e-04\n",
      "Epoch 443/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0750 - val_loss: 0.1464 - lr: 6.5610e-04\n",
      "Epoch 444/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0751 - val_loss: 0.1479 - lr: 6.5610e-04\n",
      "Epoch 445/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0746 - val_loss: 0.1471 - lr: 6.5610e-04\n",
      "Epoch 446/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0750 - val_loss: 0.1470 - lr: 6.5610e-04\n",
      "Epoch 447/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0750 - val_loss: 0.1470 - lr: 6.5610e-04\n",
      "Epoch 448/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0750 - val_loss: 0.1474 - lr: 6.5610e-04\n",
      "Epoch 449/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0749 - val_loss: 0.1478 - lr: 6.5610e-04\n",
      "Epoch 450/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0752 - val_loss: 0.1477 - lr: 6.5610e-04\n",
      "Epoch 451/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0751 - val_loss: 0.1478 - lr: 6.5610e-04\n",
      "Epoch 452/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0745 - val_loss: 0.1494 - lr: 6.5610e-04\n",
      "Epoch 453/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0748 - val_loss: 0.1490 - lr: 6.5610e-04\n",
      "Epoch 454/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0746 - val_loss: 0.1464 - lr: 6.5610e-04\n",
      "Epoch 455/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0752 - val_loss: 0.1474 - lr: 6.5610e-04\n",
      "Epoch 456/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0744 - val_loss: 0.1453 - lr: 6.5610e-04\n",
      "Epoch 457/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0748 - val_loss: 0.1475 - lr: 6.5610e-04\n",
      "Epoch 458/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0752 - val_loss: 0.1475 - lr: 6.5610e-04\n",
      "Epoch 459/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0751 - val_loss: 0.1471 - lr: 6.5610e-04\n",
      "Epoch 460/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0749 - val_loss: 0.1465 - lr: 6.5610e-04\n",
      "Epoch 461/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0748 - val_loss: 0.1455 - lr: 6.5610e-04\n",
      "Epoch 462/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0747 - val_loss: 0.1479 - lr: 6.5610e-04\n",
      "Epoch 463/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0746 - val_loss: 0.1475 - lr: 6.5610e-04\n",
      "Epoch 464/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0748 - val_loss: 0.1508 - lr: 6.5610e-04\n",
      "Epoch 465/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0749 - val_loss: 0.1472 - lr: 6.5610e-04\n",
      "Epoch 466/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0745 - val_loss: 0.1514 - lr: 6.5610e-04\n",
      "Epoch 467/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0745 - val_loss: 0.1484 - lr: 6.5610e-04\n",
      "Epoch 468/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0750 - val_loss: 0.1468 - lr: 6.5610e-04\n",
      "Epoch 469/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0750 - val_loss: 0.1490 - lr: 6.5610e-04\n",
      "Epoch 470/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0747 - val_loss: 0.1456 - lr: 6.5610e-04\n",
      "Epoch 471/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0749 - val_loss: 0.1464 - lr: 6.5610e-04\n",
      "Epoch 472/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0748 - val_loss: 0.1486 - lr: 6.5610e-04\n",
      "Epoch 473/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0741 - val_loss: 0.1484 - lr: 6.5610e-04\n",
      "Epoch 474/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0747 - val_loss: 0.1488 - lr: 6.5610e-04\n",
      "Epoch 475/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0747 - val_loss: 0.1481 - lr: 6.5610e-04\n",
      "Epoch 476/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0745 - val_loss: 0.1495 - lr: 6.5610e-04\n",
      "Epoch 477/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0747 - val_loss: 0.1484 - lr: 6.5610e-04\n",
      "Epoch 478/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0755 - val_loss: 0.1466 - lr: 6.5610e-04\n",
      "Epoch 479/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0746 - val_loss: 0.1484 - lr: 6.5610e-04\n",
      "Epoch 480/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0743 - val_loss: 0.1483 - lr: 6.5610e-04\n",
      "Epoch 481/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0748 - val_loss: 0.1480 - lr: 6.5610e-04\n",
      "Epoch 482/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0747 - val_loss: 0.1480 - lr: 6.5610e-04\n",
      "Epoch 483/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0744 - val_loss: 0.1468 - lr: 6.5610e-04\n",
      "Epoch 484/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0747 - val_loss: 0.1486 - lr: 6.5610e-04\n",
      "Epoch 485/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0747 - val_loss: 0.1469 - lr: 6.5610e-04\n",
      "Epoch 486/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0747 - val_loss: 0.1458 - lr: 6.5610e-04\n",
      "Epoch 487/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0746 - val_loss: 0.1455 - lr: 6.5610e-04\n",
      "Epoch 488/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0745 - val_loss: 0.1480 - lr: 6.5610e-04\n",
      "Epoch 489/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0748 - val_loss: 0.1481 - lr: 6.5610e-04\n",
      "Epoch 490/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0744 - val_loss: 0.1477 - lr: 6.5610e-04\n",
      "Epoch 491/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0752 - val_loss: 0.1472 - lr: 6.5610e-04\n",
      "Epoch 492/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0745 - val_loss: 0.1482 - lr: 6.5610e-04\n",
      "Epoch 493/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0742 - val_loss: 0.1495 - lr: 6.5610e-04\n",
      "Epoch 494/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0744 - val_loss: 0.1502 - lr: 6.5610e-04\n",
      "Epoch 495/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0745 - val_loss: 0.1503 - lr: 6.5610e-04\n",
      "Epoch 496/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0744 - val_loss: 0.1495 - lr: 6.5610e-04\n",
      "Epoch 497/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0743 - val_loss: 0.1486 - lr: 6.5610e-04\n",
      "Epoch 498/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0741 - val_loss: 0.1488 - lr: 6.5610e-04\n",
      "Epoch 499/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0744 - val_loss: 0.1479 - lr: 6.5610e-04\n",
      "Epoch 500/500\n",
      "1279/1279 [==============================] - 4s 3ms/step - loss: 0.0745 - val_loss: 0.1505 - lr: 6.5610e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f023c792f50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# # 设置 EarlyStopping 回调函数，如果验证集的损失不再改善，则停止训练\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "# lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.9, patience=5)\n",
    "# model.fit(   # 根据情况调整参数\n",
    "#     train_features,\n",
    "#     train_labels,\n",
    "#     validation_data=(val_features, val_labels),\n",
    "#     epochs=200,\n",
    "#     batch_size=32,\n",
    "#     callbacks=[lr_scheduler]\n",
    "# )\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 设置 EarlyStopping 回调函数，如果验证集的损失不再改善，则停止训练\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.9, patience=100)\n",
    "model.fit( # 根据情况调整参数\n",
    "train_features,\n",
    "train_labels,\n",
    "validation_data=(val_features, val_labels),\n",
    "epochs=500,\n",
    "batch_size=32,\n",
    "callbacks=[lr_scheduler]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# baselin_acc=model.evaluate(val_features,val_labels,verbose=0)\n",
    "# print(baselin_acc)\n",
    "# best_features=[]\n",
    "# best_accurary=0.0\n",
    "# for i in range(train_features.shape[1]):\n",
    "#     selected_feature=best_features+[i]\n",
    "#     selected_train_features=train_features[:,selected_feature]\n",
    "#     selected_val_features=val_features[:,selected_feature]\n",
    "    \n",
    "# selected_feature=pd.DataFrame(selected_feature)   \n",
    "# select_model = tf.keras.Sequential([\n",
    "#     normalizer,  # 归一化层作为第一层\n",
    "#     layers.Dense(300, activation=\"relu\", input_dim=selected_feature.shape[1]),\n",
    "#     layers.Dense(300, activation=\"relu\"),\n",
    "#     layers.Dense(300, activation=\"relu\"),\n",
    "#     layers.Dense(train_labels.shape[1])\n",
    "#     # normalizer,  # 归一化层作为第一层\n",
    "#     # layers.Dense(100, activation=\"relu\"),\n",
    "#     # layers.Dense(train_labels.shape[1])\n",
    "# ])\n",
    "# select_model.compile(loss=\"mse\", optimizer=\"adam\")  # 根据情况调整参数\n",
    "# select_model.summary()\n",
    "\n",
    "\n",
    "# select_model.fit(   # 根据情况调整参数\n",
    "#     selected_train_features,\n",
    "#     train_labels,\n",
    "#     validation_data=(selected_val_features, val_labels),\n",
    "#     epochs=50,\n",
    "#     batch_size=32,\n",
    "#     callbacks=[lr_scheduler]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试模型训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 MSE:0.0302\n",
      "y1 MSE:2771.0000 ------ 2771\n"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(test_features)\n",
    "print(\"y1 MSE:%.4f\" % mean_squared_error(test_labels, test_preds))\n",
    "print(\"y1 MSE:%.4f\" % len(test_labels), '------',len(test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型部署\n",
    "\n",
    "误差补偿模型的部署路径为<I>v1/models/slot0/versions/<版本号>/</I> ，且版本号必须为数字。注意，tensorflow-serving在加载模型的时候会自动加载版本号最高的模型，并卸载低版本号的模型。因此，每次部署新部署模型时需要递增版本号。由于我们的系统已经预置了一个低精度版本的模型，并且将版本号设置为1，所以用户在部署自定义模型时应当至少将版本号设置为2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /models/slot0/11111111111111/assets\n"
     ]
    }
   ],
   "source": [
    "model_version = 11111111111111\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    f'/models/slot0/{model_version}/', # v1/models/slot0/为tensorflow-serving的模型根目录\n",
    "    overwrite=True,\n",
    "    include_optimizer=True,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，tensorflow-serving卸载旧版本模型并加载新版本模型的过程往往需要数十秒的时间，在次期间对模型发送请求会得到“Servable not found for request”的错误。用户可以使用<I>docker logs adjustment-serving-container</I>查看是否已经加载完毕。\n",
    "\n",
    "接下来我们测试是否部署成功："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"inputs\": [[1.4158, 2.9711, 10.7935, 7.5279, 2.3352, 8.1042, 2.3096, 3.3367, 11.8639, 12.7142, 1.8581, 0.3898, 19.8309, 19.771, 0.0001, 1.7768, 171.764, 1434.24]]}\n",
      "{'outputs': [[0.37509656, -0.888041139, 0.309078217, -0.13684985, -0.864648044, 0.756150842, 0.469049394, 1.89120865]]}\n",
      "{'outputs': [[0.331511, -0.932553, 0.285048, -0.1435, -0.833982, 0.767568, 0.463969, 1.9048]]\n",
      "y1 MSE:0.0007\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "test_features=pd.DataFrame(test_features)\n",
    "test_labels=pd.DataFrame(test_labels)\n",
    "req_data = json.dumps({\n",
    "            'inputs': test_features.values[:1].tolist()\n",
    "        })  \n",
    "print(req_data)\n",
    "response = requests.post(f'http://fireeye-test-model-container:8501/v1/models/slot0/versions/{model_version}:predict', # 根据部署地址填写\n",
    "                         data=req_data,\n",
    "                         headers={\"content-type\": \"application/json\"})\n",
    "if response.status_code != 200:\n",
    "    raise RuntimeError('Request tf-serving failed: ' + response.text)\n",
    "resp_data = json.loads(response.text)    \n",
    "if 'outputs' not in resp_data \\\n",
    "                    or type(resp_data['outputs']) is not list:\n",
    "    raise ValueError('Malformed tf-serving response')\n",
    "\n",
    "print(resp_data)\n",
    "print(\"{'outputs':\",test_labels.values[:1].tolist())\n",
    "\n",
    "print(\"y1 MSE:%.4f\" % mean_squared_error(test_labels.values[:1].tolist(), resp_data['outputs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试成功之后，用户需要在web页面配置相关任务的服务地址，地址的格式为：<I>“http://fireeye-test-model-container:8501/v1/models/slot0/versions/<版本号>:predict ”</I>。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "regression.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
