{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9774d2d4-2fa6-40d2-9d2f-f59e23c00eba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 加载训练数据\n",
    "train_dataset = pd.read_csv('./adjustments.tsv',\n",
    "                          sep='\\t',\n",
    "                          skipinitialspace=True)\n",
    "# 加载测试数据     ！！！ 测试数据集为真实值，不能进行调整，否则将会导致实际模型测试结果和真实预测结果存在偏差，使得最终加工的作品和预期不一致\n",
    "test_dataset = pd.read_csv('./test_adjustments.tsv',\n",
    "                          sep='\\t',\n",
    "                          skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ff9bac-642c-46ef-a1d5-dcfe243e1261",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>特征0</th>\n",
       "      <th>特征1</th>\n",
       "      <th>特征2</th>\n",
       "      <th>特征3</th>\n",
       "      <th>特征4</th>\n",
       "      <th>特征5</th>\n",
       "      <th>特征6</th>\n",
       "      <th>特征7</th>\n",
       "      <th>特征8</th>\n",
       "      <th>特征9</th>\n",
       "      <th>...</th>\n",
       "      <th>特征16</th>\n",
       "      <th>特征17</th>\n",
       "      <th>补偿0</th>\n",
       "      <th>补偿1</th>\n",
       "      <th>补偿2</th>\n",
       "      <th>补偿3</th>\n",
       "      <th>补偿4</th>\n",
       "      <th>补偿5</th>\n",
       "      <th>补偿6</th>\n",
       "      <th>补偿7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4158</td>\n",
       "      <td>2.9711</td>\n",
       "      <td>10.7935</td>\n",
       "      <td>7.5279</td>\n",
       "      <td>2.3352</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>2.3096</td>\n",
       "      <td>3.3367</td>\n",
       "      <td>11.8639</td>\n",
       "      <td>12.7142</td>\n",
       "      <td>...</td>\n",
       "      <td>171.764</td>\n",
       "      <td>1434.24</td>\n",
       "      <td>0.331511</td>\n",
       "      <td>-0.932553</td>\n",
       "      <td>0.285048</td>\n",
       "      <td>-0.1435</td>\n",
       "      <td>-0.833982</td>\n",
       "      <td>0.767568</td>\n",
       "      <td>0.463969</td>\n",
       "      <td>1.904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6280</td>\n",
       "      <td>1.8616</td>\n",
       "      <td>10.1770</td>\n",
       "      <td>7.4684</td>\n",
       "      <td>2.1915</td>\n",
       "      <td>8.5945</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>2.9661</td>\n",
       "      <td>11.5816</td>\n",
       "      <td>12.2487</td>\n",
       "      <td>...</td>\n",
       "      <td>185.824</td>\n",
       "      <td>1469.19</td>\n",
       "      <td>0.894066</td>\n",
       "      <td>-0.446796</td>\n",
       "      <td>0.058519</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.715252</td>\n",
       "      <td>0.999105</td>\n",
       "      <td>0.988844</td>\n",
       "      <td>0.689742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9648</td>\n",
       "      <td>1.8103</td>\n",
       "      <td>10.1682</td>\n",
       "      <td>5.9705</td>\n",
       "      <td>2.0629</td>\n",
       "      <td>6.5349</td>\n",
       "      <td>2.8694</td>\n",
       "      <td>3.1185</td>\n",
       "      <td>11.7464</td>\n",
       "      <td>12.2074</td>\n",
       "      <td>...</td>\n",
       "      <td>187.576</td>\n",
       "      <td>1540.76</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.460716</td>\n",
       "      <td>0.997809</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.723031</td>\n",
       "      <td>0.992935</td>\n",
       "      <td>0.682903</td>\n",
       "      <td>0.749580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7119</td>\n",
       "      <td>1.6221</td>\n",
       "      <td>10.1487</td>\n",
       "      <td>6.8678</td>\n",
       "      <td>2.0694</td>\n",
       "      <td>6.8806</td>\n",
       "      <td>1.5791</td>\n",
       "      <td>2.3003</td>\n",
       "      <td>11.5545</td>\n",
       "      <td>12.0659</td>\n",
       "      <td>...</td>\n",
       "      <td>189.938</td>\n",
       "      <td>1498.29</td>\n",
       "      <td>0.998794</td>\n",
       "      <td>-0.862448</td>\n",
       "      <td>0.329694</td>\n",
       "      <td>-0.4624</td>\n",
       "      <td>0.891745</td>\n",
       "      <td>0.015078</td>\n",
       "      <td>0.997127</td>\n",
       "      <td>0.984439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3797</td>\n",
       "      <td>1.6852</td>\n",
       "      <td>10.9601</td>\n",
       "      <td>5.0035</td>\n",
       "      <td>3.1659</td>\n",
       "      <td>5.9471</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>2.6402</td>\n",
       "      <td>11.7458</td>\n",
       "      <td>12.7041</td>\n",
       "      <td>...</td>\n",
       "      <td>181.275</td>\n",
       "      <td>1465.11</td>\n",
       "      <td>0.442893</td>\n",
       "      <td>-0.990703</td>\n",
       "      <td>-0.151400</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.617880</td>\n",
       "      <td>-0.506397</td>\n",
       "      <td>-0.997502</td>\n",
       "      <td>0.376126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      特征0     特征1      特征2     特征3     特征4     特征5     特征6     特征7      特征8  \\\n",
       "0  1.4158  2.9711  10.7935  7.5279  2.3352  8.1042  2.3096  3.3367  11.8639   \n",
       "1  0.6280  1.8616  10.1770  7.4684  2.1915  8.5945  0.1379  2.9661  11.5816   \n",
       "2  0.9648  1.8103  10.1682  5.9705  2.0629  6.5349  2.8694  3.1185  11.7464   \n",
       "3  0.7119  1.6221  10.1487  6.8678  2.0694  6.8806  1.5791  2.3003  11.5545   \n",
       "4  0.3797  1.6852  10.9601  5.0035  3.1659  5.9471  0.0858  2.6402  11.7458   \n",
       "\n",
       "       特征9  ...     特征16     特征17       补偿0       补偿1       补偿2     补偿3  \\\n",
       "0  12.7142  ...  171.764  1434.24  0.331511 -0.932553  0.285048 -0.1435   \n",
       "1  12.2487  ...  185.824  1469.19  0.894066 -0.446796  0.058519 -0.4624   \n",
       "2  12.2074  ...  187.576  1540.76  0.999982  0.460716  0.997809 -0.4624   \n",
       "3  12.0659  ...  189.938  1498.29  0.998794 -0.862448  0.329694 -0.4624   \n",
       "4  12.7041  ...  181.275  1465.11  0.442893 -0.990703 -0.151400  0.2245   \n",
       "\n",
       "        补偿4       补偿5       补偿6       补偿7  \n",
       "0 -0.833982  0.767568  0.463969  1.904800  \n",
       "1  0.715252  0.999105  0.988844  0.689742  \n",
       "2  0.723031  0.992935  0.682903  0.749580  \n",
       "3  0.891745  0.015078  0.997127  0.984439  \n",
       "4  0.617880 -0.506397 -0.997502  0.376126  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "dataset = train_dataset.copy()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf23b092-bf2a-4921-a67d-b183f589d185",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 26)\n",
      "(2998, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>特征0</th>\n",
       "      <th>特征1</th>\n",
       "      <th>特征2</th>\n",
       "      <th>特征3</th>\n",
       "      <th>特征4</th>\n",
       "      <th>特征5</th>\n",
       "      <th>特征6</th>\n",
       "      <th>特征7</th>\n",
       "      <th>特征8</th>\n",
       "      <th>特征9</th>\n",
       "      <th>...</th>\n",
       "      <th>特征16</th>\n",
       "      <th>特征17</th>\n",
       "      <th>补偿0</th>\n",
       "      <th>补偿1</th>\n",
       "      <th>补偿2</th>\n",
       "      <th>补偿3</th>\n",
       "      <th>补偿4</th>\n",
       "      <th>补偿5</th>\n",
       "      <th>补偿6</th>\n",
       "      <th>补偿7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.008837</td>\n",
       "      <td>2.018090</td>\n",
       "      <td>10.501365</td>\n",
       "      <td>6.980399</td>\n",
       "      <td>2.991379</td>\n",
       "      <td>6.986831</td>\n",
       "      <td>1.983407</td>\n",
       "      <td>2.994988</td>\n",
       "      <td>11.739018</td>\n",
       "      <td>12.600981</td>\n",
       "      <td>...</td>\n",
       "      <td>172.242767</td>\n",
       "      <td>1474.643939</td>\n",
       "      <td>0.525279</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.250365</td>\n",
       "      <td>0.010439</td>\n",
       "      <td>-0.269551</td>\n",
       "      <td>0.524640</td>\n",
       "      <td>0.222959</td>\n",
       "      <td>0.561442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.573132</td>\n",
       "      <td>0.578950</td>\n",
       "      <td>0.286296</td>\n",
       "      <td>1.134771</td>\n",
       "      <td>0.574884</td>\n",
       "      <td>1.142659</td>\n",
       "      <td>1.144038</td>\n",
       "      <td>0.577503</td>\n",
       "      <td>0.128526</td>\n",
       "      <td>0.196063</td>\n",
       "      <td>...</td>\n",
       "      <td>12.198489</td>\n",
       "      <td>54.894522</td>\n",
       "      <td>0.399656</td>\n",
       "      <td>0.699439</td>\n",
       "      <td>0.497970</td>\n",
       "      <td>0.330564</td>\n",
       "      <td>0.672572</td>\n",
       "      <td>0.494762</td>\n",
       "      <td>0.716798</td>\n",
       "      <td>1.124138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000300</td>\n",
       "      <td>5.000700</td>\n",
       "      <td>2.000400</td>\n",
       "      <td>5.000800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.000700</td>\n",
       "      <td>11.155400</td>\n",
       "      <td>11.750600</td>\n",
       "      <td>...</td>\n",
       "      <td>121.207000</td>\n",
       "      <td>1377.980000</td>\n",
       "      <td>-0.950279</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-1.240660</td>\n",
       "      <td>-0.533200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.863171</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-3.354270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.518975</td>\n",
       "      <td>1.506825</td>\n",
       "      <td>10.257700</td>\n",
       "      <td>6.033325</td>\n",
       "      <td>2.501750</td>\n",
       "      <td>6.004525</td>\n",
       "      <td>1.001925</td>\n",
       "      <td>2.509325</td>\n",
       "      <td>11.668100</td>\n",
       "      <td>12.485025</td>\n",
       "      <td>...</td>\n",
       "      <td>165.299500</td>\n",
       "      <td>1437.355000</td>\n",
       "      <td>0.279705</td>\n",
       "      <td>-0.693314</td>\n",
       "      <td>-0.096690</td>\n",
       "      <td>-0.143500</td>\n",
       "      <td>-0.847946</td>\n",
       "      <td>0.307439</td>\n",
       "      <td>-0.492667</td>\n",
       "      <td>-0.107755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.014300</td>\n",
       "      <td>2.042550</td>\n",
       "      <td>10.497350</td>\n",
       "      <td>6.962800</td>\n",
       "      <td>2.955600</td>\n",
       "      <td>6.956750</td>\n",
       "      <td>1.985950</td>\n",
       "      <td>2.986350</td>\n",
       "      <td>11.762100</td>\n",
       "      <td>12.644450</td>\n",
       "      <td>...</td>\n",
       "      <td>174.141000</td>\n",
       "      <td>1456.825000</td>\n",
       "      <td>0.617192</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>0.250205</td>\n",
       "      <td>-0.143500</td>\n",
       "      <td>-0.550868</td>\n",
       "      <td>0.709205</td>\n",
       "      <td>0.495988</td>\n",
       "      <td>0.390784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.488325</td>\n",
       "      <td>2.523625</td>\n",
       "      <td>10.750450</td>\n",
       "      <td>7.944000</td>\n",
       "      <td>3.481775</td>\n",
       "      <td>7.957600</td>\n",
       "      <td>2.953800</td>\n",
       "      <td>3.496425</td>\n",
       "      <td>11.835725</td>\n",
       "      <td>12.747475</td>\n",
       "      <td>...</td>\n",
       "      <td>181.637250</td>\n",
       "      <td>1496.542500</td>\n",
       "      <td>0.860046</td>\n",
       "      <td>0.696679</td>\n",
       "      <td>0.604809</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.370577</td>\n",
       "      <td>0.905374</td>\n",
       "      <td>0.877369</td>\n",
       "      <td>1.159790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.998200</td>\n",
       "      <td>2.998600</td>\n",
       "      <td>10.999300</td>\n",
       "      <td>8.996300</td>\n",
       "      <td>3.999400</td>\n",
       "      <td>8.998500</td>\n",
       "      <td>3.996200</td>\n",
       "      <td>3.999400</td>\n",
       "      <td>11.950100</td>\n",
       "      <td>12.933200</td>\n",
       "      <td>...</td>\n",
       "      <td>193.145000</td>\n",
       "      <td>1785.880000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.832180</td>\n",
       "      <td>0.632400</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.585870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               特征0          特征1          特征2          特征3          特征4  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000   \n",
       "mean      1.008837     2.018090    10.501365     6.980399     2.991379   \n",
       "std       0.573132     0.578950     0.286296     1.134771     0.574884   \n",
       "min       0.000500     1.000000    10.000300     5.000700     2.000400   \n",
       "25%       0.518975     1.506825    10.257700     6.033325     2.501750   \n",
       "50%       1.014300     2.042550    10.497350     6.962800     2.955600   \n",
       "75%       1.488325     2.523625    10.750450     7.944000     3.481775   \n",
       "max       1.998200     2.998600    10.999300     8.996300     3.999400   \n",
       "\n",
       "               特征5          特征6          特征7          特征8          特征9  ...  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000  ...   \n",
       "mean      6.986831     1.983407     2.994988    11.739018    12.600981  ...   \n",
       "std       1.142659     1.144038     0.577503     0.128526     0.196063  ...   \n",
       "min       5.000800     0.000100     2.000700    11.155400    11.750600  ...   \n",
       "25%       6.004525     1.001925     2.509325    11.668100    12.485025  ...   \n",
       "50%       6.956750     1.985950     2.986350    11.762100    12.644450  ...   \n",
       "75%       7.957600     2.953800     3.496425    11.835725    12.747475  ...   \n",
       "max       8.998500     3.996200     3.999400    11.950100    12.933200  ...   \n",
       "\n",
       "              特征16         特征17          补偿0          补偿1          补偿2  \\\n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000   \n",
       "mean    172.242767  1474.643939     0.525279     0.005173     0.250365   \n",
       "std      12.198489    54.894522     0.399656     0.699439     0.497970   \n",
       "min     121.207000  1377.980000    -0.950279    -0.999992    -1.240660   \n",
       "25%     165.299500  1437.355000     0.279705    -0.693314    -0.096690   \n",
       "50%     174.141000  1456.825000     0.617192     0.015394     0.250205   \n",
       "75%     181.637250  1496.542500     0.860046     0.696679     0.604809   \n",
       "max     193.145000  1785.880000     1.000000     1.000000     1.832180   \n",
       "\n",
       "               补偿3          补偿4          补偿5          补偿6          补偿7  \n",
       "count  2998.000000  2998.000000  2998.000000  2998.000000  2998.000000  \n",
       "mean      0.010439    -0.269551     0.524640     0.222959     0.561442  \n",
       "std       0.330564     0.672572     0.494762     0.716798     1.124138  \n",
       "min      -0.533200    -1.000000    -0.863171    -0.999998    -3.354270  \n",
       "25%      -0.143500    -0.847946     0.307439    -0.492667    -0.107755  \n",
       "50%      -0.143500    -0.550868     0.709205     0.495988     0.390784  \n",
       "75%       0.224500     0.370577     0.905374     0.877369     1.159790  \n",
       "max       0.632400     0.999997     0.999997     1.000000     6.585870  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds=dataset.dropna()\n",
    "w=['特征'+str(i) for i in range(18)]\n",
    "ds=ds.drop_duplicates(w)\n",
    "print(ds.shape)\n",
    "ds.describe()\n",
    "\n",
    "ds1=test_dataset.dropna()\n",
    "w=['特征'+str(i) for i in range(18)]\n",
    "ds1=ds1.drop_duplicates(w)\n",
    "print(ds1.shape)\n",
    "ds1.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9880db93-b4a2-4d05-b31f-2170c37e4c81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均值 [   1.       1.998   10.5      6.998    3.001    6.995    2.005    2.999\n",
      "   11.736   12.604    1.768    0.649   19.417   19.598    3.349   17.983\n",
      "  172.057 1475.054]\n",
      "方差 [   0.332    0.332    0.083    1.327    0.333    1.337    1.331    0.336\n",
      "    0.017    0.039    0.074    0.213    0.163    0.069   10.31   525.861\n",
      "  149.833 3136.668]\n"
     ]
    }
   ],
   "source": [
    "average = np.average(ds.values[:,:18], axis=0)\n",
    "variance = np.var(ds.values[:,:18], axis=0)\n",
    "print('均值', average)\n",
    "print('方差', variance)\n",
    "#\n",
    "#{\"inputs\": [[1.4158, 2.9711, 10.7935, 7.5279, 2.3352, 8.1042, 2.3096, 3.3367, 11.8639,\n",
    "#             12.7142, 1.8581, 0.3898, 19.8309, 19.771, 0.0001, 1.7768, 171.764, 1434.24]]}\n",
    "\n",
    "#{'outputs': [[0.31156069, -0.752204835, 0.293267965, -0.0997265279, -0.819844842, 0.861867249,\n",
    "#              0.451463282, 1.86217737]]}\n",
    "\n",
    "#{'outputs': [[0.331511, -0.932553, 0.285048, -0.1435, -0.833982, 0.767568, 0.463969, 1.9048]]\n",
    "#y1 MSE:0.0057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9fe5dc-c75c-4487-8662-e50c62cd34e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45261, 26)\n",
      "(45261, 26)\n",
      "特征0        0.327301\n",
      "特征1        0.328594\n",
      "特征2        0.081372\n",
      "特征3        1.289660\n",
      "特征4        0.317790\n",
      "特征5        1.284746\n",
      "特征6        1.317164\n",
      "特征7        0.327473\n",
      "特征8        0.014106\n",
      "特征9        0.032020\n",
      "特征10       0.054281\n",
      "特征11       0.190948\n",
      "特征12       0.106917\n",
      "特征13       0.049092\n",
      "特征14       8.075554\n",
      "特征15     298.280252\n",
      "特征16     117.274083\n",
      "特征17    2204.701086\n",
      "补偿0        0.150359\n",
      "补偿1        0.501306\n",
      "补偿2        0.238429\n",
      "补偿3        0.100420\n",
      "补偿4        0.408649\n",
      "补偿5        0.250138\n",
      "补偿6        0.509829\n",
      "补偿7        1.213095\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#计算前18列特征的均值和标准差\n",
    "# mean = np.mean(ds.values[:, :18], axis=0)\n",
    "# std = np.std(ds.values[:, :18], axis=0)\n",
    "\n",
    "mean = np.mean(test_dataset.values[:, :18], axis=0)\n",
    "std = np.std(test_dataset.values[:, :18], axis=0)\n",
    "mean1 = np.mean(ds1.values[:, :18], axis=0)\n",
    "std1 = np.std(ds1.values[:, :18], axis=0)\n",
    "# 定义上限和下限\n",
    "upper_limit = mean + 3 * std\n",
    "lower_limit = mean - 3 * std\n",
    "\n",
    "\n",
    "\n",
    "upper_limit1 = mean1 + 3 * std\n",
    "lower_limit1 = mean1 - 3 * std\n",
    "# 使用布尔索引删除超出上限和下限的行\n",
    "cleaned_ds = ds[~((ds.values[:, :18] > upper_limit) | (ds.values[:, :18] < lower_limit)).any(axis=1)]\n",
    "\n",
    "\n",
    "cleaned_ds1 = ds1[~((ds1.values[:, :18] > upper_limit1) | (ds1.values[:, :18] < lower_limit1)).any(axis=1)]\n",
    "\n",
    "# 打印清理后数据集的形状\n",
    "print(cleaned_ds.shape)\n",
    "\n",
    "print(cleaned_ds.shape)\n",
    "cleaned_ds.describe()\n",
    "\n",
    "cleaned_ds=pd.DataFrame(cleaned_ds)\n",
    "variance=np.var(cleaned_ds)\n",
    "\n",
    "print(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e3eeff0-7786-4933-ae06-3de78efaaa4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40735, 18) (40735, 8)\n",
      "(4526, 18) (4526, 8)\n",
      "(2998, 18) (2998, 8)\n"
     ]
    }
   ],
   "source": [
    "train_ds=dataset.sample(frac=0.9,random_state=0)\n",
    "val_ds=dataset.drop(train_ds.index)\n",
    "\n",
    "\n",
    "train_ds=cleaned_ds.sample(frac=0.9,random_state=0)\n",
    "val_ds=cleaned_ds.drop(train_ds.index)\n",
    "\n",
    "#训练集\n",
    "train_features=train_ds.values[:,:18]\n",
    "train_labels=train_ds.values[:,18:]\n",
    "\n",
    "#验证集\n",
    "val_features=val_ds.values[:,:18]\n",
    "val_labels=val_ds.values[:,18:]\n",
    "\n",
    "# 测试集\n",
    "test_features=test_dataset.values[:,:18]\n",
    "test_labels=test_dataset.values[:,18:]\n",
    "\n",
    "print(train_features.shape,train_labels.shape)\n",
    "print(val_features.shape,val_labels.shape)\n",
    "print(test_features.shape,test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c8e7b0-992e-4600-b098-f4fcfd4c23c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from numpy import array\n",
    "from numpy.random import uniform\n",
    "from numpy import hstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976c17ce-944d-4bb6-b843-246da048f49c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------构建模型及训练-----------------\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d9269ea-8503-41b3-b510-090df5d2777a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 02:26:09.515852: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-09-01 02:26:09.515917: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-09-01 02:26:09.515942: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (58c881efe57c): /proc/driver/nvidia/version does not exist\n",
      "2023-09-01 02:26:09.516178: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, 18)               37        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               5700      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 300)               90300     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 300)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 2408      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 188,745\n",
      "Trainable params: 188,708\n",
      "Non-trainable params: 37\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 创建Normalizaiton层\n",
    "normalizer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "# # 计算并设置归一化参数\n",
    "#normalizer.adapt(train_features)\n",
    "# 构建模型\n",
    "normalizer.adapt(val_features)\n",
    "model = tf.keras.Sequential([\n",
    "    normalizer, # 归一化层作为第一层\n",
    "    layers.Dense(300, activation=\"gelu\", input_dim=train_features.shape[1]),\n",
    "    layers.Dense(300, activation=\"gelu\"),\n",
    "    layers.Dense(300,activation=\"gelu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(train_labels.shape[1])\n",
    "# normalizer, # 归一化层作为第一层\n",
    "# layers.Dense(100, activation=\"relu\"),\n",
    "# layers.Dense(train_labels.shape[1])\n",
    "])\n",
    "\n",
    "# optimizers=optimizers.Nadam(learning_rate=0.01)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\") # 根据情况调整参数\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0fb0d64-f9e3-4191-91e3-334ee0d3cdeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1273/1273 [==============================] - 4s 2ms/step - loss: 0.1513 - val_loss: 0.1277 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "1273/1273 [==============================] - 3s 2ms/step - loss: 0.1258 - val_loss: 0.1149 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1179 - val_loss: 0.1121 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1146 - val_loss: 0.1088 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1127 - val_loss: 0.1085 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1114 - val_loss: 0.1061 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1102 - val_loss: 0.1049 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1093 - val_loss: 0.1042 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1085 - val_loss: 0.1036 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1078 - val_loss: 0.1034 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1075 - val_loss: 0.1025 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1070 - val_loss: 0.1030 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1066 - val_loss: 0.1011 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1063 - val_loss: 0.1020 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1058 - val_loss: 0.1020 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1056 - val_loss: 0.1013 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1053 - val_loss: 0.1010 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1052 - val_loss: 0.1007 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1049 - val_loss: 0.1014 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1046 - val_loss: 0.1014 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1045 - val_loss: 0.1000 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1042 - val_loss: 0.1002 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1040 - val_loss: 0.1000 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1038 - val_loss: 0.0994 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1035 - val_loss: 0.0994 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "1273/1273 [==============================] - 3s 3ms/step - loss: 0.1034 - val_loss: 0.0984 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1032 - val_loss: 0.0995 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1029 - val_loss: 0.0987 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1028 - val_loss: 0.0989 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1025 - val_loss: 0.1001 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1025 - val_loss: 0.0997 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1023 - val_loss: 0.0989 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1022 - val_loss: 0.0984 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1020 - val_loss: 0.0981 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1018 - val_loss: 0.0981 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1017 - val_loss: 0.0988 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1015 - val_loss: 0.0996 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1012 - val_loss: 0.0988 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1011 - val_loss: 0.0992 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1010 - val_loss: 0.0994 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1008 - val_loss: 0.0991 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1006 - val_loss: 0.0993 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1006 - val_loss: 0.0985 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1004 - val_loss: 0.0985 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1002 - val_loss: 0.0990 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.1000 - val_loss: 0.0986 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0999 - val_loss: 0.0993 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0997 - val_loss: 0.0999 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0996 - val_loss: 0.0992 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0994 - val_loss: 0.0992 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0991 - val_loss: 0.0990 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0990 - val_loss: 0.1004 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0988 - val_loss: 0.1003 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0985 - val_loss: 0.0997 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0983 - val_loss: 0.1007 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0980 - val_loss: 0.1008 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0979 - val_loss: 0.1003 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0977 - val_loss: 0.1009 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0975 - val_loss: 0.1017 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0972 - val_loss: 0.1010 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0970 - val_loss: 0.1012 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0964 - val_loss: 0.1014 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0964 - val_loss: 0.1019 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0961 - val_loss: 0.1020 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0960 - val_loss: 0.1023 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0957 - val_loss: 0.1034 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0954 - val_loss: 0.1030 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0949 - val_loss: 0.1036 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0948 - val_loss: 0.1033 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0945 - val_loss: 0.1031 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0943 - val_loss: 0.1034 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0939 - val_loss: 0.1059 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0937 - val_loss: 0.1044 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0933 - val_loss: 0.1045 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0929 - val_loss: 0.1055 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0927 - val_loss: 0.1055 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0924 - val_loss: 0.1056 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0922 - val_loss: 0.1068 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0919 - val_loss: 0.1060 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0913 - val_loss: 0.1058 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0911 - val_loss: 0.1066 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0908 - val_loss: 0.1083 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0905 - val_loss: 0.1072 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0902 - val_loss: 0.1075 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0897 - val_loss: 0.1082 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0893 - val_loss: 0.1104 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0892 - val_loss: 0.1088 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0887 - val_loss: 0.1092 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0882 - val_loss: 0.1096 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0878 - val_loss: 0.1085 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0876 - val_loss: 0.1098 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0873 - val_loss: 0.1090 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0868 - val_loss: 0.1106 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0864 - val_loss: 0.1089 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0861 - val_loss: 0.1092 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0858 - val_loss: 0.1127 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0854 - val_loss: 0.1123 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0848 - val_loss: 0.1118 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0844 - val_loss: 0.1116 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0843 - val_loss: 0.1124 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0838 - val_loss: 0.1140 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0830 - val_loss: 0.1130 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0830 - val_loss: 0.1131 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0823 - val_loss: 0.1145 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0819 - val_loss: 0.1151 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0813 - val_loss: 0.1155 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0809 - val_loss: 0.1169 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0804 - val_loss: 0.1140 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0800 - val_loss: 0.1170 - lr: 0.0010\n",
      "Epoch 110/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0795 - val_loss: 0.1167 - lr: 0.0010\n",
      "Epoch 111/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0791 - val_loss: 0.1176 - lr: 0.0010\n",
      "Epoch 112/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0784 - val_loss: 0.1188 - lr: 0.0010\n",
      "Epoch 113/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0780 - val_loss: 0.1159 - lr: 0.0010\n",
      "Epoch 114/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0775 - val_loss: 0.1180 - lr: 0.0010\n",
      "Epoch 115/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0766 - val_loss: 0.1194 - lr: 0.0010\n",
      "Epoch 116/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0763 - val_loss: 0.1209 - lr: 0.0010\n",
      "Epoch 117/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0760 - val_loss: 0.1210 - lr: 0.0010\n",
      "Epoch 118/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0754 - val_loss: 0.1194 - lr: 0.0010\n",
      "Epoch 119/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0749 - val_loss: 0.1220 - lr: 0.0010\n",
      "Epoch 120/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0746 - val_loss: 0.1222 - lr: 0.0010\n",
      "Epoch 121/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0739 - val_loss: 0.1216 - lr: 0.0010\n",
      "Epoch 122/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0732 - val_loss: 0.1251 - lr: 0.0010\n",
      "Epoch 123/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0727 - val_loss: 0.1244 - lr: 0.0010\n",
      "Epoch 124/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0721 - val_loss: 0.1231 - lr: 0.0010\n",
      "Epoch 125/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0713 - val_loss: 0.1234 - lr: 0.0010\n",
      "Epoch 126/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0711 - val_loss: 0.1234 - lr: 0.0010\n",
      "Epoch 127/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0702 - val_loss: 0.1257 - lr: 0.0010\n",
      "Epoch 128/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0700 - val_loss: 0.1264 - lr: 0.0010\n",
      "Epoch 129/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0695 - val_loss: 0.1274 - lr: 0.0010\n",
      "Epoch 130/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0690 - val_loss: 0.1258 - lr: 0.0010\n",
      "Epoch 131/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0684 - val_loss: 0.1276 - lr: 0.0010\n",
      "Epoch 132/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0679 - val_loss: 0.1265 - lr: 0.0010\n",
      "Epoch 133/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0676 - val_loss: 0.1282 - lr: 0.0010\n",
      "Epoch 134/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0671 - val_loss: 0.1279 - lr: 0.0010\n",
      "Epoch 135/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0648 - val_loss: 0.1290 - lr: 9.0000e-04\n",
      "Epoch 136/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0643 - val_loss: 0.1280 - lr: 9.0000e-04\n",
      "Epoch 137/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0638 - val_loss: 0.1280 - lr: 9.0000e-04\n",
      "Epoch 138/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0633 - val_loss: 0.1294 - lr: 9.0000e-04\n",
      "Epoch 139/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0629 - val_loss: 0.1282 - lr: 9.0000e-04\n",
      "Epoch 140/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0622 - val_loss: 0.1302 - lr: 9.0000e-04\n",
      "Epoch 141/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0618 - val_loss: 0.1314 - lr: 9.0000e-04\n",
      "Epoch 142/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0617 - val_loss: 0.1320 - lr: 9.0000e-04\n",
      "Epoch 143/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0612 - val_loss: 0.1322 - lr: 9.0000e-04\n",
      "Epoch 144/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0606 - val_loss: 0.1312 - lr: 9.0000e-04\n",
      "Epoch 145/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0605 - val_loss: 0.1319 - lr: 9.0000e-04\n",
      "Epoch 146/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0599 - val_loss: 0.1324 - lr: 9.0000e-04\n",
      "Epoch 147/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0596 - val_loss: 0.1338 - lr: 9.0000e-04\n",
      "Epoch 148/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0591 - val_loss: 0.1325 - lr: 9.0000e-04\n",
      "Epoch 149/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0587 - val_loss: 0.1337 - lr: 9.0000e-04\n",
      "Epoch 150/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0583 - val_loss: 0.1349 - lr: 9.0000e-04\n",
      "Epoch 151/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0581 - val_loss: 0.1348 - lr: 9.0000e-04\n",
      "Epoch 152/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0575 - val_loss: 0.1349 - lr: 9.0000e-04\n",
      "Epoch 153/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0576 - val_loss: 0.1341 - lr: 9.0000e-04\n",
      "Epoch 154/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0569 - val_loss: 0.1333 - lr: 9.0000e-04\n",
      "Epoch 155/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0568 - val_loss: 0.1342 - lr: 9.0000e-04\n",
      "Epoch 156/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0563 - val_loss: 0.1348 - lr: 9.0000e-04\n",
      "Epoch 157/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0561 - val_loss: 0.1338 - lr: 9.0000e-04\n",
      "Epoch 158/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0556 - val_loss: 0.1366 - lr: 9.0000e-04\n",
      "Epoch 159/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0555 - val_loss: 0.1383 - lr: 9.0000e-04\n",
      "Epoch 160/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0553 - val_loss: 0.1373 - lr: 9.0000e-04\n",
      "Epoch 161/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0549 - val_loss: 0.1374 - lr: 9.0000e-04\n",
      "Epoch 162/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0548 - val_loss: 0.1371 - lr: 9.0000e-04\n",
      "Epoch 163/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0543 - val_loss: 0.1368 - lr: 9.0000e-04\n",
      "Epoch 164/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0542 - val_loss: 0.1368 - lr: 9.0000e-04\n",
      "Epoch 165/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0538 - val_loss: 0.1384 - lr: 9.0000e-04\n",
      "Epoch 166/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0536 - val_loss: 0.1357 - lr: 9.0000e-04\n",
      "Epoch 167/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0531 - val_loss: 0.1355 - lr: 9.0000e-04\n",
      "Epoch 168/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0531 - val_loss: 0.1381 - lr: 9.0000e-04\n",
      "Epoch 169/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0530 - val_loss: 0.1362 - lr: 9.0000e-04\n",
      "Epoch 170/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0525 - val_loss: 0.1368 - lr: 9.0000e-04\n",
      "Epoch 171/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0524 - val_loss: 0.1379 - lr: 9.0000e-04\n",
      "Epoch 172/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0522 - val_loss: 0.1380 - lr: 9.0000e-04\n",
      "Epoch 173/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0521 - val_loss: 0.1375 - lr: 9.0000e-04\n",
      "Epoch 174/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0516 - val_loss: 0.1378 - lr: 9.0000e-04\n",
      "Epoch 175/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0515 - val_loss: 0.1378 - lr: 9.0000e-04\n",
      "Epoch 176/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0512 - val_loss: 0.1403 - lr: 9.0000e-04\n",
      "Epoch 177/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0509 - val_loss: 0.1372 - lr: 9.0000e-04\n",
      "Epoch 178/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0507 - val_loss: 0.1393 - lr: 9.0000e-04\n",
      "Epoch 179/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0504 - val_loss: 0.1400 - lr: 9.0000e-04\n",
      "Epoch 180/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0505 - val_loss: 0.1367 - lr: 9.0000e-04\n",
      "Epoch 181/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0499 - val_loss: 0.1404 - lr: 9.0000e-04\n",
      "Epoch 182/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0502 - val_loss: 0.1389 - lr: 9.0000e-04\n",
      "Epoch 183/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0498 - val_loss: 0.1408 - lr: 9.0000e-04\n",
      "Epoch 184/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0498 - val_loss: 0.1396 - lr: 9.0000e-04\n",
      "Epoch 185/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0493 - val_loss: 0.1380 - lr: 9.0000e-04\n",
      "Epoch 186/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0490 - val_loss: 0.1394 - lr: 9.0000e-04\n",
      "Epoch 187/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0492 - val_loss: 0.1384 - lr: 9.0000e-04\n",
      "Epoch 188/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0488 - val_loss: 0.1389 - lr: 9.0000e-04\n",
      "Epoch 189/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0488 - val_loss: 0.1394 - lr: 9.0000e-04\n",
      "Epoch 190/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0488 - val_loss: 0.1385 - lr: 9.0000e-04\n",
      "Epoch 191/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0484 - val_loss: 0.1401 - lr: 9.0000e-04\n",
      "Epoch 192/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0482 - val_loss: 0.1403 - lr: 9.0000e-04\n",
      "Epoch 193/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0483 - val_loss: 0.1404 - lr: 9.0000e-04\n",
      "Epoch 194/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0482 - val_loss: 0.1396 - lr: 9.0000e-04\n",
      "Epoch 195/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0477 - val_loss: 0.1400 - lr: 9.0000e-04\n",
      "Epoch 196/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0478 - val_loss: 0.1411 - lr: 9.0000e-04\n",
      "Epoch 197/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0478 - val_loss: 0.1387 - lr: 9.0000e-04\n",
      "Epoch 198/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0476 - val_loss: 0.1411 - lr: 9.0000e-04\n",
      "Epoch 199/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0473 - val_loss: 0.1396 - lr: 9.0000e-04\n",
      "Epoch 200/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0472 - val_loss: 0.1399 - lr: 9.0000e-04\n",
      "Epoch 201/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0469 - val_loss: 0.1397 - lr: 9.0000e-04\n",
      "Epoch 202/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0470 - val_loss: 0.1382 - lr: 9.0000e-04\n",
      "Epoch 203/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0465 - val_loss: 0.1415 - lr: 9.0000e-04\n",
      "Epoch 204/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0462 - val_loss: 0.1412 - lr: 9.0000e-04\n",
      "Epoch 205/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0466 - val_loss: 0.1411 - lr: 9.0000e-04\n",
      "Epoch 206/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0461 - val_loss: 0.1411 - lr: 9.0000e-04\n",
      "Epoch 207/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0463 - val_loss: 0.1413 - lr: 9.0000e-04\n",
      "Epoch 208/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0462 - val_loss: 0.1407 - lr: 9.0000e-04\n",
      "Epoch 209/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0458 - val_loss: 0.1407 - lr: 9.0000e-04\n",
      "Epoch 210/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0456 - val_loss: 0.1407 - lr: 9.0000e-04\n",
      "Epoch 211/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0455 - val_loss: 0.1403 - lr: 9.0000e-04\n",
      "Epoch 212/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0455 - val_loss: 0.1403 - lr: 9.0000e-04\n",
      "Epoch 213/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0455 - val_loss: 0.1405 - lr: 9.0000e-04\n",
      "Epoch 214/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0455 - val_loss: 0.1411 - lr: 9.0000e-04\n",
      "Epoch 215/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0450 - val_loss: 0.1407 - lr: 9.0000e-04\n",
      "Epoch 216/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0449 - val_loss: 0.1402 - lr: 9.0000e-04\n",
      "Epoch 217/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0448 - val_loss: 0.1403 - lr: 9.0000e-04\n",
      "Epoch 218/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0447 - val_loss: 0.1394 - lr: 9.0000e-04\n",
      "Epoch 219/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0448 - val_loss: 0.1423 - lr: 9.0000e-04\n",
      "Epoch 220/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0445 - val_loss: 0.1426 - lr: 9.0000e-04\n",
      "Epoch 221/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0445 - val_loss: 0.1409 - lr: 9.0000e-04\n",
      "Epoch 222/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0443 - val_loss: 0.1398 - lr: 9.0000e-04\n",
      "Epoch 223/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0442 - val_loss: 0.1452 - lr: 9.0000e-04\n",
      "Epoch 224/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0442 - val_loss: 0.1426 - lr: 9.0000e-04\n",
      "Epoch 225/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0441 - val_loss: 0.1420 - lr: 9.0000e-04\n",
      "Epoch 226/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0440 - val_loss: 0.1434 - lr: 9.0000e-04\n",
      "Epoch 227/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0437 - val_loss: 0.1432 - lr: 9.0000e-04\n",
      "Epoch 228/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0438 - val_loss: 0.1417 - lr: 9.0000e-04\n",
      "Epoch 229/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0437 - val_loss: 0.1409 - lr: 9.0000e-04\n",
      "Epoch 230/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0434 - val_loss: 0.1439 - lr: 9.0000e-04\n",
      "Epoch 231/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0435 - val_loss: 0.1424 - lr: 9.0000e-04\n",
      "Epoch 232/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0436 - val_loss: 0.1426 - lr: 9.0000e-04\n",
      "Epoch 233/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0432 - val_loss: 0.1463 - lr: 9.0000e-04\n",
      "Epoch 234/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0432 - val_loss: 0.1418 - lr: 9.0000e-04\n",
      "Epoch 235/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0419 - val_loss: 0.1436 - lr: 8.1000e-04\n",
      "Epoch 236/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0415 - val_loss: 0.1420 - lr: 8.1000e-04\n",
      "Epoch 237/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0413 - val_loss: 0.1440 - lr: 8.1000e-04\n",
      "Epoch 238/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0413 - val_loss: 0.1449 - lr: 8.1000e-04\n",
      "Epoch 239/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0409 - val_loss: 0.1445 - lr: 8.1000e-04\n",
      "Epoch 240/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0408 - val_loss: 0.1414 - lr: 8.1000e-04\n",
      "Epoch 241/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0412 - val_loss: 0.1455 - lr: 8.1000e-04\n",
      "Epoch 242/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0409 - val_loss: 0.1436 - lr: 8.1000e-04\n",
      "Epoch 243/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0406 - val_loss: 0.1428 - lr: 8.1000e-04\n",
      "Epoch 244/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0408 - val_loss: 0.1444 - lr: 8.1000e-04\n",
      "Epoch 245/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0408 - val_loss: 0.1456 - lr: 8.1000e-04\n",
      "Epoch 246/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0408 - val_loss: 0.1452 - lr: 8.1000e-04\n",
      "Epoch 247/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0404 - val_loss: 0.1440 - lr: 8.1000e-04\n",
      "Epoch 248/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0404 - val_loss: 0.1467 - lr: 8.1000e-04\n",
      "Epoch 249/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0404 - val_loss: 0.1458 - lr: 8.1000e-04\n",
      "Epoch 250/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0401 - val_loss: 0.1436 - lr: 8.1000e-04\n",
      "Epoch 251/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0405 - val_loss: 0.1430 - lr: 8.1000e-04\n",
      "Epoch 252/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0400 - val_loss: 0.1425 - lr: 8.1000e-04\n",
      "Epoch 253/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0400 - val_loss: 0.1460 - lr: 8.1000e-04\n",
      "Epoch 254/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0401 - val_loss: 0.1466 - lr: 8.1000e-04\n",
      "Epoch 255/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0401 - val_loss: 0.1463 - lr: 8.1000e-04\n",
      "Epoch 256/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0400 - val_loss: 0.1448 - lr: 8.1000e-04\n",
      "Epoch 257/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0397 - val_loss: 0.1463 - lr: 8.1000e-04\n",
      "Epoch 258/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0399 - val_loss: 0.1449 - lr: 8.1000e-04\n",
      "Epoch 259/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0395 - val_loss: 0.1442 - lr: 8.1000e-04\n",
      "Epoch 260/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0397 - val_loss: 0.1454 - lr: 8.1000e-04\n",
      "Epoch 261/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0396 - val_loss: 0.1459 - lr: 8.1000e-04\n",
      "Epoch 262/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0395 - val_loss: 0.1443 - lr: 8.1000e-04\n",
      "Epoch 263/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0395 - val_loss: 0.1444 - lr: 8.1000e-04\n",
      "Epoch 264/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0393 - val_loss: 0.1465 - lr: 8.1000e-04\n",
      "Epoch 265/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0393 - val_loss: 0.1440 - lr: 8.1000e-04\n",
      "Epoch 266/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0392 - val_loss: 0.1471 - lr: 8.1000e-04\n",
      "Epoch 267/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0392 - val_loss: 0.1478 - lr: 8.1000e-04\n",
      "Epoch 268/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0393 - val_loss: 0.1445 - lr: 8.1000e-04\n",
      "Epoch 269/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0390 - val_loss: 0.1451 - lr: 8.1000e-04\n",
      "Epoch 270/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0388 - val_loss: 0.1447 - lr: 8.1000e-04\n",
      "Epoch 271/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0390 - val_loss: 0.1463 - lr: 8.1000e-04\n",
      "Epoch 272/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0391 - val_loss: 0.1450 - lr: 8.1000e-04\n",
      "Epoch 273/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0389 - val_loss: 0.1458 - lr: 8.1000e-04\n",
      "Epoch 274/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0390 - val_loss: 0.1444 - lr: 8.1000e-04\n",
      "Epoch 275/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0387 - val_loss: 0.1459 - lr: 8.1000e-04\n",
      "Epoch 276/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0388 - val_loss: 0.1457 - lr: 8.1000e-04\n",
      "Epoch 277/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0388 - val_loss: 0.1448 - lr: 8.1000e-04\n",
      "Epoch 278/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0388 - val_loss: 0.1452 - lr: 8.1000e-04\n",
      "Epoch 279/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0385 - val_loss: 0.1441 - lr: 8.1000e-04\n",
      "Epoch 280/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0384 - val_loss: 0.1470 - lr: 8.1000e-04\n",
      "Epoch 281/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0383 - val_loss: 0.1428 - lr: 8.1000e-04\n",
      "Epoch 282/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0384 - val_loss: 0.1448 - lr: 8.1000e-04\n",
      "Epoch 283/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0383 - val_loss: 0.1455 - lr: 8.1000e-04\n",
      "Epoch 284/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0383 - val_loss: 0.1438 - lr: 8.1000e-04\n",
      "Epoch 285/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0385 - val_loss: 0.1441 - lr: 8.1000e-04\n",
      "Epoch 286/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0385 - val_loss: 0.1468 - lr: 8.1000e-04\n",
      "Epoch 287/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0384 - val_loss: 0.1456 - lr: 8.1000e-04\n",
      "Epoch 288/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0380 - val_loss: 0.1476 - lr: 8.1000e-04\n",
      "Epoch 289/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0382 - val_loss: 0.1465 - lr: 8.1000e-04\n",
      "Epoch 290/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0383 - val_loss: 0.1460 - lr: 8.1000e-04\n",
      "Epoch 291/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0382 - val_loss: 0.1450 - lr: 8.1000e-04\n",
      "Epoch 292/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0378 - val_loss: 0.1475 - lr: 8.1000e-04\n",
      "Epoch 293/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0380 - val_loss: 0.1455 - lr: 8.1000e-04\n",
      "Epoch 294/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0378 - val_loss: 0.1472 - lr: 8.1000e-04\n",
      "Epoch 295/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0379 - val_loss: 0.1469 - lr: 8.1000e-04\n",
      "Epoch 296/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0377 - val_loss: 0.1454 - lr: 8.1000e-04\n",
      "Epoch 297/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0378 - val_loss: 0.1454 - lr: 8.1000e-04\n",
      "Epoch 298/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0378 - val_loss: 0.1475 - lr: 8.1000e-04\n",
      "Epoch 299/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0381 - val_loss: 0.1445 - lr: 8.1000e-04\n",
      "Epoch 300/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0377 - val_loss: 0.1475 - lr: 8.1000e-04\n",
      "Epoch 301/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0379 - val_loss: 0.1446 - lr: 8.1000e-04\n",
      "Epoch 302/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0375 - val_loss: 0.1445 - lr: 8.1000e-04\n",
      "Epoch 303/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0375 - val_loss: 0.1457 - lr: 8.1000e-04\n",
      "Epoch 304/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0378 - val_loss: 0.1460 - lr: 8.1000e-04\n",
      "Epoch 305/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0375 - val_loss: 0.1457 - lr: 8.1000e-04\n",
      "Epoch 306/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0376 - val_loss: 0.1442 - lr: 8.1000e-04\n",
      "Epoch 307/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0374 - val_loss: 0.1467 - lr: 8.1000e-04\n",
      "Epoch 308/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0375 - val_loss: 0.1462 - lr: 8.1000e-04\n",
      "Epoch 309/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0375 - val_loss: 0.1477 - lr: 8.1000e-04\n",
      "Epoch 310/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0375 - val_loss: 0.1459 - lr: 8.1000e-04\n",
      "Epoch 311/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0374 - val_loss: 0.1479 - lr: 8.1000e-04\n",
      "Epoch 312/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0372 - val_loss: 0.1445 - lr: 8.1000e-04\n",
      "Epoch 313/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0372 - val_loss: 0.1460 - lr: 8.1000e-04\n",
      "Epoch 314/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0370 - val_loss: 0.1464 - lr: 8.1000e-04\n",
      "Epoch 315/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0372 - val_loss: 0.1440 - lr: 8.1000e-04\n",
      "Epoch 316/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0374 - val_loss: 0.1456 - lr: 8.1000e-04\n",
      "Epoch 317/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0371 - val_loss: 0.1443 - lr: 8.1000e-04\n",
      "Epoch 318/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0368 - val_loss: 0.1446 - lr: 8.1000e-04\n",
      "Epoch 319/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0372 - val_loss: 0.1453 - lr: 8.1000e-04\n",
      "Epoch 320/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0370 - val_loss: 0.1460 - lr: 8.1000e-04\n",
      "Epoch 321/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0370 - val_loss: 0.1474 - lr: 8.1000e-04\n",
      "Epoch 322/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0369 - val_loss: 0.1468 - lr: 8.1000e-04\n",
      "Epoch 323/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0368 - val_loss: 0.1450 - lr: 8.1000e-04\n",
      "Epoch 324/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0369 - val_loss: 0.1463 - lr: 8.1000e-04\n",
      "Epoch 325/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0369 - val_loss: 0.1452 - lr: 8.1000e-04\n",
      "Epoch 326/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0368 - val_loss: 0.1472 - lr: 8.1000e-04\n",
      "Epoch 327/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0366 - val_loss: 0.1467 - lr: 8.1000e-04\n",
      "Epoch 328/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0369 - val_loss: 0.1454 - lr: 8.1000e-04\n",
      "Epoch 329/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0367 - val_loss: 0.1482 - lr: 8.1000e-04\n",
      "Epoch 330/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0368 - val_loss: 0.1443 - lr: 8.1000e-04\n",
      "Epoch 331/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0369 - val_loss: 0.1496 - lr: 8.1000e-04\n",
      "Epoch 332/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0365 - val_loss: 0.1471 - lr: 8.1000e-04\n",
      "Epoch 333/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0367 - val_loss: 0.1440 - lr: 8.1000e-04\n",
      "Epoch 334/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0366 - val_loss: 0.1461 - lr: 8.1000e-04\n",
      "Epoch 335/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0357 - val_loss: 0.1462 - lr: 7.2900e-04\n",
      "Epoch 336/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0354 - val_loss: 0.1449 - lr: 7.2900e-04\n",
      "Epoch 337/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0352 - val_loss: 0.1453 - lr: 7.2900e-04\n",
      "Epoch 338/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0351 - val_loss: 0.1472 - lr: 7.2900e-04\n",
      "Epoch 339/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0352 - val_loss: 0.1484 - lr: 7.2900e-04\n",
      "Epoch 340/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0349 - val_loss: 0.1471 - lr: 7.2900e-04\n",
      "Epoch 341/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0351 - val_loss: 0.1454 - lr: 7.2900e-04\n",
      "Epoch 342/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0351 - val_loss: 0.1470 - lr: 7.2900e-04\n",
      "Epoch 343/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0349 - val_loss: 0.1484 - lr: 7.2900e-04\n",
      "Epoch 344/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0350 - val_loss: 0.1461 - lr: 7.2900e-04\n",
      "Epoch 345/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0349 - val_loss: 0.1446 - lr: 7.2900e-04\n",
      "Epoch 346/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0348 - val_loss: 0.1489 - lr: 7.2900e-04\n",
      "Epoch 347/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0349 - val_loss: 0.1453 - lr: 7.2900e-04\n",
      "Epoch 348/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0347 - val_loss: 0.1462 - lr: 7.2900e-04\n",
      "Epoch 349/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0349 - val_loss: 0.1453 - lr: 7.2900e-04\n",
      "Epoch 350/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0348 - val_loss: 0.1462 - lr: 7.2900e-04\n",
      "Epoch 351/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0347 - val_loss: 0.1477 - lr: 7.2900e-04\n",
      "Epoch 352/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0346 - val_loss: 0.1471 - lr: 7.2900e-04\n",
      "Epoch 353/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0347 - val_loss: 0.1470 - lr: 7.2900e-04\n",
      "Epoch 354/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0347 - val_loss: 0.1481 - lr: 7.2900e-04\n",
      "Epoch 355/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0348 - val_loss: 0.1465 - lr: 7.2900e-04\n",
      "Epoch 356/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0348 - val_loss: 0.1496 - lr: 7.2900e-04\n",
      "Epoch 357/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0348 - val_loss: 0.1477 - lr: 7.2900e-04\n",
      "Epoch 358/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0347 - val_loss: 0.1461 - lr: 7.2900e-04\n",
      "Epoch 359/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0345 - val_loss: 0.1471 - lr: 7.2900e-04\n",
      "Epoch 360/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0347 - val_loss: 0.1474 - lr: 7.2900e-04\n",
      "Epoch 361/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0345 - val_loss: 0.1487 - lr: 7.2900e-04\n",
      "Epoch 362/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0344 - val_loss: 0.1478 - lr: 7.2900e-04\n",
      "Epoch 363/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0344 - val_loss: 0.1481 - lr: 7.2900e-04\n",
      "Epoch 364/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0343 - val_loss: 0.1497 - lr: 7.2900e-04\n",
      "Epoch 365/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0344 - val_loss: 0.1484 - lr: 7.2900e-04\n",
      "Epoch 366/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0346 - val_loss: 0.1488 - lr: 7.2900e-04\n",
      "Epoch 367/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0343 - val_loss: 0.1488 - lr: 7.2900e-04\n",
      "Epoch 368/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0343 - val_loss: 0.1480 - lr: 7.2900e-04\n",
      "Epoch 369/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0346 - val_loss: 0.1483 - lr: 7.2900e-04\n",
      "Epoch 370/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0343 - val_loss: 0.1484 - lr: 7.2900e-04\n",
      "Epoch 371/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0341 - val_loss: 0.1470 - lr: 7.2900e-04\n",
      "Epoch 372/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0342 - val_loss: 0.1459 - lr: 7.2900e-04\n",
      "Epoch 373/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0344 - val_loss: 0.1481 - lr: 7.2900e-04\n",
      "Epoch 374/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0343 - val_loss: 0.1473 - lr: 7.2900e-04\n",
      "Epoch 375/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0342 - val_loss: 0.1482 - lr: 7.2900e-04\n",
      "Epoch 376/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0343 - val_loss: 0.1499 - lr: 7.2900e-04\n",
      "Epoch 377/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0341 - val_loss: 0.1491 - lr: 7.2900e-04\n",
      "Epoch 378/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0344 - val_loss: 0.1455 - lr: 7.2900e-04\n",
      "Epoch 379/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0342 - val_loss: 0.1477 - lr: 7.2900e-04\n",
      "Epoch 380/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0339 - val_loss: 0.1472 - lr: 7.2900e-04\n",
      "Epoch 381/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0343 - val_loss: 0.1484 - lr: 7.2900e-04\n",
      "Epoch 382/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0339 - val_loss: 0.1467 - lr: 7.2900e-04\n",
      "Epoch 383/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0339 - val_loss: 0.1488 - lr: 7.2900e-04\n",
      "Epoch 384/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0340 - val_loss: 0.1464 - lr: 7.2900e-04\n",
      "Epoch 385/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0341 - val_loss: 0.1479 - lr: 7.2900e-04\n",
      "Epoch 386/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0338 - val_loss: 0.1493 - lr: 7.2900e-04\n",
      "Epoch 387/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0340 - val_loss: 0.1489 - lr: 7.2900e-04\n",
      "Epoch 388/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0341 - val_loss: 0.1471 - lr: 7.2900e-04\n",
      "Epoch 389/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0340 - val_loss: 0.1469 - lr: 7.2900e-04\n",
      "Epoch 390/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0340 - val_loss: 0.1487 - lr: 7.2900e-04\n",
      "Epoch 391/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0338 - val_loss: 0.1475 - lr: 7.2900e-04\n",
      "Epoch 392/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0339 - val_loss: 0.1480 - lr: 7.2900e-04\n",
      "Epoch 393/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0341 - val_loss: 0.1475 - lr: 7.2900e-04\n",
      "Epoch 394/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0338 - val_loss: 0.1473 - lr: 7.2900e-04\n",
      "Epoch 395/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0338 - val_loss: 0.1466 - lr: 7.2900e-04\n",
      "Epoch 396/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0338 - val_loss: 0.1479 - lr: 7.2900e-04\n",
      "Epoch 397/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0338 - val_loss: 0.1466 - lr: 7.2900e-04\n",
      "Epoch 398/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0337 - val_loss: 0.1492 - lr: 7.2900e-04\n",
      "Epoch 399/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0338 - val_loss: 0.1472 - lr: 7.2900e-04\n",
      "Epoch 400/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0339 - val_loss: 0.1465 - lr: 7.2900e-04\n",
      "Epoch 401/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0341 - val_loss: 0.1450 - lr: 7.2900e-04\n",
      "Epoch 402/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0337 - val_loss: 0.1462 - lr: 7.2900e-04\n",
      "Epoch 403/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0337 - val_loss: 0.1479 - lr: 7.2900e-04\n",
      "Epoch 404/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0338 - val_loss: 0.1469 - lr: 7.2900e-04\n",
      "Epoch 405/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0337 - val_loss: 0.1485 - lr: 7.2900e-04\n",
      "Epoch 406/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0336 - val_loss: 0.1455 - lr: 7.2900e-04\n",
      "Epoch 407/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0337 - val_loss: 0.1482 - lr: 7.2900e-04\n",
      "Epoch 408/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0335 - val_loss: 0.1490 - lr: 7.2900e-04\n",
      "Epoch 409/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0335 - val_loss: 0.1477 - lr: 7.2900e-04\n",
      "Epoch 410/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0336 - val_loss: 0.1482 - lr: 7.2900e-04\n",
      "Epoch 411/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0337 - val_loss: 0.1485 - lr: 7.2900e-04\n",
      "Epoch 412/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0335 - val_loss: 0.1494 - lr: 7.2900e-04\n",
      "Epoch 413/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0338 - val_loss: 0.1488 - lr: 7.2900e-04\n",
      "Epoch 414/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0333 - val_loss: 0.1492 - lr: 7.2900e-04\n",
      "Epoch 415/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0337 - val_loss: 0.1488 - lr: 7.2900e-04\n",
      "Epoch 416/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0337 - val_loss: 0.1480 - lr: 7.2900e-04\n",
      "Epoch 417/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0332 - val_loss: 0.1482 - lr: 7.2900e-04\n",
      "Epoch 418/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0337 - val_loss: 0.1480 - lr: 7.2900e-04\n",
      "Epoch 419/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0334 - val_loss: 0.1483 - lr: 7.2900e-04\n",
      "Epoch 420/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0334 - val_loss: 0.1473 - lr: 7.2900e-04\n",
      "Epoch 421/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0334 - val_loss: 0.1483 - lr: 7.2900e-04\n",
      "Epoch 422/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0334 - val_loss: 0.1474 - lr: 7.2900e-04\n",
      "Epoch 423/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0336 - val_loss: 0.1484 - lr: 7.2900e-04\n",
      "Epoch 424/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0336 - val_loss: 0.1488 - lr: 7.2900e-04\n",
      "Epoch 425/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0334 - val_loss: 0.1492 - lr: 7.2900e-04\n",
      "Epoch 426/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0331 - val_loss: 0.1461 - lr: 7.2900e-04\n",
      "Epoch 427/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0334 - val_loss: 0.1466 - lr: 7.2900e-04\n",
      "Epoch 428/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0335 - val_loss: 0.1488 - lr: 7.2900e-04\n",
      "Epoch 429/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0333 - val_loss: 0.1498 - lr: 7.2900e-04\n",
      "Epoch 430/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0333 - val_loss: 0.1466 - lr: 7.2900e-04\n",
      "Epoch 431/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0333 - val_loss: 0.1474 - lr: 7.2900e-04\n",
      "Epoch 432/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0331 - val_loss: 0.1461 - lr: 7.2900e-04\n",
      "Epoch 433/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0333 - val_loss: 0.1472 - lr: 7.2900e-04\n",
      "Epoch 434/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0332 - val_loss: 0.1484 - lr: 7.2900e-04\n",
      "Epoch 435/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0322 - val_loss: 0.1467 - lr: 6.5610e-04\n",
      "Epoch 436/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0320 - val_loss: 0.1475 - lr: 6.5610e-04\n",
      "Epoch 437/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0323 - val_loss: 0.1500 - lr: 6.5610e-04\n",
      "Epoch 438/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0321 - val_loss: 0.1484 - lr: 6.5610e-04\n",
      "Epoch 439/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0320 - val_loss: 0.1507 - lr: 6.5610e-04\n",
      "Epoch 440/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0319 - val_loss: 0.1491 - lr: 6.5610e-04\n",
      "Epoch 441/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0319 - val_loss: 0.1501 - lr: 6.5610e-04\n",
      "Epoch 442/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0318 - val_loss: 0.1480 - lr: 6.5610e-04\n",
      "Epoch 443/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0319 - val_loss: 0.1477 - lr: 6.5610e-04\n",
      "Epoch 444/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0321 - val_loss: 0.1489 - lr: 6.5610e-04\n",
      "Epoch 445/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0319 - val_loss: 0.1474 - lr: 6.5610e-04\n",
      "Epoch 446/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0320 - val_loss: 0.1496 - lr: 6.5610e-04\n",
      "Epoch 447/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0318 - val_loss: 0.1465 - lr: 6.5610e-04\n",
      "Epoch 448/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0319 - val_loss: 0.1497 - lr: 6.5610e-04\n",
      "Epoch 449/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0316 - val_loss: 0.1466 - lr: 6.5610e-04\n",
      "Epoch 450/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0317 - val_loss: 0.1486 - lr: 6.5610e-04\n",
      "Epoch 451/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0319 - val_loss: 0.1463 - lr: 6.5610e-04\n",
      "Epoch 452/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0318 - val_loss: 0.1473 - lr: 6.5610e-04\n",
      "Epoch 453/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0318 - val_loss: 0.1487 - lr: 6.5610e-04\n",
      "Epoch 454/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0317 - val_loss: 0.1493 - lr: 6.5610e-04\n",
      "Epoch 455/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0318 - val_loss: 0.1506 - lr: 6.5610e-04\n",
      "Epoch 456/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0315 - val_loss: 0.1497 - lr: 6.5610e-04\n",
      "Epoch 457/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0317 - val_loss: 0.1478 - lr: 6.5610e-04\n",
      "Epoch 458/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0317 - val_loss: 0.1493 - lr: 6.5610e-04\n",
      "Epoch 459/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0317 - val_loss: 0.1484 - lr: 6.5610e-04\n",
      "Epoch 460/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0317 - val_loss: 0.1502 - lr: 6.5610e-04\n",
      "Epoch 461/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0319 - val_loss: 0.1484 - lr: 6.5610e-04\n",
      "Epoch 462/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0318 - val_loss: 0.1481 - lr: 6.5610e-04\n",
      "Epoch 463/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0316 - val_loss: 0.1504 - lr: 6.5610e-04\n",
      "Epoch 464/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0317 - val_loss: 0.1480 - lr: 6.5610e-04\n",
      "Epoch 465/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0318 - val_loss: 0.1486 - lr: 6.5610e-04\n",
      "Epoch 466/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0314 - val_loss: 0.1470 - lr: 6.5610e-04\n",
      "Epoch 467/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0316 - val_loss: 0.1490 - lr: 6.5610e-04\n",
      "Epoch 468/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0316 - val_loss: 0.1494 - lr: 6.5610e-04\n",
      "Epoch 469/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0318 - val_loss: 0.1513 - lr: 6.5610e-04\n",
      "Epoch 470/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0314 - val_loss: 0.1485 - lr: 6.5610e-04\n",
      "Epoch 471/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0314 - val_loss: 0.1472 - lr: 6.5610e-04\n",
      "Epoch 472/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0315 - val_loss: 0.1477 - lr: 6.5610e-04\n",
      "Epoch 473/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0315 - val_loss: 0.1461 - lr: 6.5610e-04\n",
      "Epoch 474/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0315 - val_loss: 0.1492 - lr: 6.5610e-04\n",
      "Epoch 475/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0314 - val_loss: 0.1488 - lr: 6.5610e-04\n",
      "Epoch 476/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0315 - val_loss: 0.1514 - lr: 6.5610e-04\n",
      "Epoch 477/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0313 - val_loss: 0.1494 - lr: 6.5610e-04\n",
      "Epoch 478/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0314 - val_loss: 0.1496 - lr: 6.5610e-04\n",
      "Epoch 479/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0316 - val_loss: 0.1494 - lr: 6.5610e-04\n",
      "Epoch 480/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0315 - val_loss: 0.1492 - lr: 6.5610e-04\n",
      "Epoch 481/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0315 - val_loss: 0.1491 - lr: 6.5610e-04\n",
      "Epoch 482/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0314 - val_loss: 0.1462 - lr: 6.5610e-04\n",
      "Epoch 483/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0314 - val_loss: 0.1470 - lr: 6.5610e-04\n",
      "Epoch 484/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0315 - val_loss: 0.1503 - lr: 6.5610e-04\n",
      "Epoch 485/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0313 - val_loss: 0.1497 - lr: 6.5610e-04\n",
      "Epoch 486/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0312 - val_loss: 0.1511 - lr: 6.5610e-04\n",
      "Epoch 487/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0313 - val_loss: 0.1508 - lr: 6.5610e-04\n",
      "Epoch 488/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0313 - val_loss: 0.1487 - lr: 6.5610e-04\n",
      "Epoch 489/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0314 - val_loss: 0.1505 - lr: 6.5610e-04\n",
      "Epoch 490/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0313 - val_loss: 0.1479 - lr: 6.5610e-04\n",
      "Epoch 491/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0313 - val_loss: 0.1471 - lr: 6.5610e-04\n",
      "Epoch 492/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0314 - val_loss: 0.1504 - lr: 6.5610e-04\n",
      "Epoch 493/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0314 - val_loss: 0.1478 - lr: 6.5610e-04\n",
      "Epoch 494/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0314 - val_loss: 0.1477 - lr: 6.5610e-04\n",
      "Epoch 495/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0315 - val_loss: 0.1500 - lr: 6.5610e-04\n",
      "Epoch 496/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0314 - val_loss: 0.1500 - lr: 6.5610e-04\n",
      "Epoch 497/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0314 - val_loss: 0.1495 - lr: 6.5610e-04\n",
      "Epoch 498/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0310 - val_loss: 0.1495 - lr: 6.5610e-04\n",
      "Epoch 499/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0314 - val_loss: 0.1483 - lr: 6.5610e-04\n",
      "Epoch 500/500\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.0310 - val_loss: 0.1469 - lr: 6.5610e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f39e0789910>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 设置 EarlyStopping 回调函数，如果验证集的损失不再改善，则停止训练\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.9, patience=100)\n",
    "model.fit( # 根据情况调整参数\n",
    "train_features,\n",
    "train_labels,\n",
    "validation_data=(val_features, val_labels),\n",
    "epochs=500,\n",
    "batch_size=32,\n",
    "callbacks=[lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c4e5f76-cdd8-4d3f-a6da-32263c60d88d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 MSE:0.0711\n",
      "y1 MSE:2998.0000 ------ 2998\n"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(test_features)\n",
    "print(\"y1 MSE:%.4f\" % mean_squared_error(test_labels, test_preds))\n",
    "print(\"y1 MSE:%.4f\" % len(test_labels), '------',len(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f97739de-d01c-49fc-b8f3-b1b6ac40d4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 02:56:53.259890: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /models/slot0/20230901105653/assets\n"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "from datetime import datetime                                                                                                                                                                \n",
    "\n",
    "model_version =  datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y%m%d%H%M%S')\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    f'/models/slot0/{model_version}/', # v1/models/slot0/为tensorflow-serving的模型根目录\n",
    "    overwrite=True,\n",
    "    include_optimizer=True,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc678317-f4a5-49b2-85e4-7f25f0d95332",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"inputs\": [[1.4158, 2.9711, 10.7935, 7.5279, 2.3352, 8.1042, 2.3096, 3.3367, 11.8639, 12.7142, 1.8581, 0.3898, 19.8309, 19.771, 0.0001, 1.7768, 171.764, 1434.24]]}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Request tf-serving failed: {\n    \"error\": \"Servable not found for request: Specific(slot0, 20230901105653)\"\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_99272/1501217929.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                          headers={\"content-type\": \"application/json\"})\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Request tf-serving failed: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mresp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'outputs'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp_data\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Request tf-serving failed: {\n    \"error\": \"Servable not found for request: Specific(slot0, 20230901105653)\"\n}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "test_features=pd.DataFrame(test_features)\n",
    "test_labels=pd.DataFrame(test_labels)\n",
    "req_data = json.dumps({\n",
    "            'inputs': test_features.values[:1].tolist()\n",
    "        })  \n",
    "print(req_data)\n",
    "response = requests.post(f'http://fireeye-test-model-container:8501/v1/models/slot0/versions/{model_version}:predict', # 根据部署地址填写\n",
    "                         data=req_data,\n",
    "                         headers={\"content-type\": \"application/json\"})\n",
    "if response.status_code != 200:\n",
    "    raise RuntimeError('Request tf-serving failed: ' + response.text)\n",
    "resp_data = json.loads(response.text)    \n",
    "if 'outputs' not in resp_data \\\n",
    "                    or type(resp_data['outputs']) is not list:\n",
    "    raise ValueError('Malformed tf-serving response')\n",
    "\n",
    "print(resp_data)\n",
    "print(\"{'outputs':\",test_labels.values[:1].tolist())\n",
    "\n",
    "print(\"y1 MSE:%.4f\" % mean_squared_error(test_labels.values[:1].tolist(), resp_data['outputs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5011d7-455f-44c0-b2ed-e2e10bbb1101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea2950-0241-45aa-b1df-fc920b21649f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
